[{"authors":["admin"],"categories":null,"content":"I am a student of Masters of Statistics (M.Stat.) in Indian Statistical Institute, Kolkata. My research interest includes Applied Statistics in various economical, financial and computational fields.\nIf you are thinking how to make a profile picture like the one I have, make sure to check out Neural Style Transfer.\n","date":1572566400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1572566400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a student of Masters of Statistics (M.Stat.) in Indian Statistical Institute, Kolkata. My research interest includes Applied Statistics in various economical, financial and computational fields.\nIf you are thinking how to make a profile picture like the one I have, make sure to check out Neural Style Transfer.","tags":null,"title":"Subhrajyoty Roy","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"This is an introductory notes for learning Integral Calculus and some useful tricks to perform integration at Pre-College level.\nYou can find the notes by clicking here.\nDid you find this page helpful? Consider sharing it! 😄 ","date":1577145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577145600,"objectID":"0c6f6cbe96ac9b1239a629e09477d6d1","permalink":"/post/post2/","publishdate":"2019-12-24T00:00:00Z","relpermalink":"/post/post2/","section":"post","summary":"This is an introductory notes for learning Integral Calculus and some useful tricks to perform integration at Pre-College level.\nYou can find the notes by clicking here.\nDid you find this page helpful? Consider sharing it! 😄 ","tags":null,"title":"Introduction to Integral Calculus for 10+2 Level","type":"post"},{"authors":null,"categories":null,"content":"","date":1576195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576195200,"objectID":"fe859c6266641600992fd6a0d6f11947","permalink":"/post/post1/","publishdate":"2019-12-13T00:00:00Z","relpermalink":"/post/post1/","section":"post","summary":"","tags":null,"title":"D Basu Memorial Presentation on t-SNE: A way to Visualize Multidimensional Dataset","type":"post"},{"authors":null,"categories":null,"content":"  table { display: table; }  Background Mount Rainier, also known as Tahoma and Tacoma is a large active Stratovolcano located 59 miles south south-east of Seattle, Washington in United States of America. With a summit elevation of 14,411 ft (4,392 m), it is the highest Mountain peak of state Washington, the Cascade range; and most topographically prominent peak of the contiguous United States (i.e., leaving out Alaska, Hawaii and islands).\nDue to being a topographically prominent peak, mountainiers face a lot of struggle to climb the peak, as they have to ascend a higher slope of elevation. Also, they are faced with other challenges;\n Climbing requires traversing through the largest glacier of United States. Climbing takes 2 or 3 days, with very high failure rates. Weather and physical condition of the routes used for climbing poses a great challenge. Also, weather variables are very erratic in nature. Climbing team requires high-end professionals with experience of Glacier climbing. Climbers need to take permit by the law to start their journey.  Objective The objective of the study is to analyze patterns in climber\u0026rsquo;s traffic, as well as model the success proportion based on current weather scenario and some relevant covariates.\nDataset The dataset was obtained from Kaggle. * Timeperiod of the dataset is 25th September, 2014 to 27th November, 2017.\n There are three main routes for climbing Mt. Rainier.\n Disappointment Cleaver (Why the name Disappointment comes in, nobody knows for sure, however, one popular theory is people often gets disappointed for high rate of failure in this route. But is it True?), is the main route to climb Mt. Rainier. Emmons Winthrop. Kautz Glacier. (The last two routes are only opened where Disappointment cleaver gets closed)  Some of the weather covariates are Temperature, Humidity, Wind Speed, Solar Radiation etc.\n Climbing statistics include Team size, Number of Succeeded etc.\n  How Weather Changes over Time It seems that weather covariates are really erratic in nature.\nHow Climber\u0026rsquo;s Traffic Changes over time    Year \u0026amp; Month Total Number of Teams Average Team Size     2014-09 22 6.68   2014-10 3 1.33   2015-01 1 2   2015-03 10 4.3   2015-04 17 3.76   2015-05 163 5.50   2015-06 514 5.47   2015-07 522 5.38   2015-08 325 5.89   2015-09 102 7.58   2015-10 6 3.33   2015-11 2 2.5    It seems that in the time of late summer, a lot of teams attempt the climbing, as well as the average team size also increases. Also, during winters, the climbers traffic remains almost negligible (Consider November \u0026amp; December of 2014).\nTherefore, in our predictive model, a periodic component for the whole year should be added. Let $t$ denote the current date and let $t_0$ be the Janurary 1 of the same year, then we conisder the term,\n$$\\gamma_1 \\sin\\left(\\frac{\\pi}{365} (t - t_0)\\right) + \\gamma_2 \\cos\\left(\\frac{\\pi}{365} (t - t_0)\\right)$$\nin our generalized linear model.\nPredictive Analysis Logistic Regression The very simple, yet, sometimes very effective tool for modeling the success rate would be the use of a logistic regression model. So, we have the following model for our data;\n$$\\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\alpha_0 + \\alpha_j + \\sum_k \\beta_k X_{ik} + \\gamma_1 \\sin\\left(\\frac{\\pi}{365} (t - t_0)\\right) + \\gamma_2 \\cos\\left(\\frac{\\pi}{365} (t - t_0)\\right)$$\nwhere $p_i$ is the true proportion of successes for $i$-th sample, $\\alpha_j$ is the effect of $j$-th route used for climbing, while $X_{ik}$\u0026rsquo;s are the weather covariates which affects the success rate. Fitting such a simple logistic regression model in R is fairly easy, using glm function in stats package.\nThe above violin plot indicates the distribution of predicted values of successes, conditonal on the true number of successes. The green line is a reference $x = y$ line, deviation from which indicates inaccuracy of the prediction model. It seems that a logistic regression, although it performs greatly when true number of successes is low, it performs poorly when success rate is high. Hence, in a sense, it underestimate the success proportion.\nSome More Exploration To find out why logistic regression fails miserably, we perform some more exploratory analysis. Exploration of the data is always useful, it gives insights about approaching the statistical problem at hand.\nThe first plot shows the histogram of Number of Attempts (i.e. the Team Size) over different Routes. It shows that there is a peak at $1$ and $12$, and a slight peak at $6-7$. It indicates that there are 3 types of teams which goes for the summit.\n Solo \u0026amp; Duo. Medium size groups. Large tourist groups for expedition.  This groups make a lot of sense, and should be taken into account when performing the predictive analysis.\nThe next figures will comprises of histograms for the Number of Succedded people in each team, conditioned on the size of the team.\nThe above figures depict an interesting fact, in each team, it is more likely that either eveyone fails or everyone climbs the mountain, than some of the team members failed and others claimed the summit. It would be very inhuman thing to leave your fellow team members in that harsh condition of the mountain, and climbing for the summit by yourself, Nah! nobody would want to do that.\nHowever, from statistical perspective, this yields a difficulty, we cannot model this by Binomial distribution as we did in the logistic regression, as Binomial distribution certainly does not look like this.\nMixture of Poissons and ZNIB Now that we have understood some basic mechanisms of the climbers traffic, let us model the Team size first. Let $N_i$ denote the team size of $i$-th sample. Then,\n$$N_i = p_{i1}N_{i1} + p_{i2}N_{i2} + (1 - p_{i1} - p_{i2})N_{i3}$$\nwhere $N_{ik} \\sim \\text{Poisson}(\\lambda_k)$. Clearly, $N_i$ would be a mixture of $3$ count variables, each of which represents the type of Team we explored in previous part. To model this using covariates, we use logit model for $p_{ik}$\u0026rsquo;s while we use poisson regression with log link function to model $N_{ik}$. We can use flexmix package in R to perform this fitting for us. Here, concomitant variables are those covariates which determine the mixing probabilities in the model.\nfit.cv \u0026lt;- stepFlexmix(Attempted ~ MainRoute + sin((pi / 365) * TransDate) + cos((pi / 365) * TransDate) + `Battery Voltage AVG` + `Temperature AVG` + `Relative Humidity AVG` + `Wind Speed Daily AVG` + `Wind Direction AVG` + `Solare Radiation AVG`, data = data, model = FLXglmFix(family = \u0026quot;poisson\u0026quot;), concomitant = FLXPmultinom( ~ MainRoute + sin((pi / 365) * TransDate) + cos((pi / 365) * TransDate) + `Battery Voltage AVG` + `Temperature AVG` + `Relative Humidity AVG` + `Wind Speed Daily AVG` + `Wind Direction AVG` + `Solare Radiation AVG`), k = 2:4, nrep = 3) fit \u0026lt;- getModel(fit.cv, \u0026quot;BIC\u0026quot;) # we use BIC criterion for Model selection x \u0026lt;- refit(fit) x@components x@concomitant  The weather covariates turn out to be not significant in the log-linear means of any component, also not in the mixture component as well, but the routes and time components seem significant. This is intuitive since the team is decided long before the journey actually takes place, hence the team size should not depend on the particular weather on the day of journey.\nThe prediction for the team size also seems much better from the plot below, which shows the distribution of $\\mathbb{E}(N_i)$ as well as the distribution of upper limit and lower limit of an asymptotic confidence interval for $N_i$.\nTo model the success proportion, we consider modelling the conditional distribution of succeeded given the size of the team. Clearly, as shown before, a binomial modelling would turn into ashes as we have peaks in histogram at $0$ and $N_i$. Therefore, we model the number of succeeded;\n$$Y_i | N_i \\sim q_{i1}\\mathbb{I}_{(Y_i = 0)} + q_{i2}\\mathbb{I}_{(Y_i = N_i)} + (1 - q_{i1} - q_{i2}) \\text{Bin}(N_i, p_i)$$\nwhere $\\mathbb{I}_{(Y_i = c)}$ denotes the distribution degenerate at $c$. Therefore, the model that we have is a Zero and N Inflated Binomial (ZNIB) model. Clearly, as before we model the $q_{ij}$\u0026rsquo;s using simple logistic model, and binomial probabilities using another logistic model.\nSince the model is fairly new, there is no package (that I know of) which performs the fitting of the above model. So, we write our own code for likelihood maximization, using optim function in stats package.\ninflbinom \u0026lt;- function(form, data, sizes) { response \u0026lt;- model.response(model.frame(formula(form), data)) modelmat \u0026lt;- model.matrix(formula(form), data) print(ncol(modelmat)) loglike \u0026lt;- function(params) { # params are of three types # 1 to ncol(modelmat), ncol(modelmat) + 1 to 2*that, and so on eta1 \u0026lt;- modelmat %*% params[1:ncol(modelmat)] eta2 \u0026lt;- modelmat %*% params[(ncol(modelmat) + 1):(2 * ncol(modelmat))] eta3 \u0026lt;- modelmat %*% params[(2 * ncol(modelmat) + 1):(3 * ncol(modelmat))] m \u0026lt;- max(eta1, eta2, eta3) eta1 \u0026lt;- eta1 - m + 10 eta2 \u0026lt;- eta2 - m + 10 eta3 \u0026lt;- eta3 - m + 10 binom.prob \u0026lt;- (exp(eta3)/(1 + exp(eta3))) zeroinf.prob \u0026lt;- (exp(eta1)/(1 + exp(eta1) + exp(eta2))) + ((1/(1 + exp(eta1) + exp(eta2))) * dbinom(0, size = sizes, prob = binom.prob)) endinf.prob \u0026lt;- (exp(eta2)/(1 + exp(eta1) + exp(eta2))) + ((1/(1 + exp(eta1) + exp(eta2))) * dbinom(sizes, size = sizes, prob = binom.prob)) loglikelihood \u0026lt;- sum(log(zeroinf.prob[response == 0])) + sum(log(endinf.prob[response == sizes])) + sum(as.numeric(response \u0026gt; 0 \u0026amp; response \u0026lt; sizes) * (dbinom(response, size = sizes, prob = binom.prob, log = T) + log((1/(1 + exp(eta1) + exp(eta2)))))) return(loglikelihood) } return(loglike) } predict.inflbinom \u0026lt;- function(form, data, sizes, op) { modelmat \u0026lt;- model.matrix(formula(form), data) eta1 \u0026lt;- modelmat %*% op$par[1:ncol(modelmat)] eta2 \u0026lt;- modelmat %*% op$par[(ncol(modelmat) + 1):(2 * ncol(modelmat))] eta3 \u0026lt;- modelmat %*% op$par[(2 * ncol(modelmat) + 1):(3 * ncol(modelmat))] eta1 \u0026lt;- eta1 - max(eta1) + 10 eta2 \u0026lt;- eta2 - max(eta2) + 10 eta3 \u0026lt;- eta3 - max(eta3) + 10 binom.prob \u0026lt;- (exp(eta3)/(1 + exp(eta3))) zeroinf.prob \u0026lt;- exp(eta1)/(1 + exp(eta1) + exp(eta2)) endinf.prob \u0026lt;- exp(eta2)/(1 + exp(eta1) + exp(eta2)) other.prob \u0026lt;- (1 - zeroinf.prob - endinf.prob) preds \u0026lt;- numeric(length(sizes)) for (i in 1:length(sizes)) { x \u0026lt;- dbinom(0:sizes[i], size = sizes[i], prob = binom.prob[i]) * other.prob[i] preds[i] \u0026lt;- (which.max(x) - 1) } return(preds) } test.inflbinom \u0026lt;- function(form, op) { fisher.mat \u0026lt;- solve(op$hessian) std_err \u0026lt;- sqrt(-diag(fisher.mat)) estimate \u0026lt;- op$par z_value \u0026lt;- estimate / std_err p_value \u0026lt;- 2*pnorm(-abs(z_value)) signif.codes \u0026lt;- stars.pval(p_value) coefs \u0026lt;- colnames(model.matrix(formula(form), traindata)) zeroinf.df \u0026lt;- data.frame(coefs, estimate[1:length(coefs)], std_err[1:length(coefs)], z_value[1:length(coefs)], p_value[1:length(coefs)], signif.codes[1:length(coefs)]) binom.df \u0026lt;- data.frame(coefs, estimate[(length(coefs)+1):(2*length(coefs))], std_err[(length(coefs)+1):(2*length(coefs))], z_value[(length(coefs)+1):(2*length(coefs))], p_value[(length(coefs)+1):(2*length(coefs))], signif.codes[(length(coefs)+1):(2*length(coefs))]) endinf.df \u0026lt;- data.frame(coefs, estimate[(2*length(coefs)+1):(3*length(coefs))], std_err[(2*length(coefs)+1):(3*length(coefs))], z_value[(2*length(coefs)+1):(3*length(coefs))], p_value[(2*length(coefs)+1):(3*length(coefs))], signif.codes[(2*length(coefs)+1):(3*length(coefs))]) colnames(zeroinf.df) \u0026lt;- c(\u0026quot;\u0026quot;,\u0026quot;Estimate\u0026quot;,\u0026quot;Std.Error\u0026quot;,\u0026quot;Z-value\u0026quot;,\u0026quot;p-value\u0026quot;,\u0026quot;Signif.codes\u0026quot;) colnames(binom.df) \u0026lt;- c(\u0026quot;\u0026quot;,\u0026quot;Estimate\u0026quot;,\u0026quot;Std.Error\u0026quot;,\u0026quot;Z-value\u0026quot;,\u0026quot;p-value\u0026quot;,\u0026quot;Signif.codes\u0026quot;) colnames(endinf.df) \u0026lt;- c(\u0026quot;\u0026quot;,\u0026quot;Estimate\u0026quot;,\u0026quot;Std.Error\u0026quot;,\u0026quot;Z-value\u0026quot;,\u0026quot;p-value\u0026quot;,\u0026quot;Signif.codes\u0026quot;) return(list(\u0026quot;Zero Inflated Model\u0026quot;=zeroinf.df, \u0026quot;Binomial Model\u0026quot; = binom.df, \u0026quot;End Inflated Model\u0026quot; = endinf.df)) }  Here, inflbinom function takes the formula, data and the conditonal sizes which it uses to create the loglikelihood function for the model. predict.inflbinom function takes the formula, data, conditional sizes and the list that contains optimized parameter values (i.e. output of optim function). test.inflbinom function uses the hessian matrix output from optim function to compute Fisher information matrix, and uses that to perform Wald\u0026rsquo;s asymptotic test for significance of corresponding coeffcient.\nform \u0026lt;- \u0026quot;Succeeded ~ MainRoute + sin((pi / 365) * TransDate) + cos((pi / 365) * TransDate) + `Battery Voltage AVG` + `Temperature AVG` + `Relative Humidity AVG` + `Wind Speed Daily AVG` + `Wind Direction AVG` + `Solare Radiation AVG`\u0026quot; modelfun \u0026lt;- inflbinom(form, data, sizes = data$Attempted) set.seed(47) op \u0026lt;- optim(runif(11*3), fn = modelfun, control = list(fnscale = -1, maxit = 1e+06), hessian = T) op$convergence op$par op$value  The above code performs the maximization of the likelihood. Finally, we get the following output.\nIt seems that weather conditions are now significant. The prediction also seems a lot better.\nConclusions  The time of the year is significant in team size, but not the particular weather of the journey day. Thus an overall idea of seasons factors and the route to take decides team size traffic.\n The team size distribution would help the climbing industry to provide better service to climbers.\n However, the success proportions do depend on the weather covariates. This dependence is significant not only in the $0$ and End components, but also in the Binomial component of the success proportions.\n Thus, based on the current weather conditions, the model should help climbers to take decision whether to attempt climbing or not.\n  ","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"37bbcc0b233003f42372218496dd7d25","permalink":"/project/mount-rainier/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/project/mount-rainier/","section":"project","summary":"Regression Analysis of Kaggle Data on Mt. Rainier Climbing Statistics","tags":["Regression"],"title":"Analysis of Mount Rainier Climbing Data (Kaggle)","type":"project"},{"authors":["Dr. Diganta Mukherjee","Abhinandan Dalal","Subhrajyoty Roy"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- Tea Auctions across India occur as an ascending open auction, conducted online. Before the auction, a sample of the tea lot is sent to potential bidders, and a group of tea-testers. The seller’s reserve price is a confidential function of the tea-tester’s valuation, which also acts as a signal to the bidders. In this paper, we work with the dataset from a single tea auction house, J Thomas, of tea dust category, on 49 weeks in the time span of 2018-2019, with the following objectives in mind:\n Objective classification of the various categories of tea dust into a more manageable, and robust classification of the tea dust, based on source and grades. Predict which tea lots would be sold in the auction market, and a model for the final price conditioned on sale. To study the distribution of price and ratio of the sold tea auction lots and Discussion on the possibility of automation of the process without human intervention.  More details about the paper is available here.\n","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"88bf357e1a78347e78f3c1867aa3f412","permalink":"/publication/tea-auction-feasibility/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/tea-auction-feasibility/","section":"publication","summary":"This article examines Tea Auctions in India and forms an idea about the valuations that occur for several of the tea grades, and endeavoured to fit the model determining the final transaction price of the auction based on several relevant factors. The article also looks into the significance of manual valuations as predictors for the final price, and discussed several aspects to do away with the practice of manual valuations.","tags":["Auction theory","Gaussian Mixture Model","Mixture of Regression","Clustering","Price Modelling"],"title":"Feasibility of Transparent Price Discovery in Tea through Auction in India","type":"publication"},{"authors":["Ritwik Bhaduri","Soham Bonnerjee","Subhrajyoty Roy"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   -- Supplementary materials can be found here including the code and datasets used.\n","date":1566345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566345600,"objectID":"cb7823180fe478c3edba8283fa9ab119","permalink":"/publication/onset-detection-arxiv/","publishdate":"2019-08-21T00:00:00Z","relpermalink":"/publication/onset-detection-arxiv/","section":"publication","summary":"We develop a QBH system based on detection of onsets of a song, within a statistical framework.","tags":["Sound processing","Subset matching"],"title":"Onset detection - A new approach to QBH system","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Pramit Das","Subhrajyoty Roy"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- More details about the paper is available here.\n","date":1547078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547078400,"objectID":"1e5e9235ae9b20af955b08efac3cefbd","permalink":"/publication/us-china-trade-war/","publishdate":"2019-01-10T00:00:00Z","relpermalink":"/publication/us-china-trade-war/","section":"publication","summary":"A study has been undertaken to investigate the effect of US-Chinese trade war on the returns of large tech stocks based on both these countries. This helps us to scientifically explain the reason of steady growth in US stocks, and the downfall of major Chinese tech stocks, with the support of past data on these stocks.","tags":["Time series","Stock market analysis","Finance"],"title":"Analysing Impact of US – China Trade War on Large Tech Stocks","type":"publication"},{"authors":null,"categories":null,"content":"  table { display: table; }  The application is built as a part of the capstone project for the Data Science specialization of Coursera provided by John Hopkins Bloomberg School of Public Health in collaboration with Swiftkey.\nBackground The original dataset used to build the application has been provided by Swiftkey in three separate text files, - en-UN-blogs.txt - en-UN-news.txt - en-UN-twitters.txt, which contains a large number of english sentences from blogs, news and tweets from twitters respectively. Since, the raw corpus is huge, it has been cleaned to remove url, email, non-ASCII characters and profanity filter has also been performed using parallel programming. However, for building the prediction model, only a sample of $25,000$ text paragraphs is taken from each of the three files, which comprises of about a million sentences together. Usage of more data than this sufficiently increases the time to give a prediction and creates poor user experience with the application.\nAbout the Dataset The corpus is available in the following url: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip, We use R to download and unzip the corpus text files for us.\nurl \u0026lt;- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip' if (!file.exists('Coursera-SwiftKey.zip')){ download.file(url, destfile = \u0026quot;Coursera-SwiftKey.zip\u0026quot;, method = \u0026quot;curl\u0026quot;) } unzip('Coursera-SwiftKey.zip', exdir = \u0026quot;.\u0026quot;)  Basic Summeries Now that we have our corpus downloaded and extracted, we should check some basic summaries like sizes of the corpus text files, number of sentences etc.\nCorpusNames \u0026lt;- c('en_US.blogs', 'en_US.news', 'en_US.twitter') sizes \u0026lt;- numeric(3) sizes[1] \u0026lt;- file.info('final/en_US/en_US.blogs.txt')$size/(2^20) #so that it shows in mb sizes[2] \u0026lt;- file.info('final/en_US/en_US.news.txt')$size/(2^20) sizes[3] \u0026lt;- file.info('final/en_US/en_US.twitter.txt')$size/(2^20) # Read blogs data con \u0026lt;- file('./final/en_US/en_US.blogs.txt') blogdata \u0026lt;- readLines(con); close(con) # Read news data con \u0026lt;- file('./final/en_US/en_US.news.txt') newsdata \u0026lt;- readLines(con); close(con) # Read twitter data con \u0026lt;- file('./final/en_US/en_US.twitter.txt'); twitterdata \u0026lt;- readLines(con); close(con) lengths \u0026lt;- sapply(list(blogdata, newsdata, twitterdata), length) Chars \u0026lt;- sapply(list(blogdata, newsdata, twitterdata), function(x){sum(nchar(x))}) Sentences \u0026lt;- sapply(list(blogdata, newsdata, twitterdata), function(x){sum(stri_count_boundaries(x,type = \u0026quot;sentence\u0026quot;))}) Words \u0026lt;- sapply(list(blogdata, newsdata, twitterdata), function(x){sum(stri_count_words(x))}) data.frame(Corpus = CorpusNames, SizeinMB = sizes, Length = lengths, NumberofSenteces = Sentences, NumberofWords = Words, NumberofCharacters = Chars)     Corpus SizeinMB Length NumberofSenteces NumberofWords NumberofCharacters     en_US.blogs 200.4242 899288 2380481 37546246 206824505   en_US.news 196.2775 1010242 2025776 34762395 203223159   en_US.twitter 159.3641 2360148 3780372 30093369 162096031    Sampling the Dataset Since, the corpus is large in size, so we select 5% texts from each of the three files, and merge them together to form a corpus of reasonable size, using which we can perform related analysis and development of the text prediction system.\nset.seed(19102018) #set a seed for reproducbility blogsamples \u0026lt;- blogdata[sample(length(blogdata), size = (0.05*length(blogdata)), replace = FALSE)] #sample the blog data newssamples \u0026lt;- newsdata[sample(length(newsdata), size = (0.05*length(newsdata)), replace = FALSE)] #sample the news data twittersamples \u0026lt;- twitterdata[sample(length(twitterdata), size = (0.05*length(twitterdata)), replace = FALSE)] #sample the twitter data  Now that we have sampled our texts, we need to merge these three samples together to form our corpus.\ncorpus \u0026lt;- c(blogsamples, newssamples, twittersamples) #make the corpus  Cleaning the Corpus To clean the text, we shall first make every text to lowercase letters so that no ambiguity arises in case of other transformations. We should be concerned with the following steps in order to clean the data.\n Remove optional spaces between different words. Perform a profanity filtering. For this, we shall use the words banned by Google for profanity filtering. The list of words are here Remove any URL occuring in the text. Remove any appearance of abbreviations. Remove any appearance of punctuation marks. Remove any repeated words. Remove any appearance of digits. Remove any NonASCII character.\nurl \u0026lt;- 'https://raw.githubusercontent.com/RobertJGabriel/Google-profanity-words/master/list.txt' #download the profanity corpus if (!file.exists('bad-words.txt')){ # if the file does not exist, then we download it download.file(url, destfile = \u0026quot;bad-words.txt\u0026quot;, method = \u0026quot;curl\u0026quot;) } con \u0026lt;- file('./bad-words.txt') badwords \u0026lt;- readLines(con); close(con) corpus \u0026lt;- iconv(corpus, \u0026quot;latin1\u0026quot;, \u0026quot;ASCII\u0026quot;, sub=\u0026quot;\u0026quot;) #remove nonASCII characters corpus \u0026lt;- gsub(\u0026quot;\\\\s+\u0026quot;,\u0026quot; \u0026quot;,corpus) ## Removing optional space corpus \u0026lt;- tolower(corpus) ## lowercasing the letters for (badword in badwords){ corpus \u0026lt;- gsub(paste0(\u0026quot;\\\\s\u0026quot;,badword,\u0026quot;\\\\s\u0026quot;), \u0026quot;\u0026quot;, corpus) message(badword) #just for checking } #remove all bad words that appears corpus \u0026lt;- gsub(\u0026quot;http[[:alnum:]]*\u0026quot;,\u0026quot;\u0026quot;,corpus) #remove any URL corpus \u0026lt;- gsub(\u0026quot;([a-z]\\\\.){2,}\u0026quot;,\u0026quot;\u0026quot;, corpus) #remove any abbreviations corpus \u0026lt;- gsub(\u0026quot;[[:digit:]]\u0026quot;,\u0026quot;\u0026quot;, corpus) #remove any digits corpus \u0026lt;- gsub(\u0026quot;(\\\\w+\\\\s)\\\\1+\u0026quot;,\u0026quot;\\\\1\u0026quot;, corpus) #remove the repeated word corpus \u0026lt;- gsub(\u0026quot;[[:punct:]]\u0026quot;, \u0026quot;\u0026quot;, corpus) #remove any punctuation marks   Now that our corpus is cleaned, we save it in a text file for future references.\ncon \u0026lt;- file('./cleaned-corpus.txt') writeLines(corpus, con); close(con)  Building ist of frequent N-Grams in the Corpus We compute the list of the most frequent N-Grams in the corpus. This will serve as a basis for the text based prediction model. For this purpose, we are using ngram library which performs fast n-gram tokenization. We present the top 10 frequent n-grams for n=1,2,3,4 and 5 and visualize the corresponding bar diagram.\nNow, there might be some sentences remaining which has less than 3 words. These sentences clearly would not provide a good corpus for text prediction using n-grams.\nnwords \u0026lt;- stri_count_words(corpus) #count the number of words print(sum(nwords \u0026lt; 3)/length(nwords))  We see that this makes us to remove about 3% of the corpus.\ncorpus \u0026lt;- corpus[nwords\u0026gt;=3] #keep those corpus texts where there are at least 3 words.  1-Gram or Words ng \u0026lt;- ngram(corpus, n=1) #get the one_grams ng_table \u0026lt;- head(get.phrasetable(ng), 10) #get top 10 entries knitr::kable(ng_table) barplot(ng_table$freq, names.arg = ng_table$ngrams, las = 2, ylab = \u0026quot;Frequency\u0026quot;, main = \u0026quot;Bar Diagram of Word frequency in the Corpus\u0026quot;)  2-Gram or Bigrams ng \u0026lt;- ngram(corpus, n=2) #get the bi_grams ng_table \u0026lt;- head(get.phrasetable(ng), 10) #get top 10 entries knitr::kable(ng_table) barplot(ng_table$freq, names.arg = ng_table$ngrams, las = 2, ylab = \u0026quot;Frequency\u0026quot;, main = \u0026quot;Bar Diagram of Bigram frequency in the Corpus\u0026quot;)  For trigrams and longer n-grams, although the code generates barplots, only the frequency tables are provided.\n3-Gram or Trigrams ng \u0026lt;- ngram(corpus, n=3) #get the tri_grams ng_table \u0026lt;- head(get.phrasetable(ng), 10) #get top 10 entries knitr::kable(ng_table) barplot(ng_table$freq, names.arg = ng_table$ngrams, las = 2, ylab = \u0026quot;Frequency\u0026quot;, main = \u0026quot;Bar Diagram of Trigram frequency in the Corpus\u0026quot;)     ngrams freq prop     One of the 1651 0.0003643   A lot of 1426 0.0003146   Thanks for the 1177 0.0002597   To be a 925 0.0002041   Going to be 906 0.0001999   The end of 770 0.0001699   Out of the 723 0.0001595   I want to 721 0.0001591   It was a 693 0.0001529   Some of the 689 0.0001520    4-gram or Tetragrams nwords \u0026lt;- stri_count_words(corpus) ng \u0026lt;- ngram(corpus[nwords\u0026gt;=4], n=4) #get the 4_grams ng_table \u0026lt;- head(get.phrasetable(ng), 10) #get top 10 entries knitr::kable(ng_table) barplot(ng_table$freq, names.arg = ng_table$ngrams, las = 2, ylab = \u0026quot;Frequency\u0026quot;, main = \u0026quot;Bar Diagram of Tetragram frequency in the Corpus\u0026quot;)     ngrams freq prop     The end of the 409 9.45e-05   The rest of the 335 7.74e-05   At the end of 327 7.56e-05   Thanks for the follow 300 6.94e-05   For the first time 272 6.29e-05   At the same time 256 5.92e-05   When it comes to 228 5.27e-05   One of the most 201 4.65e-05   To be able to 200 4.62e-05   Is going to be 199 4.60e-05    5-gram or Pentagrams ng \u0026lt;- ngram(corpus[nwords\u0026gt;=5], n=5) #get the 5_grams ng_table \u0026lt;- head(get.phrasetable(ng), 10) #get top 10 entries knitr::kable(ng_table) barplot(ng_table$freq, names.arg = ng_table$ngrams, las = 2, ylab = \u0026quot;Frequency\u0026quot;, main = \u0026quot;Bar Diagram of Pentagram frequency in the Corpus\u0026quot;)     ngrams freq prop     At the end of the 190 4.60e-05   For the first time in 82 1.99e-05   In the middle of the 80 1.94e-05   The end of the day 78 1.89e-05   By the end of the 63 1.53e-05   Thank you so much for 63 1.53e-05   For the rest of the 59 1.43e-05   Its going to be a 59 1.43e-05   There are a lot of 53 1.28e-05   Happy mothers day to all 52 1.26e-05    Goals for Text Prediction Application  Using a basic 5-gram model to predict the next word based on the previous three words. Including 2-gram, 3-gram and 4-gram models at the initial level. Create larger corpus sizes in order to assess better accuracy. Include prediction of punctuation marks along with the word. Store the n-gram objects efficiently to be used on light-weight devices. Optimize the running time of the prediction algorithm and locate the bottlenecks of the code for efficiency in speed.  Prediction Algorithm The prediction algorithm used to build the application is extremely simple, the usage of an ngram model (wiki link here). For this purpose, ngram package has been used. Ngram is the n consecutive words that appears in the corpus, for example, one-gram or unigram is the normal words, while bigram is the pharases of two words appearing consecutively (like for the, I am, can not, to be etc. ). To build the application, unigrams, bigrams, trigrams and quadgrams have been computed from the sampled and cleaned corpus. Each type of n-grams is stored in a named vector sorted according the frequency of occurence in order to redcue memory usage.\nTo predict the next words, the algorithm takes the input string and the parses the last three words (if available). Then it tries to find out those quadgrams which has those three words appearing first, and from them, a sample quadgram is chosen according to the probability proportional to its frequency. This reduces the probability of entering a loophole using the common words. Also, if the last three words are not available, then we use last two or less words, using trigrams or bigrams model if required.\nFuture Prospects  With the availablity of sufficient storage and computational power, this application can be made a lot more powerful. Inclusion of whole corpus may be possible, which would significantly boost the accuracy of prediction. We may include smileys and emoticons along with prediction of the text. Currently, this algorithm does not take account for the punctuation marks, which is important for prediction of next words. Efficiently storing the ngrams so that the application becomes smaller in size and can be deployed to smartphones. Extending this model to different languages.  Acknowledgements I would like to thank Swiftkey for providing this extremely valuable dataset which is the base of the application. I also thank Coursera for putting up this amazing specialization offered by JHU on an online platform. I thank my mentors for the specialization Jeff Leek, Roger Peng and Brian Caffo for guiding step by step thorugh this capstone project. Finally, I would like to extend my thanks and regards to my fellow classmates in this course who were a great help in clearing doubts in discussion forums.\n The Application is available at shinyapps.io.   ","date":1542067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542067200,"objectID":"d1e9965740fe6d676a7be4adc61697b3","permalink":"/project/text-prediction-app/","publishdate":"2018-11-13T00:00:00Z","relpermalink":"/project/text-prediction-app/","section":"project","summary":"A simple text prediction application built for Data Science Specialization in Coursera.","tags":["Text Analysis","Machine Learning"],"title":"A simple Text Prediction Application","type":"project"}]