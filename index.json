[{"authors":["admin"],"categories":null,"content":"I am a student of Masters of Statistics (M.Stat.) in Indian Statistical Institute, Kolkata. My research interests include Applied Statistics in economical, financial and computational fields, Machine Learning, Deep learning and Data Visualization techniques.\n","date":1572566400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1572566400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a student of Masters of Statistics (M.Stat.) in Indian Statistical Institute, Kolkata. My research interests include Applied Statistics in economical, financial and computational fields, Machine Learning, Deep learning and Data Visualization techniques.","tags":null,"title":"Subhrajyoty Roy","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"Introduction In high school level, we generally hear about Euclidean Geometry and Co-ordinate Geometry. If you are familiar in Co-ordinate Geometry, then you should be able to feel that theorems and consequences in Euclidean Geometry can be naturally extended to Co-ordinate Geometry. However, Co-ordinate Geometry is extremely specific to the co-ordinate system that we are using, for instance, if we are working on a plane (i.e. similar to Euclidean Geometry), then a planar co-ordinate system or Catersian co-ordinate system is naturally used. However, when we are performing geometry on a sphere, naturally, Polar coordinate system is used. However, suppose you are a Geographist, who wants to devise a coordinate system for the natural geography of Earth, clearly a Polar or Spherical Coordinate systen does not work, since the shape is Earth is similar to that of an ellipsoid. So, it could be very much useful if we can generalize the idea of coordinate system, to naturally represent any type of curvature, for instance, the image shown at the very beginning.\nMost of the recent advancements of geometry are developed for the specific needs of visualizing different abstract concepts of diverse mathematical fields. Generalizing the idea of coordinate system would allow one to think some abstract set (or family or collection of objects pertaining to the related mathematical theory) as a curvature, to which we can associate geometrical objects and visualize them in a different and distinguished way.\nA General Curvature: Manifold Suppose you want to create a map of the earth, however, earth is not flat (at least I am not a believer of Flat Earth Theory), so one would primarily find it confusing that we can create a flat map of earth. For example consider the following map from World Atlas.\n\nBut see that, we have a single location (like some part of Alaska \u0026amp; Russia) being mapped to two different places in the map. Also note that, due to the curvature of earth's surface, the supposedly gridlines are mapped to curved lines on the map. But if you look at it, there is a nice structure in the map, all the curved gridlines look very natural and aesthetically pleasing to understand. Moreover, if you draw tangent on the horizontal curves and tangent on the vertical curves, at their intersection point, you would find them meeting at a right angle. And if you are good at the field of Complex Analysis, then you would probably figure out that it has something to do with analytic functions.\nLet us formalize the intutions that we have. We have an arbitrary set $S$, which can be called a Manifold, if we have a one-one mapping $\\phi : S\\rightarrow \\mathbb{R}^n$ for some $n\\in \\mathbb{N}$. Since the mapping is one-one, we can identify each point of $S$ by a $n$-length vector $v \\in \\mathbb{R}^n$, which can be used to serve as a coordinate system for the points in $S$. Going back to our example of earth, to identify a location in the surface of the earth, we lay out the map and find the x-coordinates (latitude) and the y-coordinates (longitude) of the particular location. So, we have a mapping (created by geographists) that maps $S$, the surface of the earth to $\\mathbb{R}^2$. As you might have guessed, the number $n$ is the dimension of the manifold.\nLet me cite another motivating example from Wikipedia page about Manifolds to show that only one mapping may not be good enough.\nNote that, the circle is essentially a curved line, with ends being merged together. Clearly, it is a one dimensional structure, although embedded on a 2-dimensional plane. So, to interpret it as a manifold, we need to create (a or several) one-one mapping(s) from it to $\\mathbb{R}$. Let us consider the circle having center at origin and having radius equal to 1 unit. Here, we shall use $4$ different one-one mappings from part of the circle to the unit interval $[0, 1]$ which is a subset of $\\mathbb{R}$.\n\\[\\begin{align*} \\phi_{Top}((x, y)) \u0026 = (x + 1)/2 \\text{ if } y \\geq 0\\\\ \\phi_{Left}((x, y)) \u0026 = (y + 1)/2 \\text{ if } x 0 \\end{align*}\\]\nThese simple functions actually give the one-one mapping shown in the above figure. Hence, for some point on the circle, we may end up having more than one representation.\nIn general, manifolds can be very abstract. However, one usually puts some restrictions like those maps being smooth and analytic, so that it is easy to work with.\nDefining Horizons When we walk on the earth, it seems that earth is flat, as we see horizons just straight ahead of us, not below us. This is precisely because earth's geometry is locally flat, i.e. at any point on earth, the local surface of the earth can be well approximated by a plane. Mathematically, this concept is called Tangent Spaces.\nDefining Curves To formalize the idea, we need to introduce curves. These are some lines on the surface of the manifold (or on the manifold). For our example, this means a path which we take when walking on the surface of the earth.\n\nAlthough the curve (the path shown in red) is a little jagged on the surface, usually mathematicians deal with the curves which are differentiable and smooth, and is easy to deal with mathematically. A curve $\\gamma$ is a continuous function from $\\mathbb{R}$ or some interval of $\\mathbb{R}$ to the manifold $S$. For example consider a path $\\gamma : [0, 1] \\rightarrow S$, then $\\gamma(0)$ and $\\gamma(1)$ denotes the two endpoints of the curve.\nWorking towards tangents Now that you have a curve to walk on, you can stop at any point and ask where the horizon is. Similar to how we define tangents or derivatives of a function, we can think of the following quantity;\n\\[\\gamma'(x_0) = \\lim_{x \\rightarrow x_0}\\dfrac{\\gamma(x) - \\gamma(x_0)}{(x - x_0)}\\]\nWe could have done that precisely, but here's the catch, is the substraction $\\gamma(x) - \\gamma(x_0)$ is even possible? Since, in general, both these are elements of $S$, which being an arbitrary set, may not support simple operations like addition or multiplication. There are two ways we can circumvent the problem:\n Restrict the type of the manifold to fields or some special algebraic structures to allow support for addition or multiplication. Consider an arbitrary function $f : S \\rightarrow\\mathbb{R}$, which can be composed with the curve to define a derivative like thing. Then, we might try to explore possible ways to remove the arbitrary function from the expression.  The 2nd approach is the one univocally taken by mathematical community. Here, we consider any function $f$, and consider the derivative;\n\\[\\frac{d}{dt}f(\\gamma(t)) = \\lim_{h \\rightarrow 0}\\dfrac{f(\\gamma(t+h)) - f(\\gamma(t))}{h}\\]\nLet the coordinate system $\\phi : S\\rightarrow \\mathbb{R}^n$ be given as; $\\phi(p) = [\\xi_1(p), \\xi_2(p), \\dots \\xi_n(p)]$, where $\\xi_i$'s are coordinate functions. Let, $\\gamma(t)$ be denoted by the coordinates $[\\gamma_1(t), \\gamma_2(t), \\dots \\gamma_n(t)]$. Note that,\n\\[\\frac{d}{dt}f(\\gamma(t)) = \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial\\xi_i} \\frac{d\\gamma_i(t)}{dt}\\]\nNote that, since we took $f$ to be an arbitrary function, the tangent is given by $\\sum_{i=1}^{n} \\frac{d\\gamma_i(t)}{dt} \\frac{\\partial}{\\partial\\xi_i}$, i.e. some linear combination (based on which curve we are walking on) of the partial differential operators $\\frac{\\partial}{\\partial\\xi_i}$. Hence, the tangent space, i.e. the space (or set) containing all such tangents at a point $p$ on the manifold $S$, is given by a vector space, with basis being the partial differential operators $\\left\\{ \\left( \\frac{\\partial}{\\partial\\xi_i} \\right)_p : i = 1, 2, \\dots n\\right\\}$.\nIntuitive Understanding of Tangents Now probably you are trying to think how it all makes sense. Because after all, the tangent space is a vector space, and we generally are comfortable with vectors whose entries are some scaler quantities, specifically real numbers or complex numbers. So, how tangent spaces of manifolds have these $\\left( \\frac{\\partial}{\\partial\\xi_i} \\right)_p$ operators as a basis vector.\nCarefully look at the picture above. The manifold $M$ is the curved regions shaded in dark grey. The tangent space $T_xM$ at the point $x$ contains all the vectors $v$ which are tangential to the manifold at the point $x$. One such vector $v$, shown in the image, does not belong to the manifold, but it belongs to the larger space in which the manifold is embedded. So, if we represent the tangent space using only $n$ element vector, this will end up specifying vectors in $\\mathbb{R}^n$ (i.e. in the coordinate system), but this would only give some curve in the manifold, not something outside of the manifold $M$. Therefore, we need some type of generalization of vectors, which can be applied to any space (precisely the space where manifold $M$ is embedded in).\nThe differential operator precisely does that. It tracks how the inverse map (or inverse coordinate system) $\\phi^{-1}$ applies from $\\mathbb{R}^n$ to the space (the space where manifold $M$ is embedded in), and hence it allows one to define vectors in that space, by tracking how changes in $\\mathbb{R}^n$ transfers to that space.\nFrom Tangents to Riemannian Manifold Now that we have a geometry of tangent space and the manifold, the next thing one would try to find is distance between two points on the manifold. We know that, linear algebra allows us to define distance through a norm or with inner product in general, and we have a vector space here to perform these linear algebra. So, for each point $p \\in S$, i.e. point on the manifold, the tangent space $T_pS$ at $p$, is a vector space, hence can be equipped with a inner product. Such a manifold is called Riemannian.\nLet us use the notation $(\\partial_i)_p$ to denote the basis vector $(\\frac{\\partial}{\\partial\\xi_i})_p$ in the tangent space $T_pS$. Also, we define an inner product $\\langle (\\partial_i)_p, (\\partial_j)_p\\rangle = (g_{ij})_p$. Then, we can define the matrix $G_p$ with entries $\\left\\{ (g_{ij})_p \\right\\}_{i, j =1}^n$, which constitutes the inner product matrix on the tangent space $T_pS$. Due to this being an inner product matrix, it must be positive definite matrix.\nAlso, note that, the length of a vector in that space is;\n\\[\\Vert D\\Vert = \\langle D, D\\rangle = \\sum_{i, j = 1}^n (g_{ij})_p D_iD_j\\]\nwhere $D = \\sum_i D_i (\\partial_i)_p$. Generalizing this notion, we can find small such distances on the tangent spaces, and add (or integrate) them up in order to find the distance of a curve on that manifold. So, if $\\gamma : [a, b]\\rightarrow S$, is a curve on the manifold $S$, then its length is simply given by;\n\\[\\Vert\\gamma\\Vert = \\int_{a}^{b} (g_{ij})_p (\\gamma'_i)_p(\\gamma'_j)_p dt\\]\nwhere $(\\gamma'_i)_p = (\\frac{\\partial}{\\partial t}\\gamma_i)_{\\gamma^{-1}(p)}$, where $\\gamma_i(t)$'s are the coordinates of $\\gamma(t)$. So, you consider the derivative of the coordinate functions of the curve at the point $\\gamma^{-1}(p)$, i.e. the point of $t \\in [a, b]$ at which we have the curve function at required point $p$, so that we can use corresponding tangent space in order to calculate length.\nGeneralizing Straight Lines Using these concept of lengths, we can define Geodesics, which are kind of a generalized version of straight line. In a euclidean plane, a striaght line is a axiomatically defined as the curve which has minimum distance when connecting two points.\n\nIn this figure for example, the blue points shown on the plane, are connected via a curve which entirely lies on the plane, hence the curve with smallest such distance is the straight line segment joining those points. However, if we take two points on the surface of the sphere, then the shortest possible curve joining those two, which entirely lies on the surface of that sphere is a curved line. For instance, if you think of going from Hong Kong to New York, you cannot travel in a straight line, since that would require you to dig through earth's crust, make a tunnel to travel through. However, if we consider the shortest possible curve, which a aeroplane takes, is actually a curved line in 3-dimensional space, but is natural analogue to the gridlines (i.e. the meridians or latitudal lines), hence seems like a straight line with respect to the surface of the manifold.\nHence, as we know precisely how we can define lengths on a Riemannian manifold, we can move on to compute geodesic. Intuitively, for many cases, finding the geodesic is easier, once we use the coordinate functions. Note that, $\\phi: S\\rightarrow \\mathbb{R}^n$ is our coordinate map of the manifold $S$. Hence, any geodesic in the manifold would in principal, be a geodesic in the coordinate system (which happens under certain tricky mathematical conditions), hence, we can, in principle, find the geodesic connecting the point $a, b\\in S$, as;\n\\[\\left\\{ \\phi^{-1}(\\phi(a) t + (1-t)\\phi(b)) : t \\in [0, 1] \\right\\}\\]\nsince $\\phi(a) t + (1-t)\\phi(b)$, as $t\\in[0, 1]$ is the equation of the straight line connecting the coordinates $\\phi(a)$ and $\\phi(b)$ in the coordinate space, and then for each point on that line, we invert it back to the manifold to get a geodesic connecting $a$ and $b$.\nHowever, such intuitive things break down under a very general conditions. Surprisingly enough, on the surface of a manifold, you can create a generalized triangle, whose sides are geodesics, and have the sum of the angles of the triangle be greater or less than 180 degrees. For instance, we see one such example on our beloved planet as well.\n   So, as you can guess, many intuitive results of Euclidean geometry breaks down, thus creating a lot of newer scope of growth in this discipline.\nReferences  https://en.wikipedia.org/wiki/Manifold Shun-Ichi Amari, Hiroshi Nagaoka - Methods of information geometry-American Mathematical Society (2000)  Did you find this page helpful? Consider sharing it! 😄 ","date":1580428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580428800,"objectID":"122591be03ff19b49a97c0378f58bb18","permalink":"/post/post4/","publishdate":"2020-01-31T00:00:00Z","relpermalink":"/post/post4/","section":"post","summary":"Introduction In high school level, we generally hear about Euclidean Geometry and Co-ordinate Geometry. If you are familiar in Co-ordinate Geometry, then you should be able to feel that theorems and consequences in Euclidean Geometry can be naturally extended to Co-ordinate Geometry. However, Co-ordinate Geometry is extremely specific to the co-ordinate system that we are using, for instance, if we are working on a plane (i.e. similar to Euclidean Geometry), then a planar co-ordinate system or Catersian co-ordinate system is naturally used.","tags":null,"title":"Intuitive Introduction to Differential Geometry","type":"post"},{"authors":null,"categories":null,"content":" This is a rather long post, but here we shall about something really interesting, about how to mix the content of a picture with the style from another picture, called Neural Style Transfer. Texture Networks is a Neural Network approach devised by Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi and Victor Lempitsky in 2016, which is extremely useful to synthesize new types of textures (which is extensively used in production of clothings with exclusive designs), as well as being able to work as a style transfer mechanism.\nWhat is Neural Style Transfer? I think the answer to this question is better to show visually, rather than talking about it.\nSo, I think now one should get the idea. Neural Style transfer combines the aesthestics of an image on to another image (here the image of the girl named Karya, which has been provided by Dmitry Ulyanov\u0026rsquo;s Github), retaining the content of the image (i.e. retaining the girl in the stylized output). Note that, the effect is particularly visible in 2nd and 3rd images, whereas, for first image, the style aspect is greatly emphasized.\nI am going to discuss exactly how I created those stylized images, and hopefully, after reading this, you would be able to reproduce similar results with images of your choice.\nPrerequisites I am not talking about things you need to know beforehand to understand intricate details of the mechanism, but the software requirements that I will be using to create something similar to texture networks. So, at the very beginning, I import all required packages in python. Also, I shall be using a NVidia GeForce GTX 1060Ti Graphics card with CUDA computing capability 6.1, which is not at par with the GPU devices used at professional level, but this speeds up the computations by a lot rather than using a CPU.\nimport tensorflow as tf import numpy as np import time import functools import PIL.Image import IPython.display as display import matplotlib.pyplot as plt  tf.__version__  '2.1.0-rc0'  tf.test.gpu_device_name()  '/device:GPU:0'  Making Some Utility Functions Before proceeding with describing how the Texture Network is created, I would create some utility functions to help us later. For instance, this utility functions will allow us to load images from a path, and visualize an image given its tensor.\nWhat is a tensor? Let us revisit some high school mathematics for a bit. We know that matrix is an 2-dimensional array of numbers, if simply put. Tensor is a generalization of that, it is an n-dimensional array.\nNow, when a mathematician introduces matrix, it is essentially an efficient way of representing linear functions from a vector space to another vector space. A Vector Space is a space comprised of vectors, and a vector is something that satisfies some mathematical properties. But, we don\u0026rsquo;t need that. Think of vectors in the most simple way, it is something that has a magnitude and a direction, like speed.\nComing back to tensor, it is introduced as an efficient representation of Multilinear Maps between vector spaces. Let $V_1, V_2, \\dots V_n, W$ be some vector spaces, then, a function $f : V_1 \\times V_2 \\times\\dots V_n \\rightarrow W$ is said to be multilinear map, if $f(v_1, v_2, \\dots v_i, \\dots v_n)$ is linear in $v_i$ given all other arguments $v_1, \\dots v_{i-1}, v_{i+1}, \\dots v_n$ are fixed. And such linearity holds of any of its arguments. So, if you know mathematics of Linear Algebra and Matrices, then you would clearly understand that tensor is just a multidimensional generalization of matrices.\nWhat an Image is do to with Tensor? Now, to understand how tensor comes into play to define images, one needs to understand the mechanism of how an image is stored digitally. For this, consider a black grid like chessboard, but all cells are coloured white. Now, you start colouring some cells to black, and then you would be able to generate some pictures with tons of block like artifacts. The following example from logicalzero.com shows such a smiley face just colouring a 8x8 grid.\nA smiley was okay, but it was not very appealing. Now, since we have a 8x8 grid, and each of the cells can be coloured in 2 ways, black or white. Hence, by simple combinatorics, this generates $2^{256}$ possible images, out of this 8x8 grid, which is about $1.15 \\times 10^{77}$. That\u0026rsquo;s a lot! However, not all such combinations will result in visually appealing images, something that we can actually call as a potential image with our natural sense. So, among these vast majority of combinations, only a few will make up images, that our brain can visualize and understand as an image.\nHowever, if we wish to create more complicated images, we need a bigger grid. The reason being that these 8x8 grid cannot be used to approximate complicated curves in the image we encounter in daily life. For instance, increasing the number of cells in the grid, we can create an image of a panda.\nNote: This image transpires as a solution of a puzzle called Nonogram, which is also called as Picross or Visual Crosswords or Japanese Crosswords.\nUsing finer grids actually results in a better picture, as you can see. Digital black and white images are represented using the technique described above, and each of the grid cell is called a Pixel. Now, note that, we can represent this grid using a 2-dimensional matrix of 0\u0026rsquo;s and 1\u0026rsquo;s, where a white pixel would be represented as 0 and a black pixel would be repesented as 1. For instance, the smiley image can be matrixified like this:\n$$\\begin{bmatrix} 0 \u0026 0 \u0026 1 \u0026 1 \u0026 1 \u0026 1 \u0026 0 \u0026 0\\\\ 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0\\\\ 1 \u0026 0 \u0026 1 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \u0026 1\\\\ 1 \u0026 0 \u0026 1 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \u0026 1\\\\ 1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 1\\\\ 1 \u0026 0 \u0026 1 \u0026 1 \u0026 1 \u0026 0 \u0026 0 \u0026 1\\\\ 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0\\\\ 0 \u0026 0 \u0026 1 \u0026 1 \u0026 1 \u0026 1 \u0026 0 \u0026 0\\\\ \\end{bmatrix}$$   Now, we shall use colour images in this context of Neural Style Transfer. To represent a colour image, we require 3 such matrices. One for Red channel, one for Blue channel and another for Green Channel. Also, the elements of the matrices will be allowed to take values between 0 and 255 (to be represented by 8 digit binary numbers) or to take any real value between 0 and 1, representing the denisty of the colour. For instance, in the above black and white images, we can put the value 0.5 in some elements to represent that those pixels should be coloured using gray, which is a colour midway between black and white. Hence, allowing floating point values would ensure a richer distribution of images.\nComing back to the link between images and tensor, an image is represented by 3 such matrices, in combination, a 3-dimensional tensor, which the dimension or shape being (3, height of the image, width of the image), where 3 being number of channels.\ndef tensor_to_image(tensor): tensor = tf.clip_by_value(tensor, clip_value_min=0.0, clip_value_max=255.0) tensor = np.array(tensor, dtype=np.uint8) # convert tf array to np array of integers if np.ndim(tensor)\u0026gt;3: assert tensor.shape[0] == 1 # asserts that the BATCH_SIZE = 1 tensor = tensor[0] # take the first image return PIL.Image.fromarray(tensor)  def load_img(path_to_img, rescale = False): # we rescale the image to max dimension 256 for fasters processing max_dim = 256 img = tf.io.read_file(path_to_img) # read the image img = tf.image.decode_image(img, channels=3) # decode into image content img = tf.image.convert_image_dtype(img, tf.float32) # convert to float if rescale: img = tf.image.resize(img, tf.constant([max_dim, max_dim])) else: shape = tf.cast(tf.shape(img)[:-1], tf.float32) # get the shape of image, cast it to float type for division, expect the last channel dimension long_dim = max(shape) scale = max_dim / long_dim # scale accordingly new_shape = tf.cast(shape * scale, tf.int32) # cast the new shape to integer img = tf.image.resize(img, new_shape) # resize image img = img[tf.newaxis, :] # newaxis builts a new batch axis in the image at first dimension return img  style_path = '../input/artistic-style-transfer/pattern-rooster.jpg' content_path = '../input/artistic-style-transfer/celebGAN_male.png'  content_image = load_img(content_path, rescale = True) tensor_to_image(content_image * 255.0)  This is the content image, on which we shall apply some style. Note that, the loaded images have elements between 0 and 1, while the tensor_to_img function takes in a tensor with values between 0 and 255, hence we need to multiply all the elements by 255 to convert it to the crucial range where it can be visualized.\nSurprisingly, the image I am using is actually an image generated via Neural Network called Progressive GAN, which is really interesting work by Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen, who trained a network on Celeb A dataset to generate images of new celebrities, who do not exist in real life.\nstyle_image = load_img(style_path, rescale = False) tensor_to_image(style_image * 255.0)  This is the style image that we can going to use. So, we think, the final image would look like the image of the artificial celebrity, tessalatted like the style image.\nArchitecture of Texture Network Texture Network comprised of two main components.\n A Generator Network. A Descriptor Network.  A Generator Network is a neural network which takes input of the content image and some random noise, and output our desired stylized image.\nA Descriptor Network is a neural network which takes input of the desired stylized image, and then try to figure out the underlying style and content of the stylized image, and try to match it with the original style image and content image.\nIf you are familiar with Generative Adversarial Networks (GAN), then you would find a lot similarity of Texture Network with GANs. However, unlike to the case of GAN, here, the descriptor network will not be trained, but will be used to measure the style and content of the stylized images generated by Generator Network.\nTo understand why such a descriptor network is needed at all, consider an image of a person. If one shifts the image just by one pixel to the left, then using a simple squared error loss between the original image and shifted image would become large, however, from our perception, both images would look identical. Hence, to actually compare the two images, we specifically need to compare high level representations of the images, which will be provided by the descriptor network.\nChoice of Descriptor Network We use VGG19 as our descriptor network as it is a Very Deep Convolutional Networks for Large-Scale Image Recognition. It was developed by Karen Simonyan, Andrew Zisserman in 2014, and is trained with the ImageNet dataset, comprising of millions of images. Hence, high level features of this network will be an acurate representation of the style and content of images.\nvgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet') # Load VGG19 pretrained Network from Keras print() for layer in vgg.layers: print(layer.name) # print layer names so that we can reference them later  Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5 80142336/80134624 [==============================] - 1s 0us/step input_1 block1_conv1 block1_conv2 block1_pool block2_conv1 block2_conv2 block2_pool block3_conv1 block3_conv2 block3_conv3 block3_conv4 block3_pool block4_conv1 block4_conv2 block4_conv3 block4_conv4 block4_pool block5_conv1 block5_conv2 block5_conv3 block5_conv4 block5_pool  Note that, VGG19 has a sturcture called a Convolutional Block. Each such block consists of 5 layers of neuron, the first 4 layers being convolutional layers, and the last layers being a pooling layer. The top layer which was excluded from the loaded model is specifically a hidden dense layer that connects the final pooled layer to the output layer, which outputs the classification of the Imagenet image. However, we do not need this final hidden dense layer.\nCreating Generator Network To build the generator network, we need something called a Circular Convolution. To understand various convolutional arithmetic properly, I would recommend checking out the following resources.\n https://github.com/vdumoulin/conv_arithmetic contains a simple animation showing convolutions with different parameters. https://ezyang.github.io/convolution-visualizer/index.html has a really good interactive environment where you can set the parameters and take look at the corresponding convolutional operation.  Coming back to circular convolution, it is a simple 2-dimensional convolution with with a particular type of padding called Circular Padding. It essentially wraps the image from top to bottom and from left to right.\ndef periodic_padding(x, padding=1): ''' x: shape (batch_size, d1, d2) return x padded with periodic boundaries. i.e. torus or donut ''' d1 = x.shape[1] # dimension 1: height d2 = x.shape[2] # dimension 2: width p = padding # assemble padded x from slices # tl,tc,tr # padded_x = ml,mc,mr # bl,bc,br top_left = x[:, -p:, -p:] # top left top_center = x[:, -p:, :] # top center top_right = x[:, -p:, :p] # top right middle_left = x[:, :, -p:] # middle left middle_center = x # middle center middle_right = x[:, :, :p] # middle right bottom_left = x[:, :p, -p:] # bottom left bottom_center = x[:, :p, :] # bottom center bottom_right = x[:, :p, :p] # bottom right top = tf.concat([top_left, top_center, top_right], axis=2) middle = tf.concat([middle_left, middle_center, middle_right], axis=2) bottom = tf.concat([bottom_left, bottom_center, bottom_right], axis=2) padded_x = tf.concat([top, middle, bottom], axis=1) return padded_x  In the function periodic_padding, we give several input images, however, with only one channel for each image. Here, the input is a tensor of the shape (number of images, height of the image, width of the image). Hence, we can think of the input as if we are passing many 2-dimensional matrices of shape (height of the image, width of the image). We also pass the amount of padding that we want. Let us see what it outputs, when we pass a single 2D matrix as follows;\n$$\\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{bmatrix}$$\na = tf.constant([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]) periodic_padding(a)  \u0026lt;tf.Tensor: shape=(1, 5, 5), dtype=int32, numpy= array([[[9, 7, 8, 9, 7], [3, 1, 2, 3, 1], [6, 4, 5, 6, 4], [9, 7, 8, 9, 7], [3, 1, 2, 3, 1]]], dtype=int32)\u0026gt;  Note that, the output tensor has size 5x5, which it obtained by one unit of padding in top, bottom, left and right. Also note that, to the left of 1, we have 3, hence it is as if the rightmost column of the original matrix is wrapped to the left side of the matrix. Similar wrapping is also seen in vertical direction.\nHowever, for an image, we need to perform this circular padding for each of the channel. Hence, we first split up the 3 channels of the image, then append circular padding for each one, and finally combine them together. This is done through CircularPadding function.\ndef CircularPadding(inputs, kernel_size = 3): \u0026quot;\u0026quot;\u0026quot;Prepares padding for Circular convolution\u0026quot;\u0026quot;\u0026quot; # split all the filters n_filters_in = inputs.shape[-1] input_split = tf.split(inputs, n_filters_in, axis = -1) output_split = [] for part in input_split: part = tf.squeeze(part, axis = -1) outs = periodic_padding(part, padding = int(kernel_size / 2)) outs = tf.expand_dims(outs, axis = -1) output_split.append(outs) return tf.concat(output_split, axis = -1)  The diagram above shows the main idea of a convolutional layer. Let us say, we have an image, represented by a 5x5x3 tensor (pretty bad for visualizing, but pretty good for understanding), and we wish to perform convolution of this image with a 3x3 kernel. Then the convolution is basically a weighted combination of all the neighbouring pixels from all the layers. To understand mathematically, let us introduce some notations.\nLet $X$ denote the tensor image, and $X_{ijc}$ denote the value of the $(i, j)$-th pixel at the $c$-th channel. Now, let us say we wish to find out the value of the convolution at $(i_0, j_0)$ cell. Then, the convolution is defined as;\n$$H(i_0, j_0) = b + \\sum_{(i, j) \\in N(i_0, j_0)}\\sum_{c} X_{ijc} K((i - i_0), (j - j_0), c)$$\nwhere $K(\\cdot, \\cdot, \\cdot)$ is kernel weights i.e. some parameters of the network which the network is going to learn. Also, the parameter $b$ is the bias term and $N(i_0, j_0)$ is a neighbourhood of the pixel $(i_0, j_0)$. Therefore, if we have an image with $n$ channels, and we convolute a $k\\times k$ kernel on it, then we specifically require $(k^2n + 1)$ parameters including a bias term.\nWhy do we need Convolution? To understand convolution better, consider two vectors $\\textbf{x} = [x_1, x_2, \\dots x_n]$ and $\\textbf{y} = [y_1, y_2, \\dots y_n]$, then the dot product between them is defined as;\n$$\\textbf{x}\\cdot\\textbf{y} = \\sum_{k=1}^{n} x_ky_k$$\nThat\u0026rsquo;s high school algebra. However, we also know that, dot product measures the similarity between the vectors $\\textbf{x}$ and $\\textbf{y}$, i.e. it is maximum when $\\textbf{x},\\textbf{y}$ are collinear, and is minimum when these are orthogonal to each other. Note that, the above formula of convolution exactly looks like the formula of a dot product, hence it measures the similarity between the patch of the image and the kernel that we have.\nNow suppose, we have a kernel that looks like as follows:\n$$K = \\begin{bmatrix} -1 \u0026amp; 1 \u0026amp; -1\\\\ -1 \u0026amp; 1 \u0026amp; -1\\\\ -1 \u0026amp; 1 \u0026amp; -1 \\end{bmatrix}$$\nThen, if we convolute the image with this kernel, then it attains maximum value when we have a horizontal line, and it will attain minimum value when we have a vertical line. Hence, the convolution will tell us the presence of horizontal and vertical edges in the images, hence will provide combined information or featurs about the images to next level. Similar lower level features can again be convoluted to give rise to higher level features.\nIt should also be ntoeworthy that each such result of convolution will tells about existence of one particular feature in the image. Hence, to effectively use it, we shall need to learn many such features, which in the literature of Image Processing is described as filters.\nComing back to the Design of Generator of Texture Network, we need several blocks. There are mainly two types of blocks.\n Convolutional Block: It takes the input tensor (may be image or may be features processed in lower level of the network), and performs some Circular Convolution to process the tensor further and obtain some higher level features.\n Join Block: It takes the lower resolution tensor and a high resolution processed noise tensor as input, then it upsamples the lower resolution tensor to match the shape of the high resolution noise and finally merges them together. This noise actually allows the image to have very delicate and intricate variation in the image, as well as create the effect of increment of resolution.\n  Designing Convolutional Block def conv_block(input_size, in_filters, out_filters): \u0026quot;\u0026quot;\u0026quot;Implements the convolutional block with 3x3, 3x3, 1x1 filters, with proper batch normalization and activation\u0026quot;\u0026quot;\u0026quot; inputs = tf.keras.layers.Input((input_size, input_size, in_filters, )) # in_filters many channels of input image # first 3x3 conv conv1_pad = tf.keras.layers.Lambda(lambda x: CircularPadding(x))(inputs) conv1_out = tf.keras.layers.Conv2D(out_filters, kernel_size = (3, 3), strides = 1, padding = 'valid', name = 'conv1')(conv1_pad) hidden_1 = tf.keras.layers.BatchNormalization()(conv1_out) conv1_out_final = tf.keras.layers.LeakyReLU(name = 'rel1')(hidden_1) # second 3x3 conv conv2_pad = tf.keras.layers.Lambda(lambda x: CircularPadding(x))(conv1_out_final) conv2_out = tf.keras.layers.Conv2D(out_filters, kernel_size = (3, 3), strides = 1, padding = 'valid', name = 'conv2')(conv2_pad) hidden_2 = tf.keras.layers.BatchNormalization()(conv2_out) conv2_out_final = tf.keras.layers.LeakyReLU(name = 'rel2')(hidden_2) # final 1x1 conv conv3_out = tf.keras.layers.Conv2D(out_filters, kernel_size = (1, 1), strides = 1, padding = 'same', name = 'conv3')(conv2_out_final) hidden_3 = tf.keras.layers.BatchNormalization()(conv3_out) conv3_out_final = tf.keras.layers.LeakyReLU(name = 'rel3')(hidden_3) # final model conv_block = tf.keras.models.Model(inputs, conv3_out_final) return conv_block  model = conv_block(16, 3, 8) model.summary()  Model: \u0026quot;model\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 16, 16, 3)] 0 _________________________________________________________________ lambda (Lambda) (None, 18, 18, 3) 0 _________________________________________________________________ conv1 (Conv2D) (None, 16, 16, 8) 224 _________________________________________________________________ batch_normalization (BatchNo (None, 16, 16, 8) 32 _________________________________________________________________ rel1 (LeakyReLU) (None, 16, 16, 8) 0 _________________________________________________________________ lambda_1 (Lambda) (None, 18, 18, 8) 0 _________________________________________________________________ conv2 (Conv2D) (None, 16, 16, 8) 584 _________________________________________________________________ batch_normalization_1 (Batch (None, 16, 16, 8) 32 _________________________________________________________________ rel2 (LeakyReLU) (None, 16, 16, 8) 0 _________________________________________________________________ conv3 (Conv2D) (None, 16, 16, 8) 72 _________________________________________________________________ batch_normalization_2 (Batch (None, 16, 16, 8) 32 _________________________________________________________________ rel3 (LeakyReLU) (None, 16, 16, 8) 0 ================================================================= Total params: 976 Trainable params: 928 Non-trainable params: 48 _________________________________________________________________  tf.keras.utils.plot_model(model, show_shapes=True)  In the function conv_block, we take the height and width of the input tensor and the number of channels of the input tensor, and the number of filters to finally output after processing. Note that, after each of the Circular Convolution, we perform a Batch Normalization and a Leaky ReLU layer.\nBatch Normalization layer basically normalizes the outputs with respect to the batch axis (i.e. with respect to the ? or None axis indicated in the diagram, which means you are pass arbitrary number of images through the network) so that the mean values remain close to 0, and standard deviation remains close to 1. The following image from the 2018 paper on Group Normalization by Yuxin Wu, Kaiming He described the idea of normalization through the following interesting image.\nFinally, Leaky ReLU is a layer than performs an nonlinear activation to the input vector. Leaky ReLU is basically the following function;\n$$f(x) = \\begin{cases} x \u0026amp; \\text{ if } x \\geq 0 \\\\ \\alpha x \u0026amp; \\text{ if } x \u0026lt; 0 \\end{cases}$$\nwhere $\\alpha \u0026lt; 1$ is a non-trainable constant. It is usually fixed at a very low value like $0.05$ or $0.01$. Compared to that, ReLU function is similar to Leaky ReLU, but it outputs 0 if $x \u0026lt; 0$. Hence, Leaky ReLU is essentially a leaky version of ReLU, as it leaks out some small value for negative arguments. Since, such nonlinearity does not require additional parameters, we see 0 in the above summary output.\nDesigning Join Block def join_block(input_size, n_filter_low, n_filter_high): input1 = tf.keras.layers.Input((input_size, input_size, n_filter_low, )) # input to low resolution image input2 = tf.keras.layers.Input((2*input_size, 2*input_size, n_filter_high, )) # input to high resolution image upsampled_input = tf.keras.layers.UpSampling2D(size = (2, 2))(input1) hidden_1 = tf.keras.layers.BatchNormalization()(upsampled_input) hidden_2 = tf.keras.layers.BatchNormalization()(input2) outputs = tf.keras.layers.Concatenate(axis=-1)([hidden_1, hidden_2]) # final model join_block = tf.keras.models.Model([input1, input2], outputs) return join_block  model = join_block(128, 32, 8) model.summary()  Model: \u0026quot;model_1\u0026quot; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_3 (InputLayer) [(None, 128, 128, 32 0 __________________________________________________________________________________________________ up_sampling2d (UpSampling2D) (None, 256, 256, 32) 0 input_3[0][0] __________________________________________________________________________________________________ input_4 (InputLayer) [(None, 256, 256, 8) 0 __________________________________________________________________________________________________ batch_normalization_3 (BatchNor (None, 256, 256, 32) 128 up_sampling2d[0][0] __________________________________________________________________________________________________ batch_normalization_4 (BatchNor (None, 256, 256, 8) 32 input_4[0][0] __________________________________________________________________________________________________ concatenate (Concatenate) (None, 256, 256, 40) 0 batch_normalization_3[0][0] batch_normalization_4[0][0] ================================================================================================== Total params: 160 Trainable params: 80 Non-trainable params: 80 __________________________________________________________________________________________________  tf.keras.utils.plot_model(model, show_shapes=True)  The join block is extremely simple, it upsamples the low resolution processed features of the image. Then it normalizes both the higher resolution processed noise, and upsampled version of low resolution features, so that the effect of both branches remain comparable in the network. Finally, it combines the normalized versions.\nCompleting the Generator According to the paper describing Texture Networks, the generator should have a structure similar to the following figure.\nHowever, it was also mentioned that for style transfer, increasing the number of noise from 5 to 6, actually provides much better quality. So, we start the network from a noise of size 8x8x3, and keep increasing it till 256x256x3, which is of the same size of our original content image.\ndef generator_network(): # create input nodes for noise tensors noise1 = tf.keras.layers.Input((256, 256, 3, ), name = 'noise_1') noise2 = tf.keras.layers.Input((128, 128, 3, ), name = 'noise_2') noise3 = tf.keras.layers.Input((64, 64, 3, ), name = 'noise_3') noise4 = tf.keras.layers.Input((32, 32, 3, ), name = 'noise_4') noise5 = tf.keras.layers.Input((16, 16, 3, ), name = 'noise_5') noise6 = tf.keras.layers.Input((8, 8, 3, ), name = 'noise_6') content = tf.keras.layers.Input((256, 256, 3, ), name = 'content_input') # downsample the content image content_image_8 = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, tf.constant([8, 8])))(content) content_image_16 = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, tf.constant([16, 16])))(content) content_image_32 = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, tf.constant([32, 32])))(content) content_image_64 = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, tf.constant([64, 64])))(content) content_image_128 = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, tf.constant([128, 128])))(content) # create concatenation of downsampled content image and input nodes noise6_con = tf.keras.layers.Concatenate(axis=-1)([noise6, content_image_8]) noise5_con = tf.keras.layers.Concatenate(axis=-1)([noise5, content_image_16]) noise4_con = tf.keras.layers.Concatenate(axis=-1)([noise4, content_image_32]) noise3_con = tf.keras.layers.Concatenate(axis=-1)([noise3, content_image_64]) noise2_con = tf.keras.layers.Concatenate(axis=-1)([noise2, content_image_128]) noise1_con = tf.keras.layers.Concatenate(axis=-1)([noise1, content]) noise6_conv = conv_block(8, 6, 8)(noise6_con) # that produces 8x8x8 tensor noise5_conv = conv_block(16, 6, 8)(noise5_con) # that produces 16x16x8 tensor join5 = join_block(8, 8, 8)([noise6_conv, noise5_conv]) # that produces 16x16x16 tensor join5_conv = conv_block(16, 16, 16)(join5) # produces 16x16x16 tensor noise4_conv = conv_block(32, 6, 8)(noise4_con) # that produces 32x32x8 tensor join4 = join_block(16, 16, 8)([join5_conv, noise4_conv]) # produces 32x32x24 tensor join4_conv = conv_block(32, 24, 24)(join4) # produces 32x32x24 tensor noise3_conv = conv_block(64, 6, 8)(noise3_con) # that produces 64x64x8 tensor join3 = join_block(32, 24, 8)([join4_conv, noise3_conv]) # produces 64x64x32 tensor join3_conv = conv_block(64, 32, 32)(join3) # produces 64x64x32 tensor noise2_conv = conv_block(128, 6, 8)(noise2_con) # that produces 128x128x8 tensor join2 = join_block(64, 32, 8)([join3_conv, noise2_conv]) # produces 128x128x40 tensor join2_conv = conv_block(128, 40, 40)(join2) # produces 128x128x40 tensor noise1_conv = conv_block(256, 6, 8)(noise1_con) # that produces 256x256x8 tensor join1 = join_block(128, 40, 8)([join2_conv, noise1_conv]) # produces 256x256x48 tensor output = conv_block(256, 48, 3)(join1) # produces 256x256x3 tensor model = tf.keras.models.Model([content, noise1, noise2, noise3, noise4, noise5, noise6], output, name = 'generator') return model  generator = generator_network()  generator.summary()  Model: \u0026quot;generator\u0026quot; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== content_input (InputLayer) [(None, 256, 256, 3) 0 __________________________________________________________________________________________________ noise_6 (InputLayer) [(None, 8, 8, 3)] 0 __________________________________________________________________________________________________ lambda_2 (Lambda) (None, 8, 8, 3) 0 content_input[0][0] __________________________________________________________________________________________________ noise_5 (InputLayer) [(None, 16, 16, 3)] 0 __________________________________________________________________________________________________ lambda_3 (Lambda) (None, 16, 16, 3) 0 content_input[0][0] __________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 8, 8, 6) 0 noise_6[0][0] lambda_2[0][0] __________________________________________________________________________________________________ concatenate_2 (Concatenate) (None, 16, 16, 6) 0 noise_5[0][0] lambda_3[0][0] __________________________________________________________________________________________________ model_2 (Model) (None, 8, 8, 8) 1192 concatenate_1[0][0] __________________________________________________________________________________________________ model_3 (Model) (None, 16, 16, 8) 1192 concatenate_2[0][0] __________________________________________________________________________________________________ noise_4 (InputLayer) [(None, 32, 32, 3)] 0 __________________________________________________________________________________________________ lambda_4 (Lambda) (None, 32, 32, 3) 0 content_input[0][0] __________________________________________________________________________________________________ model_4 (Model) (None, 16, 16, 16) 64 model_2[1][0] model_3[1][0] __________________________________________________________________________________________________ concatenate_3 (Concatenate) (None, 32, 32, 6) 0 noise_4[0][0] lambda_4[0][0] __________________________________________________________________________________________________ model_5 (Model) (None, 16, 16, 16) 5104 model_4[1][0] __________________________________________________________________________________________________ model_6 (Model) (None, 32, 32, 8) 1192 concatenate_3[0][0] __________________________________________________________________________________________________ noise_3 (InputLayer) [(None, 64, 64, 3)] 0 __________________________________________________________________________________________________ lambda_5 (Lambda) (None, 64, 64, 3) 0 content_input[0][0] __________________________________________________________________________________________________ model_7 (Model) (None, 32, 32, 24) 96 model_5[1][0] model_6[1][0] __________________________________________________________________________________________________ concatenate_4 (Concatenate) (None, 64, 64, 6) 0 noise_3[0][0] lambda_5[0][0] __________________________________________________________________________________________________ model_8 (Model) (None, 32, 32, 24) 11304 model_7[1][0] __________________________________________________________________________________________________ model_9 (Model) (None, 64, 64, 8) 1192 concatenate_4[0][0] __________________________________________________________________________________________________ noise_2 (InputLayer) [(None, 128, 128, 3) 0 __________________________________________________________________________________________________ lambda_6 (Lambda) (None, 128, 128, 3) 0 content_input[0][0] __________________________________________________________________________________________________ model_10 (Model) (None, 64, 64, 32) 128 model_8[1][0] model_9[1][0] __________________________________________________________________________________________________ concatenate_5 (Concatenate) (None, 128, 128, 6) 0 noise_2[0][0] lambda_6[0][0] __________________________________________________________________________________________________ model_11 (Model) (None, 64, 64, 32) 19936 model_10[1][0] __________________________________________________________________________________________________ model_12 (Model) (None, 128, 128, 8) 1192 concatenate_5[0][0] __________________________________________________________________________________________________ noise_1 (InputLayer) [(None, 256, 256, 3) 0 __________________________________________________________________________________________________ model_13 (Model) (None, 128, 128, 40) 160 model_11[1][0] model_12[1][0] __________________________________________________________________________________________________ concatenate_6 (Concatenate) (None, 256, 256, 6) 0 noise_1[0][0] content_input[0][0] __________________________________________________________________________________________________ model_14 (Model) (None, 128, 128, 40) 31000 model_13[1][0] __________________________________________________________________________________________________ model_15 (Model) (None, 256, 256, 8) 1192 concatenate_6[0][0] __________________________________________________________________________________________________ model_16 (Model) (None, 256, 256, 48) 192 model_14[1][0] model_15[1][0] __________________________________________________________________________________________________ model_17 (Model) (None, 256, 256, 3) 1431 model_16[1][0] ================================================================================================== Total params: 76,567 Trainable params: 75,269 Non-trainable params: 1,298 __________________________________________________________________________________________________  tf.keras.utils.plot_model(generator, show_shapes = True)  Finally, we have a generator model with about 75,000 parameters.\nIt should be clear that the low level features, i.e. the outputs of the lower level (closer to input layer that final layer in VGG19) layers are basically features such a strokes, edges which defines the artistic features of the image, whereas, high level features, i.e. the outputs of the higher level (closer to final layer than input layer in VGG19) are features that contains the summarization of the image, namely the abstract object that image contains, specifically the content of the image.\nHowever, Gatys et al. in their paper A Neural Algorithm for Artistic Style depicts that the style is basically represented by the correlation of the low level features of an image, rather than the particular output of those low level features. An intuitive explanation to this particular observation can be given as follows: Consider our style image to be an image of a floor covered with square tiles, like a chessboard. Now, think of applying this tiling style to a complicated image of a landscape. To do this, one may need to consider rotating the tiles to be in a shape like diamonds, which may be able to capture better details of some corners presented in the content image. In this case, if we have a vertical and horizontal line detecting kernels as well as criss-cross line detecting kernels in VGG19, then such a pattern would not give similar output for both style image and final stylized image, but would give similar amount of correlation between those features.\nBased on this idea, Gatys et al. introduced Gram Matrix to measure the style of an image. Let us particularly concentration on one such low level layer first, for which we have a output tensor of shape (batch size, height of the image, width of the image, number of filters output). Then the corresponding gram matrix of this layer is defined as a matrix of shape (batch size, height of the image, width of the image), whose elements are given by;\n$$G_{bij} = \\dfrac{\\sum_c \\sum_d X_{bijc} X_{bijd}}{HW}$$\nwhere $X$ is the output tensor of that layer, $H, W$ denoting the height and width of the images. This formula can be implemented by tf.linalg.einsum function available in tensorflow, which performs the sum operation (of the numerator here) based on a given equation.\ndef gram_matrix(input_tensor): result = tf.linalg.einsum('bijc,bijd-\u0026gt;bcd', input_tensor, input_tensor) # compute the sum in numerator input_shape = tf.shape(input_tensor) # get the shape num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32) return result/(num_locations)  Now, I create a function called vgg_layers which returns our descriptor network, given the name of layers of VGG19 network which will be used to compute content and style of the image outputted by Generator Network.\nFurthermore, a class called TextureNetwork is created, which is inherited from tf.keras.models.Model. tf.keras.models.Model is a basic class to build new type of neural network model in keras. Inheritence allows us to automatically defines several properties of the keras model, in order to perform optimization and necessary computation in the training stage. Any object of TextureNetwork class is a Texture Network, initialized by specified names of content layers and style layers. Also, the call method of the class has been overridden from call method of tf.keras.models.Model class, which allows to define the exact workflow of feed forward system of our Texture Network. It finally outputs the generated image, along with the content output and style outputs (i.e. the gram matrices for style layers), which we can use to compute the loss function.\ndef vgg_layers(layer_names): \u0026quot;\u0026quot;\u0026quot; Creates a vgg model that returns a list of intermediate output values.\u0026quot;\u0026quot;\u0026quot; # Load our model. Load pretrained VGG, trained on imagenet data vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet') # load the vgg model vgg.trainable = False # do not train over vgg model parameters outputs = [vgg.get_layer(name).output for name in layer_names] # the output of the layers that we want model = tf.keras.Model([vgg.input], outputs) # create a keras model return model class TextureNetwork(tf.keras.models.Model): def __init__(self, style_layers, content_layers): super(TextureNetwork, self).__init__() # initialize the superClass self.vgg = vgg_layers(style_layers + content_layers) # obtain a VGG19 model with outputs being the style and content layers self.style_layers = style_layers self.content_layers = content_layers self.num_style_layers = len(style_layers) self.vgg.trainable = False # we are not going to train vgg network self.gen = generator_network() # create a generator network as part of it self.gen.trainable = True # we are going to train this generator def call(self, content, batch_size = 16): # generates noise required for the network noise1 = tf.random.uniform((batch_size, 256, 256, 3)) noise2 = tf.random.uniform((batch_size, 128, 128, 3)) noise3 = tf.random.uniform((batch_size, 64, 64, 3)) noise4 = tf.random.uniform((batch_size, 32, 32, 3)) noise5 = tf.random.uniform((batch_size, 16, 16, 3)) noise6 = tf.random.uniform((batch_size, 8, 8, 3)) gen_image = self.gen([content, noise1, noise2, noise3, noise4, noise5, noise6]) # pass through the generator to obtain generated image preprocessed_input = tf.keras.applications.vgg19.preprocess_input(gen_image) # preprocess the image outputs = self.vgg(preprocessed_input) # get the output from only the required layers style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:]) style_outputs = [gram_matrix(style_output) for style_output in style_outputs] # create style type output to compare style_dict = {style_name:value for style_name, value in zip(self.style_layers, style_outputs)} content_dict = {content_name:value for content_name, value in zip(self.content_layers, content_outputs)} return {'gen':gen_image, 'content':content_dict, 'style':style_dict}  Now that we have defined our Texture Network class, we need to specify exactly which layers we are going to use for style features. According to the paper on Texture Network, the style layers should be rel1_1, rel2_1, rel3_1, rel4_1, while the content layer should be rel4_2. To compare it with the naming convention of VGG19, replace rel with block and add the convolution layer specified. However, I decided to add layer block5_conv2, since according to Gatys et al. this layer has good features related to the content, and for the style image, we can get the tessellation effect accurately by using this particular layer. However, you are welcome to try different layers, which may result in interesting stylized images.\nstyle_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv2'] content_layers = ['block4_conv2']  optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3) # use an Adam optimizer tex_net = TextureNetwork(style_layers, content_layers) # create the texture network  Note that, since the initial weights are set to 0, or close to 0, the illiterate network produces a black image. So, let us teach the network to learn its trainable parameters and weights, so that it can produces meaningful outputs.\noutput = tex_net(content_image, 1) tensor_to_image(output['gen'])  Before training, we require the target variables, based on which our network compares its performance and computes the loss function. In the paper on Texture Network, the author trains this network on MS COCO and ImageNet dataset. However, we shall use only the content and style image that we have repeatedly, since training on those large datasets would take particularly large amount of time and much computational power, which I lack because of scarcity of funds. So, I shall feed in the same content image and style image to the network repeatedly, and hope that the random noise inputs to the network is going to prevent overfitting of the network.\nSo, I define a function called extract targets which will extract the style and content from the style and content images, and then those values can be used to compute the loss function. To explicity write the loss function, we shall use the formula;\n$$\\text{Loss} = w_c \\times L_c + w_s \\times \\sum_{k} L_{s_k}$$\nwhere $L_c$ is the content loss, computed as the mean squared error between the content tensor of content image and the content tensor of generated image. $L_{s_k}$ is similar mean squared error between the gram matix of style image and gram matrix of generated image at $k$-th style layer, and $w_c$ and $w_s$ denotes the weights corresponding to content and style loss.\ndef extract_targets(inputs): inputs = inputs*255.0 preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs) # preprocess the input image outputs = vgg_layers(style_layers + content_layers)(preprocessed_input) # get the output from only the required layers style_outputs, content_outputs = (outputs[:len(style_layers)], outputs[len(style_layers):]) style_outputs = [gram_matrix(style_output) for style_output in style_outputs] # create style type output to compare style_dict = {style_name:value for style_name, value in zip(style_layers, style_outputs)} content_dict = {content_name:value for content_name, value in zip(content_layers, content_outputs)} return {'content':content_dict, 'style':style_dict}  style_targets = extract_targets(style_image)['style'] content_targets = extract_targets(content_image)['content']  style_weight = 1e-5 content_weight = 1  This particular choice to style and content weights seems to work for me pretty well. However, you are encouraged to try out different style and content weight combination to dig up more interesting findings.\ndef custom_loss(outputs, batch_size): gen_outputs = outputs['gen'] style_outputs = outputs['style'] # for generated image, get the style content_outputs = outputs['content'] # get content batch_loss = 0 for i in range(batch_size): style_loss = tf.add_n([tf.reduce_mean((style_outputs[name][i]-style_targets[name])**2) for name in style_outputs.keys()]) style_loss *= style_weight / len(style_layers) content_loss = tf.add_n([tf.reduce_mean((content_outputs[name][i]-content_targets[name])**2) for name in content_outputs.keys()]) content_loss *= content_weight / len(content_layers) loss = style_loss + content_loss batch_loss += loss batch_loss /= batch_size return batch_loss  Finally, we define our train_step method, which performs one step of training of the Texture Network. The tf.function decorator used for the function actually converts this function and underlying object to tensorflow graph, based on which we can perform the feed forward pass as well as compute the gradients based on back propagation. Defining this decorator is essential for the training. More details on this is available at Tensorflow website.\nIn the train_step function, we use tf.GradientTape to record the feed forward passes through the network. It works like computing a recording of the feed forward pass through the model, and finally playing it backwards, in order to perform back propagation. This gradient tape records all the gradients happening through the graph, and finally enables us to apply the training rule to update the current value of the parameters using these gradients by apply_gradients function.\n@tf.function() def train_step(content_image, batch_size): with tf.GradientTape() as tape: outputs = tex_net(content_image, batch_size) loss = custom_loss(outputs, batch_size) gradients = tape.gradient(loss, tex_net.trainable_variables) # obtain the gradients recorded by the tape optimizer.apply_gradients(zip(gradients, tex_net.trainable_variables)) # apply the training rule using the gradients to modify the current value of prameters return output, loss  Finally, I used 2500 iterations of update to train the network. I also show the images generated by the network after each 250 iterations, to see how the network improves over training.\nbatch_size = 32 my_content = tf.concat([content_image for _ in range(batch_size)], axis = 0) n_epoch = 10 n_iter = 250 iter_to_show_output = 25 loss_array = [] for epoch in range(n_epoch): msg = 'Epoch: ' + str(epoch) print(msg) os.system('echo ' + msg) for step in range(n_iter): outputs, loss = train_step(my_content, batch_size) if step % iter_to_show_output == 0: os.system('echo loss: ' + str(float(loss))) print('Loss: ', loss) loss_array.append(loss) display.display(tensor_to_image(tex_net(content_image, 1)['gen']))  Epoch: 0 Loss: tf.Tensor(27138224.0, shape=(), dtype=float32) Loss: tf.Tensor(26169034.0, shape=(), dtype=float32) Loss: tf.Tensor(26064738.0, shape=(), dtype=float32) Loss: tf.Tensor(26034120.0, shape=(), dtype=float32) Loss: tf.Tensor(26004590.0, shape=(), dtype=float32) Loss: tf.Tensor(24625778.0, shape=(), dtype=float32) Loss: tf.Tensor(22932934.0, shape=(), dtype=float32) Loss: tf.Tensor(22210894.0, shape=(), dtype=float32) Loss: tf.Tensor(21038836.0, shape=(), dtype=float32) Loss: tf.Tensor(19649052.0, shape=(), dtype=float32)  Epoch: 1 Loss: tf.Tensor(16565710.0, shape=(), dtype=float32) Loss: tf.Tensor(13611194.0, shape=(), dtype=float32) Loss: tf.Tensor(12152441.0, shape=(), dtype=float32) Loss: tf.Tensor(11187487.0, shape=(), dtype=float32) Loss: tf.Tensor(10576777.0, shape=(), dtype=float32) Loss: tf.Tensor(9360093.0, shape=(), dtype=float32) Loss: tf.Tensor(8675145.0, shape=(), dtype=float32) Loss: tf.Tensor(8301968.0, shape=(), dtype=float32) Loss: tf.Tensor(7836261.0, shape=(), dtype=float32) Loss: tf.Tensor(7389797.5, shape=(), dtype=float32)  Epoch: 2 Loss: tf.Tensor(7004705.0, shape=(), dtype=float32) Loss: tf.Tensor(6608028.0, shape=(), dtype=float32) Loss: tf.Tensor(6266657.5, shape=(), dtype=float32) Loss: tf.Tensor(6142354.0, shape=(), dtype=float32) Loss: tf.Tensor(5814227.0, shape=(), dtype=float32) Loss: tf.Tensor(5569461.0, shape=(), dtype=float32) Loss: tf.Tensor(5358932.5, shape=(), dtype=float32) Loss: tf.Tensor(5086491.0, shape=(), dtype=float32) Loss: tf.Tensor(4859027.0, shape=(), dtype=float32) Loss: tf.Tensor(4643354.0, shape=(), dtype=float32)  Epoch: 3 Loss: tf.Tensor(4957266.0, shape=(), dtype=float32) Loss: tf.Tensor(4299568.5, shape=(), dtype=float32) Loss: tf.Tensor(4091998.8, shape=(), dtype=float32) Loss: tf.Tensor(4018616.8, shape=(), dtype=float32) Loss: tf.Tensor(3923811.2, shape=(), dtype=float32) Loss: tf.Tensor(3894964.5, shape=(), dtype=float32) Loss: tf.Tensor(3704813.2, shape=(), dtype=float32) Loss: tf.Tensor(3689166.5, shape=(), dtype=float32) Loss: tf.Tensor(3605529.8, shape=(), dtype=float32) Loss: tf.Tensor(3521357.8, shape=(), dtype=float32)  Epoch: 4 Loss: tf.Tensor(3489987.0, shape=(), dtype=float32) Loss: tf.Tensor(3460784.0, shape=(), dtype=float32) Loss: tf.Tensor(3378974.0, shape=(), dtype=float32) Loss: tf.Tensor(3401102.8, shape=(), dtype=float32) Loss: tf.Tensor(3323982.8, shape=(), dtype=float32) Loss: tf.Tensor(3285243.0, shape=(), dtype=float32) Loss: tf.Tensor(3220874.8, shape=(), dtype=float32) Loss: tf.Tensor(3295160.2, shape=(), dtype=float32) Loss: tf.Tensor(3169093.5, shape=(), dtype=float32) Loss: tf.Tensor(3131125.2, shape=(), dtype=float32)  Epoch: 5 Loss: tf.Tensor(3111150.5, shape=(), dtype=float32) Loss: tf.Tensor(3085978.8, shape=(), dtype=float32) Loss: tf.Tensor(3064475.2, shape=(), dtype=float32) Loss: tf.Tensor(3012352.0, shape=(), dtype=float32) Loss: tf.Tensor(2990475.0, shape=(), dtype=float32) Loss: tf.Tensor(2958282.2, shape=(), dtype=float32) Loss: tf.Tensor(2929994.5, shape=(), dtype=float32) Loss: tf.Tensor(2923794.8, shape=(), dtype=float32) Loss: tf.Tensor(2887351.2, shape=(), dtype=float32) Loss: tf.Tensor(2852276.8, shape=(), dtype=float32)  Epoch: 6 Loss: tf.Tensor(2829871.8, shape=(), dtype=float32) Loss: tf.Tensor(2833075.2, shape=(), dtype=float32) Loss: tf.Tensor(2788235.2, shape=(), dtype=float32) Loss: tf.Tensor(2777346.8, shape=(), dtype=float32) Loss: tf.Tensor(2735421.0, shape=(), dtype=float32) Loss: tf.Tensor(2742302.5, shape=(), dtype=float32) Loss: tf.Tensor(2696765.2, shape=(), dtype=float32) Loss: tf.Tensor(2700100.2, shape=(), dtype=float32) Loss: tf.Tensor(2741282.8, shape=(), dtype=float32) Loss: tf.Tensor(3431759.8, shape=(), dtype=float32)  Epoch: 7 Loss: tf.Tensor(2922398.0, shape=(), dtype=float32) Loss: tf.Tensor(2783216.5, shape=(), dtype=float32) Loss: tf.Tensor(2718686.0, shape=(), dtype=float32) Loss: tf.Tensor(2677556.8, shape=(), dtype=float32) Loss: tf.Tensor(2645984.5, shape=(), dtype=float32) Loss: tf.Tensor(2625516.8, shape=(), dtype=float32) Loss: tf.Tensor(2597460.2, shape=(), dtype=float32) Loss: tf.Tensor(2577406.0, shape=(), dtype=float32) Loss: tf.Tensor(2555875.2, shape=(), dtype=float32) Loss: tf.Tensor(2537895.5, shape=(), dtype=float32)  Epoch: 8 Loss: tf.Tensor(2528999.2, shape=(), dtype=float32) Loss: tf.Tensor(2511399.5, shape=(), dtype=float32) Loss: tf.Tensor(2496434.2, shape=(), dtype=float32) Loss: tf.Tensor(2484290.0, shape=(), dtype=float32) Loss: tf.Tensor(2472602.2, shape=(), dtype=float32) Loss: tf.Tensor(2465217.5, shape=(), dtype=float32) Loss: tf.Tensor(2449646.8, shape=(), dtype=float32) Loss: tf.Tensor(2445076.5, shape=(), dtype=float32) Loss: tf.Tensor(2431088.0, shape=(), dtype=float32) Loss: tf.Tensor(2422472.8, shape=(), dtype=float32)  Epoch: 9 Loss: tf.Tensor(2416303.5, shape=(), dtype=float32) Loss: tf.Tensor(2413389.5, shape=(), dtype=float32) Loss: tf.Tensor(2430066.5, shape=(), dtype=float32) Loss: tf.Tensor(3215110.2, shape=(), dtype=float32) Loss: tf.Tensor(2591146.2, shape=(), dtype=float32) Loss: tf.Tensor(2484897.5, shape=(), dtype=float32) Loss: tf.Tensor(2440708.5, shape=(), dtype=float32) Loss: tf.Tensor(2416797.5, shape=(), dtype=float32) Loss: tf.Tensor(2400430.2, shape=(), dtype=float32) Loss: tf.Tensor(2389489.8, shape=(), dtype=float32)  Note how the generated image becomes better and better with more iteration, by mimicking the style of tessellation on our image of AI generated celebrity. But the image does not change much after epoch 5, except the colour gets more violet-ish rather than blue-ish. However, you can manipulate the style weights and content weights properly, in order to have a good balance between the content and style. Also, if we take a look at the loss function, we see that the loss was decreasing rapidly at the beginning, and finally it has more or less stabilized at a point where it cannot be lowered further by much. Hence, by looking at the loss function, it seems the generator is trained enough to meet its capacities.\nplt.plot(loss_array) plt.title('Loss Function over time') plt.show()  From the generation the final stylized image, it seems as if the image of the celebrity is properly tessellated, as we desired. However, it is mostly blue, and there are some funny artifacts and the colour of the skin appearing at the forehead area. This can possibly be resolved by carefully trying out different style and content layer repesentations, as well as adding a variational loss to the custom loss function.\ntensor_to_image(tex_net(content_image, 1)['gen'])  Useful References  Texture Networks: Feed-forward Synthesis of Textures and Stylized Images - Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, Victor Lempitsky. https://arxiv.org/abs/1603.03417. https://www.tensorflow.org/tutorials/generative/style_transfer A Neural Algorithm of Artistic Style - Leon A. Gatys, Alexander S. Ecker, Matthias Bethge. https://arxiv.org/abs/1508.06576. https://github.com/DmitryUlyanov/texture_nets Perceptual Losses for Real-Time Style Transfer and Super-Resolution - Justin Johnson, Alexandre Alahi, Li Fei-Fei. https://arxiv.org/abs/1603.08155. https://en.wikipedia.org/wiki/Circular_convolution.  Thank you very much for reading! If you find it interesting, please consider sharing! ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578873600,"objectID":"d084acea3d1051f530a6486b095c70b4","permalink":"/post/post3/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/post/post3/","section":"post","summary":"This is a rather long post, but here we shall about something really interesting, about how to mix the content of a picture with the style from another picture, called Neural Style Transfer.","tags":null,"title":"Texture Networks","type":"post"},{"authors":null,"categories":null,"content":"","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"ba83eb199db0b8b37eef13c675a5bd62","permalink":"/project/seq2seq-chatbot/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/project/seq2seq-chatbot/","section":"project","summary":"A simple Chatbot with Seq2Seq model with Bahadau Attention Mechanism trained on Cornell Movie Corpus dataset.","tags":["Deep Learning","Text Analysis"],"title":"Chatbot with Seq2Seq model","type":"project"},{"authors":null,"categories":null,"content":"This is an introductory notes for learning Integral Calculus and some useful tricks to perform integration at Pre-College level.\nYou can find the notes by clicking here.\nDid you find this page helpful? Consider sharing it! 😄 ","date":1577145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577145600,"objectID":"0c6f6cbe96ac9b1239a629e09477d6d1","permalink":"/post/post2/","publishdate":"2019-12-24T00:00:00Z","relpermalink":"/post/post2/","section":"post","summary":"This is an introductory notes for learning Integral Calculus and some useful tricks to perform integration at Pre-College level.\nYou can find the notes by clicking here.\nDid you find this page helpful? Consider sharing it! 😄 ","tags":null,"title":"Introduction to Integral Calculus for 10+2 Level","type":"post"},{"authors":null,"categories":null,"content":"","date":1576195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576195200,"objectID":"fe859c6266641600992fd6a0d6f11947","permalink":"/post/post1/","publishdate":"2019-12-13T00:00:00Z","relpermalink":"/post/post1/","section":"post","summary":"","tags":null,"title":"D Basu Memorial Presentation on t-SNE: A way to Visualize Multidimensional Dataset","type":"post"},{"authors":null,"categories":null,"content":"  table { display: table; }  Background Mount Rainier, also known as Tahoma and Tacoma is a large active Stratovolcano located 59 miles south south-east of Seattle, Washington in United States of America. With a summit elevation of 14,411 ft (4,392 m), it is the highest Mountain peak of state Washington, the Cascade range; and most topographically prominent peak of the contiguous United States (i.e., leaving out Alaska, Hawaii and islands).\nDue to being a topographically prominent peak, mountainiers face a lot of struggle to climb the peak, as they have to ascend a higher slope of elevation. Also, they are faced with other challenges;\n Climbing requires traversing through the largest glacier of United States. Climbing takes 2 or 3 days, with very high failure rates. Weather and physical condition of the routes used for climbing poses a great challenge. Also, weather variables are very erratic in nature. Climbing team requires high-end professionals with experience of Glacier climbing. Climbers need to take permit by the law to start their journey.  Objective The objective of the study is to analyze patterns in climber\u0026rsquo;s traffic, as well as model the success proportion based on current weather scenario and some relevant covariates.\nDataset The dataset was obtained from Kaggle.\n Timeperiod of the dataset is 25th September, 2014 to 27th November, 2017.\n There are three main routes for climbing Mt. Rainier.\n Disappointment Cleaver (Why the name Disappointment comes in, nobody knows for sure, however, one popular theory is people often gets disappointed for high rate of failure in this route. But is it True?), is the main route to climb Mt. Rainier. Emmons Winthrop. Kautz Glacier. (The last two routes are only opened where Disappointment cleaver gets closed)  Some of the weather covariates are Temperature, Humidity, Wind Speed, Solar Radiation etc.\n Climbing statistics include Team size, Number of Succeeded etc.\n  How Weather Changes over Time It seems that weather covariates are really erratic in nature.\nHow Climber\u0026rsquo;s Traffic Changes over time    Year \u0026amp; Month Total Number of Teams Average Team Size     2014-09 22 6.68   2014-10 3 1.33   2015-01 1 2   2015-03 10 4.3   2015-04 17 3.76   2015-05 163 5.50   2015-06 514 5.47   2015-07 522 5.38   2015-08 325 5.89   2015-09 102 7.58   2015-10 6 3.33   2015-11 2 2.5    It seems that in the time of late summer, a lot of teams attempt the climbing, as well as the average team size also increases. Also, during winters, the climbers traffic remains almost negligible (Consider November \u0026amp; December of 2014).\nTherefore, in our predictive model, a periodic component for the whole year should be added. Let $t$ denote the current date and let $t_0$ be the Janurary 1 of the same year, then we conisder the term,\n$$\\gamma_1 \\sin\\left(\\frac{\\pi}{365} (t - t_0)\\right) + \\gamma_2 \\cos\\left(\\frac{\\pi}{365} (t - t_0)\\right)$$\nin our generalized linear model.\nPredictive Analysis Logistic Regression The very simple, yet, sometimes very effective tool for modeling the success rate would be the use of a logistic regression model. So, we have the following model for our data;\n$$\\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\alpha_0 + \\alpha_j + \\sum_k \\beta_k X_{ik} + \\gamma_1 \\sin\\left(\\frac{\\pi}{365} (t - t_0)\\right) + \\gamma_2 \\cos\\left(\\frac{\\pi}{365} (t - t_0)\\right)$$\nwhere $p_i$ is the true proportion of successes for $i$-th sample, $\\alpha_j$ is the effect of $j$-th route used for climbing, while $X_{ik}$\u0026rsquo;s are the weather covariates which affects the success rate. Fitting such a simple logistic regression model in R is fairly easy, using glm function in stats package.\nThe above violin plot indicates the distribution of predicted values of successes, conditonal on the true number of successes. The green line is a reference $x = y$ line, deviation from which indicates inaccuracy of the prediction model. It seems that a logistic regression, although it performs greatly when true number of successes is low, it performs poorly when success rate is high. Hence, in a sense, it underestimate the success proportion.\nSome More Exploration To find out why logistic regression fails miserably, we perform some more exploratory analysis. Exploration of the data is always useful, it gives insights about approaching the statistical problem at hand.\nThe first plot shows the histogram of Number of Attempts (i.e. the Team Size) over different Routes. It shows that there is a peak at $1$ and $12$, and a slight peak at $6-7$. It indicates that there are 3 types of teams which goes for the summit.\n Solo \u0026amp; Duo. Medium size groups. Large tourist groups for expedition.  This groups make a lot of sense, and should be taken into account when performing the predictive analysis.\nThe next figures will comprises of histograms for the Number of Succedded people in each team, conditioned on the size of the team.\nThe above figures depict an interesting fact, in each team, it is more likely that either eveyone fails or everyone climbs the mountain, than some of the team members failed and others claimed the summit. It would be very inhuman thing to leave your fellow team members in that harsh condition of the mountain, and climbing for the summit by yourself, Nah! nobody would want to do that.\nHowever, from statistical perspective, this yields a difficulty, we cannot model this by Binomial distribution as we did in the logistic regression, as Binomial distribution certainly does not look like this.\nMixture of Poissons and ZNIB Now that we have understood some basic mechanisms of the climbers traffic, let us model the Team size first. Let $N_i$ denote the team size of $i$-th sample. Then,\n$$N_i = p_{i1}N_{i1} + p_{i2}N_{i2} + (1 - p_{i1} - p_{i2})N_{i3}$$\nwhere $N_{ik} \\sim \\text{Poisson}(\\lambda_k)$. Clearly, $N_i$ would be a mixture of $3$ count variables, each of which represents the type of Team we explored in previous part. To model this using covariates, we use logit model for $p_{ik}$\u0026rsquo;s while we use poisson regression with log link function to model $N_{ik}$. We can use flexmix package in R to perform this fitting for us. Here, concomitant variables are those covariates which determine the mixing probabilities in the model.\nfit.cv \u0026lt;- stepFlexmix(Attempted ~ MainRoute + sin((pi / 365) * TransDate) + cos((pi / 365) * TransDate) + `Battery Voltage AVG` + `Temperature AVG` + `Relative Humidity AVG` + `Wind Speed Daily AVG` + `Wind Direction AVG` + `Solare Radiation AVG`, data = data, model = FLXglmFix(family = \u0026quot;poisson\u0026quot;), concomitant = FLXPmultinom( ~ MainRoute + sin((pi / 365) * TransDate) + cos((pi / 365) * TransDate) + `Battery Voltage AVG` + `Temperature AVG` + `Relative Humidity AVG` + `Wind Speed Daily AVG` + `Wind Direction AVG` + `Solare Radiation AVG`), k = 2:4, nrep = 3) fit \u0026lt;- getModel(fit.cv, \u0026quot;BIC\u0026quot;) # we use BIC criterion for Model selection x \u0026lt;- refit(fit) x@components x@concomitant  The weather covariates turn out to be not significant in the log-linear means of any component, also not in the mixture component as well, but the routes and time components seem significant. This is intuitive since the team is decided long before the journey actually takes place, hence the team size should not depend on the particular weather on the day of journey.\nThe prediction for the team size also seems much better from the plot below, which shows the distribution of $\\mathbb{E}(N_i)$ as well as the distribution of upper limit and lower limit of an asymptotic confidence interval for $N_i$.\nTo model the success proportion, we consider modelling the conditional distribution of succeeded given the size of the team. Clearly, as shown before, a binomial modelling would turn into ashes as we have peaks in histogram at $0$ and $N_i$. Therefore, we model the number of succeeded;\n$$Y_i | N_i \\sim q_{i1}\\mathbb{I}_{(Y_i = 0)} + q_{i2}\\mathbb{I}_{(Y_i = N_i)} + (1 - q_{i1} - q_{i2}) \\text{Bin}(N_i, p_i)$$\nwhere $\\mathbb{I}_{(Y_i = c)}$ denotes the distribution degenerate at $c$. Therefore, the model that we have is a Zero and N Inflated Binomial (ZNIB) model. Clearly, as before we model the $q_{ij}$\u0026rsquo;s using simple logistic model, and binomial probabilities using another logistic model.\nSince the model is fairly new, there is no package (that I know of) which performs the fitting of the above model. So, we write our own code for likelihood maximization, using optim function in stats package.\ninflbinom \u0026lt;- function(form, data, sizes) { response \u0026lt;- model.response(model.frame(formula(form), data)) modelmat \u0026lt;- model.matrix(formula(form), data) print(ncol(modelmat)) loglike \u0026lt;- function(params) { # params are of three types # 1 to ncol(modelmat), ncol(modelmat) + 1 to 2*that, and so on eta1 \u0026lt;- modelmat %*% params[1:ncol(modelmat)] eta2 \u0026lt;- modelmat %*% params[(ncol(modelmat) + 1):(2 * ncol(modelmat))] eta3 \u0026lt;- modelmat %*% params[(2 * ncol(modelmat) + 1):(3 * ncol(modelmat))] m \u0026lt;- max(eta1, eta2, eta3) eta1 \u0026lt;- eta1 - m + 10 eta2 \u0026lt;- eta2 - m + 10 eta3 \u0026lt;- eta3 - m + 10 binom.prob \u0026lt;- (exp(eta3)/(1 + exp(eta3))) zeroinf.prob \u0026lt;- (exp(eta1)/(1 + exp(eta1) + exp(eta2))) + ((1/(1 + exp(eta1) + exp(eta2))) * dbinom(0, size = sizes, prob = binom.prob)) endinf.prob \u0026lt;- (exp(eta2)/(1 + exp(eta1) + exp(eta2))) + ((1/(1 + exp(eta1) + exp(eta2))) * dbinom(sizes, size = sizes, prob = binom.prob)) loglikelihood \u0026lt;- sum(log(zeroinf.prob[response == 0])) + sum(log(endinf.prob[response == sizes])) + sum(as.numeric(response \u0026gt; 0 \u0026amp; response \u0026lt; sizes) * (dbinom(response, size = sizes, prob = binom.prob, log = T) + log((1/(1 + exp(eta1) + exp(eta2)))))) return(loglikelihood) } return(loglike) } predict.inflbinom \u0026lt;- function(form, data, sizes, op) { modelmat \u0026lt;- model.matrix(formula(form), data) eta1 \u0026lt;- modelmat %*% op$par[1:ncol(modelmat)] eta2 \u0026lt;- modelmat %*% op$par[(ncol(modelmat) + 1):(2 * ncol(modelmat))] eta3 \u0026lt;- modelmat %*% op$par[(2 * ncol(modelmat) + 1):(3 * ncol(modelmat))] eta1 \u0026lt;- eta1 - max(eta1) + 10 eta2 \u0026lt;- eta2 - max(eta2) + 10 eta3 \u0026lt;- eta3 - max(eta3) + 10 binom.prob \u0026lt;- (exp(eta3)/(1 + exp(eta3))) zeroinf.prob \u0026lt;- exp(eta1)/(1 + exp(eta1) + exp(eta2)) endinf.prob \u0026lt;- exp(eta2)/(1 + exp(eta1) + exp(eta2)) other.prob \u0026lt;- (1 - zeroinf.prob - endinf.prob) preds \u0026lt;- numeric(length(sizes)) for (i in 1:length(sizes)) { x \u0026lt;- dbinom(0:sizes[i], size = sizes[i], prob = binom.prob[i]) * other.prob[i] preds[i] \u0026lt;- (which.max(x) - 1) } return(preds) } test.inflbinom \u0026lt;- function(form, op) { fisher.mat \u0026lt;- solve(op$hessian) std_err \u0026lt;- sqrt(-diag(fisher.mat)) estimate \u0026lt;- op$par z_value \u0026lt;- estimate / std_err p_value \u0026lt;- 2*pnorm(-abs(z_value)) signif.codes \u0026lt;- stars.pval(p_value) coefs \u0026lt;- colnames(model.matrix(formula(form), traindata)) zeroinf.df \u0026lt;- data.frame(coefs, estimate[1:length(coefs)], std_err[1:length(coefs)], z_value[1:length(coefs)], p_value[1:length(coefs)], signif.codes[1:length(coefs)]) binom.df \u0026lt;- data.frame(coefs, estimate[(length(coefs)+1):(2*length(coefs))], std_err[(length(coefs)+1):(2*length(coefs))], z_value[(length(coefs)+1):(2*length(coefs))], p_value[(length(coefs)+1):(2*length(coefs))], signif.codes[(length(coefs)+1):(2*length(coefs))]) endinf.df \u0026lt;- data.frame(coefs, estimate[(2*length(coefs)+1):(3*length(coefs))], std_err[(2*length(coefs)+1):(3*length(coefs))], z_value[(2*length(coefs)+1):(3*length(coefs))], p_value[(2*length(coefs)+1):(3*length(coefs))], signif.codes[(2*length(coefs)+1):(3*length(coefs))]) colnames(zeroinf.df) \u0026lt;- c(\u0026quot;\u0026quot;,\u0026quot;Estimate\u0026quot;,\u0026quot;Std.Error\u0026quot;,\u0026quot;Z-value\u0026quot;,\u0026quot;p-value\u0026quot;,\u0026quot;Signif.codes\u0026quot;) colnames(binom.df) \u0026lt;- c(\u0026quot;\u0026quot;,\u0026quot;Estimate\u0026quot;,\u0026quot;Std.Error\u0026quot;,\u0026quot;Z-value\u0026quot;,\u0026quot;p-value\u0026quot;,\u0026quot;Signif.codes\u0026quot;) colnames(endinf.df) \u0026lt;- c(\u0026quot;\u0026quot;,\u0026quot;Estimate\u0026quot;,\u0026quot;Std.Error\u0026quot;,\u0026quot;Z-value\u0026quot;,\u0026quot;p-value\u0026quot;,\u0026quot;Signif.codes\u0026quot;) return(list(\u0026quot;Zero Inflated Model\u0026quot;=zeroinf.df, \u0026quot;Binomial Model\u0026quot; = binom.df, \u0026quot;End Inflated Model\u0026quot; = endinf.df)) }  Here, inflbinom function takes the formula, data and the conditonal sizes which it uses to create the loglikelihood function for the model. predict.inflbinom function takes the formula, data, conditional sizes and the list that contains optimized parameter values (i.e. output of optim function). test.inflbinom function uses the hessian matrix output from optim function to compute Fisher information matrix, and uses that to perform Wald\u0026rsquo;s asymptotic test for significance of corresponding coeffcient.\nform \u0026lt;- \u0026quot;Succeeded ~ MainRoute + sin((pi / 365) * TransDate) + cos((pi / 365) * TransDate) + `Battery Voltage AVG` + `Temperature AVG` + `Relative Humidity AVG` + `Wind Speed Daily AVG` + `Wind Direction AVG` + `Solare Radiation AVG`\u0026quot; modelfun \u0026lt;- inflbinom(form, data, sizes = data$Attempted) set.seed(47) op \u0026lt;- optim(runif(11*3), fn = modelfun, control = list(fnscale = -1, maxit = 1e+06), hessian = T) op$convergence op$par op$value  The above code performs the maximization of the likelihood. Finally, we get the following output.\nIt seems that weather conditions are now significant. The prediction also seems a lot better.\nConclusions  The time of the year is significant in team size, but not the particular weather of the journey day. Thus an overall idea of seasons factors and the route to take decides team size traffic.\n The team size distribution would help the climbing industry to provide better service to climbers.\n However, the success proportions do depend on the weather covariates. This dependence is significant not only in the $0$ and End components, but also in the Binomial component of the success proportions.\n Thus, based on the current weather conditions, the model should help climbers to take decision whether to attempt climbing or not.\n  ","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"37bbcc0b233003f42372218496dd7d25","permalink":"/project/mount-rainier/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/project/mount-rainier/","section":"project","summary":"Regression Analysis of Kaggle Data on Mt. Rainier Climbing Statistics","tags":["Regression"],"title":"Analysis of Mount Rainier Climbing Data","type":"project"},{"authors":["Dr. Diganta Mukherjee","Abhinandan Dalal","Subhrajyoty Roy"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- Tea Auctions across India occur as an ascending open auction, conducted online. Before the auction, a sample of the tea lot is sent to potential bidders, and a group of tea-testers. The seller’s reserve price is a confidential function of the tea-tester’s valuation, which also acts as a signal to the bidders. In this paper, we work with the dataset from a single tea auction house, J Thomas, of tea dust category, on 49 weeks in the time span of 2018-2019, with the following objectives in mind:\n Objective classification of the various categories of tea dust into a more manageable, and robust classification of the tea dust, based on source and grades. Predict which tea lots would be sold in the auction market, and a model for the final price conditioned on sale. To study the distribution of price and ratio of the sold tea auction lots and Discussion on the possibility of automation of the process without human intervention.  More details about the paper is available here.\n","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"88bf357e1a78347e78f3c1867aa3f412","permalink":"/publication/tea-auction-feasibility/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/tea-auction-feasibility/","section":"publication","summary":"This article examines Tea Auctions in India and forms an idea about the valuations that occur for several of the tea grades, and endeavoured to fit the model determining the final transaction price of the auction based on several relevant factors. The article also looks into the significance of manual valuations as predictors for the final price, and discussed several aspects to do away with the practice of manual valuations.","tags":["Auction theory","Gaussian Mixture Model","Mixture of Regression","Clustering","Price Modelling"],"title":"Feasibility of Transparent Price Discovery in Tea through Auction in India","type":"publication"},{"authors":["Ritwik Bhaduri","Soham Bonnerjee","Subhrajyoty Roy"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   -- Supplementary materials can be found here including the code and datasets used.\n","date":1566345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566345600,"objectID":"cb7823180fe478c3edba8283fa9ab119","permalink":"/publication/onset-detection-arxiv/","publishdate":"2019-08-21T00:00:00Z","relpermalink":"/publication/onset-detection-arxiv/","section":"publication","summary":"We develop a QBH system based on detection of onsets of a song, within a statistical framework.","tags":["Sound processing","Subset matching"],"title":"Onset detection - A new approach to QBH system","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Pramit Das","Subhrajyoty Roy"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- More details about the paper is available here.\n","date":1547078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547078400,"objectID":"1e5e9235ae9b20af955b08efac3cefbd","permalink":"/publication/us-china-trade-war/","publishdate":"2019-01-10T00:00:00Z","relpermalink":"/publication/us-china-trade-war/","section":"publication","summary":"A study has been undertaken to investigate the effect of US-Chinese trade war on the returns of large tech stocks based on both these countries. This helps us to scientifically explain the reason of steady growth in US stocks, and the downfall of major Chinese tech stocks, with the support of past data on these stocks.","tags":["Time series","Stock market analysis","Finance"],"title":"Analysing Impact of US – China Trade War on Large Tech Stocks","type":"publication"},{"authors":null,"categories":null,"content":"  table { display: table; }  The application is built as a part of the capstone project for the Data Science specialization of Coursera provided by John Hopkins Bloomberg School of Public Health in collaboration with Swiftkey.\nBackground The original dataset used to build the application has been provided by Swiftkey in three separate text files, - en-UN-blogs.txt - en-UN-news.txt - en-UN-twitters.txt, which contains a large number of english sentences from blogs, news and tweets from twitters respectively. Since, the raw corpus is huge, it has been cleaned to remove url, email, non-ASCII characters and profanity filter has also been performed using parallel programming. However, for building the prediction model, only a sample of $25,000$ text paragraphs is taken from each of the three files, which comprises of about a million sentences together. Usage of more data than this sufficiently increases the time to give a prediction and creates poor user experience with the application.\nAbout the Dataset The corpus is available in the following url: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip, We use R to download and unzip the corpus text files for us.\nurl \u0026lt;- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip' if (!file.exists('Coursera-SwiftKey.zip')){ download.file(url, destfile = \u0026quot;Coursera-SwiftKey.zip\u0026quot;, method = \u0026quot;curl\u0026quot;) } unzip('Coursera-SwiftKey.zip', exdir = \u0026quot;.\u0026quot;)  Basic Summeries Now that we have our corpus downloaded and extracted, we should check some basic summaries like sizes of the corpus text files, number of sentences etc.\nCorpusNames \u0026lt;- c('en_US.blogs', 'en_US.news', 'en_US.twitter') sizes \u0026lt;- numeric(3) sizes[1] \u0026lt;- file.info('final/en_US/en_US.blogs.txt')$size/(2^20) #so that it shows in mb sizes[2] \u0026lt;- file.info('final/en_US/en_US.news.txt')$size/(2^20) sizes[3] \u0026lt;- file.info('final/en_US/en_US.twitter.txt')$size/(2^20) # Read blogs data con \u0026lt;- file('./final/en_US/en_US.blogs.txt') blogdata \u0026lt;- readLines(con); close(con) # Read news data con \u0026lt;- file('./final/en_US/en_US.news.txt') newsdata \u0026lt;- readLines(con); close(con) # Read twitter data con \u0026lt;- file('./final/en_US/en_US.twitter.txt'); twitterdata \u0026lt;- readLines(con); close(con) lengths \u0026lt;- sapply(list(blogdata, newsdata, twitterdata), length) Chars \u0026lt;- sapply(list(blogdata, newsdata, twitterdata), function(x){sum(nchar(x))}) Sentences \u0026lt;- sapply(list(blogdata, newsdata, twitterdata), function(x){sum(stri_count_boundaries(x,type = \u0026quot;sentence\u0026quot;))}) Words \u0026lt;- sapply(list(blogdata, newsdata, twitterdata), function(x){sum(stri_count_words(x))}) data.frame(Corpus = CorpusNames, SizeinMB = sizes, Length = lengths, NumberofSenteces = Sentences, NumberofWords = Words, NumberofCharacters = Chars)     Corpus SizeinMB Length NumberofSenteces NumberofWords NumberofCharacters     en_US.blogs 200.4242 899288 2380481 37546246 206824505   en_US.news 196.2775 1010242 2025776 34762395 203223159   en_US.twitter 159.3641 2360148 3780372 30093369 162096031    Sampling the Dataset Since, the corpus is large in size, so we select 5% texts from each of the three files, and merge them together to form a corpus of reasonable size, using which we can perform related analysis and development of the text prediction system.\nset.seed(19102018) #set a seed for reproducbility blogsamples \u0026lt;- blogdata[sample(length(blogdata), size = (0.05*length(blogdata)), replace = FALSE)] #sample the blog data newssamples \u0026lt;- newsdata[sample(length(newsdata), size = (0.05*length(newsdata)), replace = FALSE)] #sample the news data twittersamples \u0026lt;- twitterdata[sample(length(twitterdata), size = (0.05*length(twitterdata)), replace = FALSE)] #sample the twitter data  Now that we have sampled our texts, we need to merge these three samples together to form our corpus.\ncorpus \u0026lt;- c(blogsamples, newssamples, twittersamples) #make the corpus  Cleaning the Corpus To clean the text, we shall first make every text to lowercase letters so that no ambiguity arises in case of other transformations. We should be concerned with the following steps in order to clean the data.\n Remove optional spaces between different words. Perform a profanity filtering. For this, we shall use the words banned by Google for profanity filtering. The list of words are here Remove any URL occuring in the text. Remove any appearance of abbreviations. Remove any appearance of punctuation marks. Remove any repeated words. Remove any appearance of digits. Remove any NonASCII character.\nurl \u0026lt;- 'https://raw.githubusercontent.com/RobertJGabriel/Google-profanity-words/master/list.txt' #download the profanity corpus if (!file.exists('bad-words.txt')){ # if the file does not exist, then we download it download.file(url, destfile = \u0026quot;bad-words.txt\u0026quot;, method = \u0026quot;curl\u0026quot;) } con \u0026lt;- file('./bad-words.txt') badwords \u0026lt;- readLines(con); close(con) corpus \u0026lt;- iconv(corpus, \u0026quot;latin1\u0026quot;, \u0026quot;ASCII\u0026quot;, sub=\u0026quot;\u0026quot;) #remove nonASCII characters corpus \u0026lt;- gsub(\u0026quot;\\\\s+\u0026quot;,\u0026quot; \u0026quot;,corpus) ## Removing optional space corpus \u0026lt;- tolower(corpus) ## lowercasing the letters for (badword in badwords){ corpus \u0026lt;- gsub(paste0(\u0026quot;\\\\s\u0026quot;,badword,\u0026quot;\\\\s\u0026quot;), \u0026quot;\u0026quot;, corpus) message(badword) #just for checking } #remove all bad words that appears corpus \u0026lt;- gsub(\u0026quot;http[[:alnum:]]*\u0026quot;,\u0026quot;\u0026quot;,corpus) #remove any URL corpus \u0026lt;- gsub(\u0026quot;([a-z]\\\\.){2,}\u0026quot;,\u0026quot;\u0026quot;, corpus) #remove any abbreviations corpus \u0026lt;- gsub(\u0026quot;[[:digit:]]\u0026quot;,\u0026quot;\u0026quot;, corpus) #remove any digits corpus \u0026lt;- gsub(\u0026quot;(\\\\w+\\\\s)\\\\1+\u0026quot;,\u0026quot;\\\\1\u0026quot;, corpus) #remove the repeated word corpus \u0026lt;- gsub(\u0026quot;[[:punct:]]\u0026quot;, \u0026quot;\u0026quot;, corpus) #remove any punctuation marks   Now that our corpus is cleaned, we save it in a text file for future references.\ncon \u0026lt;- file('./cleaned-corpus.txt') writeLines(corpus, con); close(con)  Building ist of frequent N-Grams in the Corpus We compute the list of the most frequent N-Grams in the corpus. This will serve as a basis for the text based prediction model. For this purpose, we are using ngram library which performs fast n-gram tokenization. We present the top 10 frequent n-grams for n=1,2,3,4 and 5 and visualize the corresponding bar diagram.\nNow, there might be some sentences remaining which has less than 3 words. These sentences clearly would not provide a good corpus for text prediction using n-grams.\nnwords \u0026lt;- stri_count_words(corpus) #count the number of words print(sum(nwords \u0026lt; 3)/length(nwords))  We see that this makes us to remove about 3% of the corpus.\ncorpus \u0026lt;- corpus[nwords\u0026gt;=3] #keep those corpus texts where there are at least 3 words.  1-Gram or Words ng \u0026lt;- ngram(corpus, n=1) #get the one_grams ng_table \u0026lt;- head(get.phrasetable(ng), 10) #get top 10 entries knitr::kable(ng_table) barplot(ng_table$freq, names.arg = ng_table$ngrams, las = 2, ylab = \u0026quot;Frequency\u0026quot;, main = \u0026quot;Bar Diagram of Word frequency in the Corpus\u0026quot;)  2-Gram or Bigrams ng \u0026lt;- ngram(corpus, n=2) #get the bi_grams ng_table \u0026lt;- head(get.phrasetable(ng), 10) #get top 10 entries knitr::kable(ng_table) barplot(ng_table$freq, names.arg = ng_table$ngrams, las = 2, ylab = \u0026quot;Frequency\u0026quot;, main = \u0026quot;Bar Diagram of Bigram frequency in the Corpus\u0026quot;)  For trigrams and longer n-grams, although the code generates barplots, only the frequency tables are provided.\n3-Gram or Trigrams ng \u0026lt;- ngram(corpus, n=3) #get the tri_grams ng_table \u0026lt;- head(get.phrasetable(ng), 10) #get top 10 entries knitr::kable(ng_table) barplot(ng_table$freq, names.arg = ng_table$ngrams, las = 2, ylab = \u0026quot;Frequency\u0026quot;, main = \u0026quot;Bar Diagram of Trigram frequency in the Corpus\u0026quot;)     ngrams freq prop     One of the 1651 0.0003643   A lot of 1426 0.0003146   Thanks for the 1177 0.0002597   To be a 925 0.0002041   Going to be 906 0.0001999   The end of 770 0.0001699   Out of the 723 0.0001595   I want to 721 0.0001591   It was a 693 0.0001529   Some of the 689 0.0001520    4-gram or Tetragrams nwords \u0026lt;- stri_count_words(corpus) ng \u0026lt;- ngram(corpus[nwords\u0026gt;=4], n=4) #get the 4_grams ng_table \u0026lt;- head(get.phrasetable(ng), 10) #get top 10 entries knitr::kable(ng_table) barplot(ng_table$freq, names.arg = ng_table$ngrams, las = 2, ylab = \u0026quot;Frequency\u0026quot;, main = \u0026quot;Bar Diagram of Tetragram frequency in the Corpus\u0026quot;)     ngrams freq prop     The end of the 409 9.45e-05   The rest of the 335 7.74e-05   At the end of 327 7.56e-05   Thanks for the follow 300 6.94e-05   For the first time 272 6.29e-05   At the same time 256 5.92e-05   When it comes to 228 5.27e-05   One of the most 201 4.65e-05   To be able to 200 4.62e-05   Is going to be 199 4.60e-05    5-gram or Pentagrams ng \u0026lt;- ngram(corpus[nwords\u0026gt;=5], n=5) #get the 5_grams ng_table \u0026lt;- head(get.phrasetable(ng), 10) #get top 10 entries knitr::kable(ng_table) barplot(ng_table$freq, names.arg = ng_table$ngrams, las = 2, ylab = \u0026quot;Frequency\u0026quot;, main = \u0026quot;Bar Diagram of Pentagram frequency in the Corpus\u0026quot;)     ngrams freq prop     At the end of the 190 4.60e-05   For the first time in 82 1.99e-05   In the middle of the 80 1.94e-05   The end of the day 78 1.89e-05   By the end of the 63 1.53e-05   Thank you so much for 63 1.53e-05   For the rest of the 59 1.43e-05   Its going to be a 59 1.43e-05   There are a lot of 53 1.28e-05   Happy mothers day to all 52 1.26e-05    Goals for Text Prediction Application  Using a basic 5-gram model to predict the next word based on the previous three words. Including 2-gram, 3-gram and 4-gram models at the initial level. Create larger corpus sizes in order to assess better accuracy. Include prediction of punctuation marks along with the word. Store the n-gram objects efficiently to be used on light-weight devices. Optimize the running time of the prediction algorithm and locate the bottlenecks of the code for efficiency in speed.  Prediction Algorithm The prediction algorithm used to build the application is extremely simple, the usage of an ngram model (wiki link here). For this purpose, ngram package has been used. Ngram is the n consecutive words that appears in the corpus, for example, one-gram or unigram is the normal words, while bigram is the pharases of two words appearing consecutively (like for the, I am, can not, to be etc. ). To build the application, unigrams, bigrams, trigrams and quadgrams have been computed from the sampled and cleaned corpus. Each type of n-grams is stored in a named vector sorted according the frequency of occurence in order to redcue memory usage.\nTo predict the next words, the algorithm takes the input string and the parses the last three words (if available). Then it tries to find out those quadgrams which has those three words appearing first, and from them, a sample quadgram is chosen according to the probability proportional to its frequency. This reduces the probability of entering a loophole using the common words. Also, if the last three words are not available, then we use last two or less words, using trigrams or bigrams model if required.\nFuture Prospects  With the availablity of sufficient storage and computational power, this application can be made a lot more powerful. Inclusion of whole corpus may be possible, which would significantly boost the accuracy of prediction. We may include smileys and emoticons along with prediction of the text. Currently, this algorithm does not take account for the punctuation marks, which is important for prediction of next words. Efficiently storing the ngrams so that the application becomes smaller in size and can be deployed to smartphones. Extending this model to different languages.  Acknowledgements I would like to thank Swiftkey for providing this extremely valuable dataset which is the base of the application. I also thank Coursera for putting up this amazing specialization offered by JHU on an online platform. I thank my mentors for the specialization Jeff Leek, Roger Peng and Brian Caffo for guiding step by step thorugh this capstone project. Finally, I would like to extend my thanks and regards to my fellow classmates in this course who were a great help in clearing doubts in discussion forums.\n The Application is available at shinyapps.io.   ","date":1542067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542067200,"objectID":"31bcb58fb0fd1d059c91d1873c70305a","permalink":"/project/text-pred-app/","publishdate":"2018-11-13T00:00:00Z","relpermalink":"/project/text-pred-app/","section":"project","summary":"A simple text prediction application built for Data Science Specialization in Coursera.","tags":["Text Analysis","Machine Learning"],"title":"A simple Text Prediction Application","type":"project"}]