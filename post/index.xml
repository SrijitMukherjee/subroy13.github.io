<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Subhrajyoty Roy</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© All rights reserved for contents only, 2019</copyright><lastBuildDate>Wed, 15 Apr 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Application of tf-idf in extracting meaningful words</title>
      <link>/post/post8/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/post8/</guid>
      <description>

&lt;iframe
       src=&#34;./post8.html&#34;
       width=&#34;100%&#34;
       height=&#34;1000px&#34;
       style=&#34;border:none;&#34;&gt;
&lt;/iframe&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tidytextmining.com/&#34; target=&#34;_blank&#34;&gt;Text Minining in R&lt;/a&gt; - A Tidy Approach by Julia Silge and David Robinson (2020-03-07)&lt;/li&gt;
&lt;li&gt;Package Vignette: &lt;a href=&#34;https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html&#34; target=&#34;_blank&#34;&gt;gutenbergr: Search and download public domain texts from Project Gutenberg&lt;/a&gt;  by David Robinson (2019-09-10)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>eSIR - An Epidemiological Model incoportating quarantining effect</title>
      <link>/post/post7/</link>
      <pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/post7/</guid>
      <description>

&lt;h1 id=&#34;current-scenario-in-india&#34;&gt;Current Scenario in INDIA&lt;/h1&gt;

&lt;p&gt;Taking cue from global powerhouses failing catastrophically to contain the onslaught on life that&amp;rsquo;s been lunched by Covid-19, Prime Minister of India Shri Narendra Modi issued instructions for a nationwide &lt;em&gt;Lockdown&lt;/em&gt; starting from 25 March&amp;rsquo;20 to 14 April&amp;rsquo;20, wth possible plans for further extension. The decision have been welcomed and lauded by various researchers and scientists alike, esopecially after countries like USA and Italy acted too late to a possibly aggravated situation. But why this praise? In this post, we&amp;rsquo;ll find out how lockdown and other forms of quarantining helps contain the infection.&lt;/p&gt;

&lt;p&gt;But before that, a gentle reminder of what we are up against: till date, Covid-19 virus has affected $722$ people in India, and $486702$ people worldwide and has claimed $16$ lives in India till now. We have discussed about the origin of the virus and possible measures to contain infection in our last &lt;a href=&#34;https://subroy13.github.io/post/post6/&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This is a post to show how we can incorporate quarantining effects in statistical modelling of epidemiology. In past few days of Shelter-in-place lockdown situation in Kolkata (and throughout the whole India from today onwards), me and one of my ingenious friend &lt;a href=&#34;https://soham01.netlify.com/&#34; target=&#34;_blank&#34;&gt;Soham Bonnerjee&lt;/a&gt; (we were attending the same college before lockdown to purse Masters degree in Statistics), was reading about different epidemiological models available in the literature and using it to generate projections for the number of infected people in India, and this is a result of that reading and further independent developements.&lt;/p&gt;

&lt;h1 id=&#34;loading-the-packages-and-the-dataset&#34;&gt;Loading the Packages and the Dataset&lt;/h1&gt;

&lt;p&gt;We are using the same dataset source as before, except that we have updated it to include some of the current observations. Similar to before, we shall be using &lt;code&gt;dplyr&lt;/code&gt; for data manipulation and summarization, &lt;code&gt;lubridate&lt;/code&gt; for handling dates and times, and &lt;code&gt;ggplot2&lt;/code&gt; for plotting.&lt;/p&gt;

&lt;p&gt;However, due to some Monte Carlo simulation later, we shall be needing &lt;code&gt;rjags&lt;/code&gt; and we shall use &lt;code&gt;gtools&lt;/code&gt; for specification of some useful probability distributions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(readr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(rjags)
library(gtools)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, finally we load the updated dataset, which I have already downloaded from &lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19&#34; target=&#34;_blank&#34;&gt;JHU CSSE&lt;/a&gt;, and manipulated to get the required form of data using the code discussed in my previous post &lt;a href=&#34;https://subroy13.github.io/post/post6/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. We shall take a look at the last few columns to see how much we have updated.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat &amp;lt;- read_csv(&#39;./datasets/covid-19-data.csv&#39;)
knitr::kable(tail(dat, 3))
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;State&lt;/th&gt;
&lt;th&gt;Country&lt;/th&gt;
&lt;th&gt;Lat&lt;/th&gt;
&lt;th&gt;Long&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;th&gt;Confirmed&lt;/th&gt;
&lt;th&gt;Deaths&lt;/th&gt;
&lt;th&gt;Recovered&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Zhejiang&lt;/td&gt;
&lt;td&gt;China&lt;/td&gt;
&lt;td&gt;29.1832&lt;/td&gt;
&lt;td&gt;120.0934&lt;/td&gt;
&lt;td&gt;2020-03-24&lt;/td&gt;
&lt;td&gt;1240&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1221&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Zhejiang&lt;/td&gt;
&lt;td&gt;China&lt;/td&gt;
&lt;td&gt;29.1832&lt;/td&gt;
&lt;td&gt;120.0934&lt;/td&gt;
&lt;td&gt;2020-03-25&lt;/td&gt;
&lt;td&gt;1241&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1221&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Zhejiang&lt;/td&gt;
&lt;td&gt;China&lt;/td&gt;
&lt;td&gt;29.1832&lt;/td&gt;
&lt;td&gt;120.0934&lt;/td&gt;
&lt;td&gt;2020-03-26&lt;/td&gt;
&lt;td&gt;1243&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1222&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;description-of-esir-model&#34;&gt;Description of eSIR Model&lt;/h1&gt;

&lt;p&gt;The eSIR (Extended SIR) model is really similar to the SIR model, except for the fact that it includes  a function that parametrizes the quarantining effects. As with SIR modelling, it has 3 different compartments, of states in which a person can be. This model has been developed very recently by &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.02.29.20029421v1.full.pdf&#34; target=&#34;_blank&#34;&gt;Wang et al.&lt;/a&gt; The states are as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR
  A(Susceptible) --&amp;gt;|&amp;quot;&amp;amp;#946&amp;amp;#960(t)&amp;quot;| B(Infected)
  B --&amp;gt;|&amp;amp;#947| C(Removed)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As described in previous &lt;a href=&#34;https://subroy13.github.io/post/post6/&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt;, &lt;strong&gt;Susceptibles&lt;/strong&gt; are the general population, who is susceptible to get the disease from an infectious person. &lt;strong&gt;Infected&lt;/strong&gt; state reperesents the persons who have the symptoms of the infection and is able to spread it. And finally, &lt;strong&gt;Recovered / Removed&lt;/strong&gt; is the state when a person is recovered from the disease and gain immunity to it, or is dead. Let, $Y_t^S, Y_t^I, Y_t^R$ denotes the proportion of people in these states respectively at the time $t$. Note that $Y_t^S + Y_t^I + Y_t^R = 1$.  $\pi(t)$ denotes the proportion of people transiting from &lt;strong&gt;Susceptible&lt;/strong&gt; to &lt;strong&gt;Infected&lt;/strong&gt; state at time $t$, with the &lt;em&gt;proportion&lt;/em&gt; being commensurate to the proportion of people transiting from &lt;strong&gt;Susceptible&lt;/strong&gt; to &lt;strong&gt;Infected&lt;/strong&gt; state at time $t$ in the original SIR modelling. Thus We can vary $\pi(t)$ from time to time to perfectly capture the effect of quarantining. In other words, the rate at which a susceptible person becomes infected is not a time varying proportion, namely $\beta \pi(t)$, where $\beta$ is the usual rate of transmission of the disease, while $\pi(t)$ is the quarantining effect which might restrict movements of general public in order to make the effective transmission rate lower than the usual quantity $\beta$.&lt;/p&gt;

&lt;p&gt;Let $\theta_t=(\theta_t^S, \theta_t^I, \theta_t^R)^T$ be the vector of underlying prevalence of the population in these three states. Since $Y_t^S, Y_t^I, Y_t^R$ are proportions, we model them via Beta distribution:&lt;/p&gt;

&lt;p&gt;$$\begin{align}
Y_t^I \mid \theta_t, \tau \sim \text{Beta}(\lambda^I\theta_t^I,\lambda^I(1-\theta_t^I))\\&lt;br /&gt;
&amp;amp; \\&lt;br /&gt;
Y_t^R \mid \theta_t, \tau \sim \text{Beta}(\lambda^R\theta_t^R,\lambda^R(1-\theta_t^R))\\&lt;br /&gt;
\end{align}$$&lt;/p&gt;

&lt;p&gt;where $\tau=(\beta, \gamma, \theta_0^T, \kappa, \lambda^I, \lambda^R)^T$, $\beta$, $\gamma$ are as in SIR model; $\kappa$, $\lambda^I, \lambda^R$ are parameters governing the latent process we&amp;rsquo;ll define soon.&lt;/p&gt;

&lt;p&gt;Since $\theta_t$ is a probability, we specify a Dirichlet model for it:&lt;/p&gt;

&lt;p&gt;$$\theta_t \mid \theta_{t-1}, \tau \sim \text{Dirichlet} (k f(\theta_{t-1}, \beta, \gamma))$$&lt;/p&gt;

&lt;p&gt;These together is called a Beta-Dirichlet State Space Model.&lt;/p&gt;

&lt;p&gt;Here $f(\cdot)$ is the solution of the system of ODEs:&lt;/p&gt;

&lt;p&gt;$$\begin{align}
\dfrac{d\theta_t^S}{dt} &amp;amp; = -\pi(t)\beta \theta_t^S \theta_t^I\\&lt;br /&gt;
&amp;amp; \\&lt;br /&gt;
\dfrac{d\theta_t^I}{dt} &amp;amp; = \pi(t)\beta \theta_t^S \theta_t^I - \gamma \theta_t^I\\&lt;br /&gt;
&amp;amp; \\&lt;br /&gt;
\dfrac{d\theta_t^R}{dt} &amp;amp; = \gamma \theta_t^I\\&lt;br /&gt;
\end{align}$$&lt;/p&gt;

&lt;p&gt;and this is exactly where the SIR modelling takes place. We solve this system of ODEs via &lt;em&gt;Runge-Kutta&lt;/em&gt;(RK4) approximation.&lt;/p&gt;

&lt;p&gt;To implement the Markov Chain Monte Carlo method, we need to specify prior for the hyperparameters $\tau=(\beta, \gamma, \theta_0^T, \kappa, \lambda^I, \lambda^R)^{\top}$. We do that as follows:&lt;/p&gt;

&lt;p&gt;We initialize $\theta_0$ via a distribution that uses the observed data:&lt;/p&gt;

&lt;p&gt;$$\begin{align}
\theta_0^I \sim \text{Beta}(1, \dfrac{1}{Y_1^I})\\&lt;br /&gt;
&amp;amp; \\&lt;br /&gt;
\theta_0^R \sim \text{Beta}(1, \dfrac{1}{Y_1^R})\\&lt;br /&gt;
&amp;amp; \\&lt;br /&gt;
\theta_0^S= 1-\theta_0^I-\theta_0^R\\&lt;br /&gt;
\end{align}$$&lt;/p&gt;

&lt;p&gt;We specify the other hyperparameters according to the &lt;a href=&#34;https://arxiv.org/pdf/1007.0908.pdf&#34; target=&#34;_blank&#34;&gt;SARS data&lt;/a&gt; in Hong-Kong.&lt;/p&gt;

&lt;p&gt;$$\begin{align}
R_0= \dfrac{\beta}{\gamma} \sim \text{Log}\mathbb{N}(1.099,0.096) \implies \mathbb{E}(R_0)=3.15, \mathbb{V}(R_0)=1 \\&lt;br /&gt;
&amp;amp; \\&lt;br /&gt;
\gamma \sim \text{Log}\mathbb{N}(-2.955,0.910) \implies \mathbb{E}(\gamma)=0.0117, \mathbb{V}(\gamma)=0.01 \\&lt;br /&gt;
&amp;amp; \\&lt;br /&gt;
k \sim \text{Gamma}(2, 0.0001)\\&lt;br /&gt;
&amp;amp;  \\&lt;br /&gt;
\lambda^I \sim \text{Gamma}(2, 0.0001)\\&lt;br /&gt;
&amp;amp; \\&lt;br /&gt;
\lambda^R \sim \text{Gamma}(2, 0.0001)\\&lt;br /&gt;
\end{align}$$&lt;/p&gt;

&lt;p&gt;Now suppose , data upto time point $t_0$ are observed: $(Y_1, \cdots Y_{t_0})$. We want to predict upto time point $T$. To this end, we generate $M$ MCMC samples, such that for $m \in {1,\cdots, M}$, $(Y_{t_0+1}^{(m)}, \cdots, Y_T)$ is a draw from $Y_t \mid \theta_t, \tau$. The algorihm is described below:&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Algorithm:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;for $m$ in $1, \cdots, M$&lt;/p&gt;

&lt;p&gt;$\qquad$  for $t$ in $t_0+1,\cdots, T$,&lt;/p&gt;

&lt;p&gt;$\qquad \qquad$  Draw $\theta_t^{(m)}$ from $[\theta_t \mid \theta_{t-1}^{(m)}, \tau^{(m)}]$&lt;/p&gt;

&lt;p&gt;$\qquad \qquad$  Draw $Y_t^{(m)}$ from $[Y_t \mid \theta_{t}^{(m)}, \tau^{(m)} ]$&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Finally for each time point $t$, $t= t_0+1,\cdots, T$ estimate $Y_t$ by $\hat{Y}_t=\dfrac{1}{M}\displaystyle \sum_{m=1}^M Y_t^{(m)}$.&lt;/p&gt;

&lt;h1 id=&#34;estimation-of-esir-model&#34;&gt;Estimation of eSIR Model&lt;/h1&gt;

&lt;h2 id=&#34;initialization&#34;&gt;Initialization&lt;/h2&gt;

&lt;p&gt;Note that, we wish to have a lognormal distribution $R_0 \sim LogN(\mu, \sigma^2)$, such that, $E(R_0), Var(R_0)$ is at a specified value. The specific reason is that, $R_0 = \beta_0 / \gamma_0$, which is a positive quantity, hence is better modelled by a gamma or lognormal distribution than a normal distribution, which has support as the whole of real line. Therefore, given $E(R_0) = a$, and $Var(R_0) = b$, we wish to figure out $\mu, \sigma$, the parmeters of the lognormal distribution. This can be obtained through the following simple formula and is implemented in the following function.&lt;/p&gt;

&lt;p&gt;$$\sigma^2 = \log\left( \dfrac{Var(R_0)}{E(R_0)^2} +1 \right) \qquad \qquad \mu = \log(E(R_0)) - \dfrac{\sigma^2}{2}$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lognorm.param&amp;lt;-function(mu0,var0){
  var &amp;lt;- log(var0/mu0^2+1)
  mu &amp;lt;- log(mu0)-var/2
  return(round(c(mu,var),3))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we specify some control parameters, which specifies the initial $E(R_0), Var(R_0)$ and some other parameters, as well as the parameters for the markov chain (like the length of the chain, the number of parallel chains to construct in order to speed up the process, number of samples to define burn in period etc.) You may want to skip these technical details for now.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;init.params &amp;lt;- list(R0 = 3.15, R0_sd = 1, gamma0 = 0.0117, gamma0_sd = 0.1)
control.params &amp;lt;- list(nchain=4, nadapt=1e4, ndraw=5e2, thin=10, nburnin=2e2)
control.params$mclen &amp;lt;- round(control.params$ndraw / control.params$thin) * control.params$nchain    #number of MCMC draws in total
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;performing-mcmc&#34;&gt;Performing MCMC&lt;/h2&gt;

&lt;p&gt;Now, we shall be creating a function called &lt;code&gt;do.MCMC&lt;/code&gt; which takes the observed proportion of Infected (I), Removed ( R ), then the value of the function $\pi(t)$ till the observed time period (and the control parameters). Then, it shall perform the MCMC step by generating the posterior sampels, and it shall output that posterior samples, which we shall later use to generate predictions, as well as get estimates.&lt;/p&gt;

&lt;h3 id=&#34;defining-the-jags-code&#34;&gt;Defining the JAGS Code&lt;/h3&gt;

&lt;p&gt;The very first thing to implement MCMC or any Bayesian Computation in &lt;code&gt;rjags&lt;/code&gt; is the specification of a JAGS code. JAGS is the acronym for &lt;a href=&#34;http://mcmc-jags.sourceforge.net/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Just Another Gibbs Sampler&lt;/strong&gt;&lt;/a&gt;, which is a program for simulation from Bayesian hierarchical models using Markov chain Monte Carlo (MCMC), developed by Martyn Plummer.&lt;/p&gt;

&lt;p&gt;JAGS code is mainly created based on two simple operations.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;An assignment operator &amp;ldquo;&amp;lt;-&amp;rdquo; is used to denote a deterministic relation.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A tilde operator &amp;ldquo;~&amp;rdquo; is used to denote a stochastic relation. In this case, we have a probability distribution on right hand side, according to which the left hand side variable is being generated.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Based on the above operations, the following JAGS code creates the Bayesian model described above.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.string &amp;lt;- paste0(&amp;quot;
             model{
                   for(t in 2:(T_obs+1)){
                   Km[t-1,1] &amp;lt;- -beta*pi[t-1]*theta[t-1,1]*theta[t-1,2]
                   Km[t-1,9] &amp;lt;- gamma*theta[t-1,2]
                   Km[t-1,5] &amp;lt;- -Km[t-1,1]-Km[t-1,9]
                   Km[t-1,2] &amp;lt;- -beta*pi[t-1]*(theta[t-1,1]+0.5*Km[t-1,1])*(theta[t-1,2]+0.5*Km[t-1,5])
                   Km[t-1,10] &amp;lt;- gamma*(theta[t-1,2]+0.5*Km[t-1,5])
                   Km[t-1,6] &amp;lt;- -Km[t-1,2]-Km[t-1,10]
                   Km[t-1,3] &amp;lt;- -beta*pi[t-1]*(theta[t-1,1]+0.5*Km[t-1,2])*(theta[t-1,2]+0.5*Km[t-1,6])
                   Km[t-1,11] &amp;lt;- gamma*(theta[t-1,2]+0.5*Km[t-1,6])
                   Km[t-1,7] &amp;lt;- -Km[t-1,3]-Km[t-1,11]
                   Km[t-1,4] &amp;lt;- -beta*pi[t-1]*(theta[t-1,1]+Km[t-1,3])*(theta[t-1,2]+Km[t-1,7])
                   Km[t-1,12] &amp;lt;- gamma*(theta[t-1,2]+Km[t-1,7])
                   Km[t-1,8] &amp;lt;- -Km[t-1,4]-Km[t-1,12]
                   alpha[t-1,1] &amp;lt;- theta[t-1,1]+(Km[t-1,1]+2*Km[t-1,2]+2*Km[t-1,3]+Km[t-1,4])/6
                   alpha[t-1,2] &amp;lt;- theta[t-1,2]+(Km[t-1,5]+2*Km[t-1,6]+2*Km[t-1,7]+Km[t-1,8])/6
                   alpha[t-1,3] &amp;lt;- theta[t-1,3]+(Km[t-1,9]+2*Km[t-1,10]+2*Km[t-1,11]+Km[t-1,12])/6
                   theta[t,1:3] ~ ddirch(k*alpha[t-1,1:3])
                   I[t-1] ~ dbeta(lambdaI*theta[t,2],lambdaI*(1-theta[t,2]))
                   R[t-1] ~ dbeta(lambdaR*theta[t,3],lambdaR*(1-theta[t,3]))
                  }
                  theta[1,1] &amp;lt;-  1- theta[1,2]- theta[1,3]
                  theta[1,2] ~ dbeta(&amp;quot;,1,&amp;quot;,&amp;quot;,1/I[1],&amp;quot;)
                  theta[1,3] ~ dbeta(&amp;quot;,1,&amp;quot;,&amp;quot;,1/R[1],&amp;quot;)
                  gamma ~  dlnorm(&amp;quot;,lognorm_gamma_param[1],&amp;quot;,&amp;quot;,1/lognorm_gamma_param[2],&amp;quot;)
                  R0 ~ dlnorm(&amp;quot;,lognorm_R0_param[1],&amp;quot;,&amp;quot;,1/lognorm_R0_param[2],&amp;quot;)
                  beta &amp;lt;- R0*gamma
                  k ~  dgamma(2,0.0001)
                  lambdaI ~ dgamma(2,0.0001)
                  lambdaR ~ dgamma(2,0.0001)
               }
            &amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, &lt;code&gt;init.params&lt;/code&gt; are to be passed accordingly in &lt;code&gt;lognorm.param&lt;/code&gt; function to obtain the &lt;code&gt;lognorm_gamma_param&lt;/code&gt; and &lt;code&gt;lognorm_R0_param&lt;/code&gt; variables.&lt;/p&gt;

&lt;h3 id=&#34;performing-mcmc-1&#34;&gt;Performing MCMC&lt;/h3&gt;

&lt;p&gt;Now, once we have the &lt;code&gt;model.string&lt;/code&gt; containing the JAGS code, we need to open a connection object to that string. Because, &lt;code&gt;rjags&lt;/code&gt; expect the JAGS code to be written in a file, a connection object can gimick the behavior of a file, just based on that string. Once we have the JAGS code ready, passing the data nodes in the JAGS model helps us in creating the posterior model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.spec &amp;lt;- textConnection(model.string)
posterior &amp;lt;- jags.model(model.spec, data=list(&#39;I&#39;=I,&#39;R&#39;=R,&#39;T_obs&#39;=T_obs,&#39;pi&#39;=pi), 
                    n.chains =control.params$nchain, n.adapt = control.params$nadapt)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we first update the posterior for the burn-in period. In this time, the posterior generates samples are not close to the original sample, and hence we need to discard them. Typically, we set burn-in to be about $2000$ iterations. After that, we pass&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;update(posterior, control.params$nburnin) # update the posterior for burn in times, for this time, do not monitor anything
jags_sample &amp;lt;- jags.samples(posterior, c(&#39;theta&#39;,&#39;gamma&#39;,&#39;R0&#39;,&#39;beta&#39;,&#39;I&#39;,&#39;lambdaI&#39;,&#39;lambdaR&#39;,&#39;k&#39;),
                        n.iter = control.params$ndraw * control.params$nchain,
                        thin = control.params$thin)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, we combine these into a function called &lt;code&gt;do.MCMC&lt;/code&gt; which we shall use later.&lt;/p&gt;

&lt;h2 id=&#34;forecasting-using-esir-model&#34;&gt;Forecasting using eSIR model&lt;/h2&gt;

&lt;p&gt;Now, since we have the MCMC samples obtained as the output of &lt;code&gt;do.MCMC&lt;/code&gt; function, we can extend the chain using the exact simulation of the process described by the model above, and then, we can obtain the estimates of the proportion of infected and recovered people, as well as the confidence interval for that proportion.&lt;/p&gt;

&lt;p&gt;Again, we shall create &lt;code&gt;forecast.SIR&lt;/code&gt; function which takes in the output of &lt;code&gt;do.MCMC&lt;/code&gt;, and then run the simulation chains starting from the MCMC samples. Finally, it computes the estimates and confidence intervals for the estimates of proportion of infected and removed from the samples from all the chains.&lt;/p&gt;

&lt;h3 id=&#34;extract-components-of-mcmc-samples&#34;&gt;Extract components of MCMC samples&lt;/h3&gt;

&lt;p&gt;Here, we extract the components of MCMC samples from the output of &lt;code&gt;do.MCMC&lt;/code&gt; function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# extract components from mcmc samples 
theta_pre &amp;lt;- array(as.mcmc.list(mcmc_sample$theta)[[1]], dim = c(control.params$mclen, T_obs+1, 3))
R0_pre &amp;lt;- as.mcmc.list(mcmc_sample$R0)[[1]]
gamma_pre &amp;lt;- as.mcmc.list(mcmc_sample$gamma)[[1]]
beta_pre &amp;lt;- as.mcmc.list(mcmc_sample$beta)[[1]]
lambdaI_pre &amp;lt;- as.mcmc.list(mcmc_sample$lambdaI)[[1]]
lambdaR_pre &amp;lt;- as.mcmc.list(mcmc_sample$lambdaR)[[1]]
k_pre &amp;lt;- as.mcmc.list(mcmc_sample$k)[[1]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We also initialize blank arrays to store the future forecasts. Here, &lt;code&gt;T_new&lt;/code&gt; is the number of time points for which new forecasting is to be done.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;theta_post &amp;lt;- array(0, dim=c(control.params$mclen, T_new, 3))
I_post &amp;lt;- matrix(NA, nrow=control.params$mclen, ncol=T_new)
R_post &amp;lt;- matrix(NA, nrow=control.params$mclen, ncol=T_new)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;peform-forecasting&#34;&gt;Peform Forecasting&lt;/h3&gt;

&lt;p&gt;The following piece of code basically performs the forecasting starting from the last obtained posterior samples as obtained by MCMC. Now, let us go through the code step by step.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for(l in 1:control.params$mclen){
    
    thetalt1 &amp;lt;- theta_pre[l, T_obs+1, 1]
    thetalt2 &amp;lt;- theta_pre[l, T_obs+1, 2]
    thetalt3 &amp;lt;- theta_pre[l, T_obs+1, 3]
    betal &amp;lt;- beta_pre[l]
    gammal &amp;lt;- gamma_pre[l]
    kt &amp;lt;- k_pre[l]
    lambdaIl &amp;lt;- lambdaI_pre[l]
    lambdaRl &amp;lt;- lambdaR_pre[l]
    if (betal&amp;lt;0 | gammal&amp;lt;0 | thetalt1&amp;lt;0 | thetalt2&amp;lt;0 |thetalt3&amp;lt;0) { 
      next 
    }
    
    
    for(t in 1:T_new ){
      # perform runge kutta
      Km &amp;lt;- numeric(12)
      alpha_post &amp;lt;- numeric(3)
      
      Km[1] &amp;lt;- -betal*pi_new[t]*thetalt1*thetalt2
      Km[9] &amp;lt;- gammal*thetalt2
      Km[5] &amp;lt;- -Km[1]-Km[9]
      
      Km[2] &amp;lt;- -betal*pi_new[t]*(thetalt1+0.5*Km[1])*(thetalt2+0.5*Km[5])
      Km[10] &amp;lt;- gammal*(thetalt2+0.5*Km[5])
      Km[6] &amp;lt;- -Km[2]-Km[10]
      
      Km[3] &amp;lt;- -betal*pi_new[t]*(thetalt1+0.5*Km[2])*(thetalt2+0.5*Km[6])
      Km[11] &amp;lt;- gammal*(thetalt2+0.5*Km[6])
      Km[7] &amp;lt;- -Km[3]-Km[11]
      
      Km[4] &amp;lt;- -betal*pi_new[t]*(thetalt1+Km[3])*(thetalt2+Km[7])
      Km[12] &amp;lt;- gammal*(thetalt2+Km[7])
      Km[8] &amp;lt;- -Km[4]-Km[12]
      
      alpha_post[1] &amp;lt;- thetalt1+(Km[1]+2*Km[2]+2*Km[3]+Km[4])/6
      alpha_post[2] &amp;lt;- thetalt2+(Km[5]+2*Km[6]+2*Km[7]+Km[8])/6
      alpha_post[3] &amp;lt;- thetalt3+(Km[9]+2*Km[10]+2*Km[11]+Km[12])/6
      
      thetalt_tmp &amp;lt;- rdirichlet(1, kt* abs(alpha_post))
      thetalt1 &amp;lt;- theta_post[l,t,1] &amp;lt;- thetalt_tmp[1]
      thetalt2 &amp;lt;- theta_post[l,t,2] &amp;lt;- thetalt_tmp[2]
      thetalt3 &amp;lt;- theta_post[l,t,3] &amp;lt;- thetalt_tmp[3]
      
      I_post[l,t] &amp;lt;- rbeta(1,lambdaIl*thetalt2,lambdaIl*(1-thetalt2))
      R_post[l,t] &amp;lt;- rbeta(1,lambdaRl*thetalt3,lambdaRl*(1-thetalt3))
      
    }
    
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The outer loop runs for each of the MCMC chains generated by &lt;code&gt;mcmc_output&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If any of $\beta, \gamma, \theta^S,\theta^I, \theta^R$ turns out to be less than $0$, the chain is stopped and is not proceeded further.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the inner loop, for each new timepoint, we perform the &lt;a href=&#34;https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods&#34; target=&#34;_blank&#34;&gt;Runge Kutta methods&lt;/a&gt; of numerically solving the differential equations,&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;At the end of the loop, we have all the estimates for proportion of Infected and Removed, as well as the three latent proportion parameters $\theta$ which accords with the underlying mathematical model of SIR.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Here, &lt;code&gt;pi_new[t] = pi[T_obs + t]&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;derivation-of-3-important-dates&#34;&gt;Derivation of 3 important dates&lt;/h3&gt;

&lt;p&gt;Associated with the estimation of eSIR model, there are three important dates, which we should output from our algorithm.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The date when $\dfrac{d^2 \theta^I_t}{dt^2} = 0$. This date is the time, when the rate of change in the proportion of infected people starts to be decreasing, so we are in a hopeful scenario that the number of new infected cases is going to be decreasing over time.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The date when $\dfrac{d \theta^I_t}{dt} = 0$, i.e. the date from when the number of new recovered cases starts to exceed the number of new infected cases. From this time onwards, the epidemic nature of the disease vanishes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The date when $\theta^I_t = 0$. However, it is impossible to have this proportion equal to $0$, in finite times. Therefore, we shall be looking for a date, when $\theta^I_t &amp;lt; \epsilon$, where $\epsilon$ is a very small quantity, which can be treated readily at any time.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To simplify the calculation of derivative, we track the quantity $(\beta \pi(t) \theta^S_t\theta^I_t - \gamma \theta^I_t)$ over time, which is equal to the derivative due to SIR modelling.&lt;/p&gt;

&lt;p&gt;The following picture taken from the &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.02.29.20029421v1.full.pdf&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; by Wang et al. shows the concept clearly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./deriv.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;theta_diff_pre &amp;lt;- theta_pre[, 1:T_obs, 2] * sweep(beta_pre %*% t(pi[1:T_obs]) * theta_pre[, 1:T_obs, 1], MARGIN = 1, STATS = gamma_pre)
theta_diff_post &amp;lt;- theta_post[, , 2] * sweep(beta_pre %*% t(pi[(T_obs+1):(T_obs+T_new)]) * theta_post[, , 1], MARGIN = 1, STATS = gamma_pre)
theta_diff &amp;lt;- cbind(theta_diff_pre, theta_diff_post)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we find the maximum of each row of &lt;code&gt;theta_diff&lt;/code&gt; and locate the timepoints, where &lt;code&gt;theta_diff&lt;/code&gt; is changing its signs. They should give up the required first and second important dates.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;first_dates &amp;lt;- apply(theta_diff, 1, which.max)
second_dates &amp;lt;- apply(theta_diff, 1, function(x) { length(x) + 1 - which(cumprod(rev(x &amp;lt;= 0)) == 0)[1] } )    # the first time it becomes negative and stays negative throughout
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, that we have the dates for each chain, let us take the mean, median, and quantiles, which would give us estimates and confidence intervals for such dates.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;conf.quant &amp;lt;- c(conf.level/2, 0.5, 1-conf.level/2)    # conf.level is the confidence coefficients for the confidence interval
first_date_summary &amp;lt;- c( mean(first_dates, na.rm = T), quantile(first_dates, probs = conf.quant, na.rm = T) )
second_date_summary &amp;lt;- c( mean(second_dates, na.rm = T), quantile(second_dates, probs = conf.quant, na.rm = T) )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;summarising-the-forecast-outputs&#34;&gt;Summarising the forecast outputs&lt;/h3&gt;

&lt;p&gt;Once we have perform forecasting for each and every chain of the MCMC samples, we can aggregated them to obtain the mean, median, and quantiles, which would give us estimates for the proportion of people in state infected and removed, as well as the confidence interval for those estimates.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# till the observed time points
thetaI_band &amp;lt;- t(apply(theta_pre[,-1,2] , 2, quantile, probs=conf.quant, na.rm=T))   
thetaR_band&amp;lt;- t(apply(theta_pre[,-1,3] , 2, quantile, probs=conf.quant, na.rm=T))
thetaI_mean &amp;lt;- colMeans(theta_pre[, -1, 2], na.rm = T)
thetaR_mean &amp;lt;- colMeans(theta_pre[, -1, 3], na.rm = T)

# for the new time points
YI_band &amp;lt;- t(apply(I_post , 2, quantile, probs=conf.quant, na.rm=T))   
YR_band&amp;lt;- t(apply(R_post , 2, quantile, probs=conf.quant, na.rm=T))
YI_mean &amp;lt;- colMeans(I_post, na.rm = T)
YR_mean &amp;lt;- colMeans(R_post, na.rm = T)

# combine into a data frame
infected.dat &amp;lt;- data.frame( rbind( cbind(thetaI_mean, thetaI_band, rep(&amp;quot;pre&amp;quot;, T_obs) ), cbind(YI_mean, YI_band, rep(&amp;quot;post&amp;quot;, T_new) ) ) )
removed.dat &amp;lt;- data.frame( rbind( cbind(thetaR_mean, thetaR_band, rep(&amp;quot;pre&amp;quot;, T_obs) ), cbind(YR_mean, YR_band, rep(&amp;quot;post&amp;quot;, T_new) ) ) )

colnames(infected.dat) &amp;lt;- c(&amp;quot;mean&amp;quot;, &amp;quot;lower&amp;quot;, &amp;quot;median&amp;quot;, &amp;quot;upper&amp;quot;)
colnames(removed.dat) &amp;lt;- c(&amp;quot;mean&amp;quot;, &amp;quot;lower&amp;quot;, &amp;quot;median&amp;quot;, &amp;quot;upper&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, we can return a list comprising of these two datasets, as well as the summary corresponding to the first and second date obtained earlier. This whole piece of code, is compressed into a function called &lt;code&gt;forecast.SIR&lt;/code&gt;, which we shall call later.&lt;/p&gt;

&lt;h1 id=&#34;checking-performance-of-esir&#34;&gt;Checking Performance of eSIR&lt;/h1&gt;

&lt;p&gt;Now, we shall be using the data for &lt;code&gt;Italy&lt;/code&gt;, in order to figure out how our &lt;code&gt;eSIR&lt;/code&gt; model performs for this data.&lt;/p&gt;

&lt;h2 id=&#34;performance-for-italy&#34;&gt;Performance for Italy&lt;/h2&gt;

&lt;p&gt;According to &lt;a href=&#34;https://www.worldometers.info/world-population/italy-population/&#34; target=&#34;_blank&#34;&gt;Worldometers&lt;/a&gt; sources, the projected population for Italy is about 6 crores. Based on that, we compute the observed proportion of the infected and removed (including deaths and recovered), and fit the eSIR model. The following code calls the &lt;code&gt;do.MCMC&lt;/code&gt; function and performs the MCMC fitting. Here, we use 4 weeks of data, of the available 5 weeks of data, and we shall try to forecast for the last week, to see how the model performed.&lt;/p&gt;

&lt;p&gt;Also, the $\pi(t)$ function choosen for Italy is as follows;&lt;/p&gt;

&lt;p&gt;$$\pi(t) = \begin{cases}
1 &amp;amp; \text{ if } t &amp;lt; \text{4th March, 2020}\\&lt;br /&gt;
0.25 &amp;amp; \text{ if } t &amp;gt; \text{9th March, 2020}\\&lt;br /&gt;
0.5 &amp;amp; \text{ otherwise }\\&lt;br /&gt;
\end{cases}$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;subdat &amp;lt;- dat %&amp;gt;% mutate(Removed = Deaths + Recovered) %&amp;gt;% filter(Country == &amp;quot;Italy&amp;quot; &amp;amp; Removed &amp;gt; 0)

N &amp;lt;- 60550075
R &amp;lt;- subdat$Removed / N
I &amp;lt;- subdat$Confirmed/N - R
pi0 &amp;lt;- ifelse(subdat$Date &amp;lt; as.Date(&#39;2020-03-04&#39;), 1, 
              ifelse(subdat$Date &amp;gt; as.Date(&#39;2020-03-09&#39;), 0.25, 0.5))

init.params &amp;lt;- list(R0 = 3.15, R0_sd = 1, gamma0 = 0.0117, gamma0_sd = 0.1)
control.params &amp;lt;- list(nchain=4, nadapt=1e4, ndraw=5e2, thin=10, nburnin=2e2)
control.params$mclen &amp;lt;- round(control.params$ndraw / control.params$thin) * control.params$nchain 

mcmc_samples &amp;lt;- do.MCMC(I[1:28], R[1:28], pi0[1:28], init.params, control.params)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see something like this, which tells you that the JAGS code for MCMC is running properly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 56
   Unobserved stochastic nodes: 35
   Total graph size: 1591

Initializing model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we see from the following plot, the predictions for the last week is underestimating the true proportions. However, the observed proportions lie within the bound of $95\%$ confidence band.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;preds &amp;lt;- forecast.SIR(7, 28, mcmc_samples, pi = pi0, control.params = control.params, start.date = &amp;quot;2020-02-21&amp;quot;)
ggplot(preds$infected, aes(x = date)) + 
    geom_ribbon(aes(ymin = lower, ymax = upper, fill = phase), alpha = 0.25) +
    geom_line(aes(y = mean, color = phase), size = 1) +
    labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;P(Infected)&amp;quot;) +
    geom_point(data = subdat, aes(x = Date, y = (Confirmed - Removed)/N), color = &amp;quot;black&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./unnamed-chunk-20-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;However, if we assume that, the Italy government maintains the current lockdown indefinitely, then $\pi(t) = 0.25$ will continue to remain. In such case, the important dates (as mentioned above) for change in the rate of newly infected can be obtained, as shown in the following plot.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pi0 &amp;lt;- c(pi0, rep(0.25, 200))
preds &amp;lt;- forecast.SIR(7+200, 28, mcmc_samples, pi = pi0, control.params = control.params, start.date = &amp;quot;2020-02-21&amp;quot;)
ggplot(preds$infected, aes(x = date)) + 
    geom_ribbon(aes(ymin = lower, ymax = upper, fill = phase), alpha = 0.25) +
    geom_line(aes(y = mean, color = phase), size = 1) +
    geom_vline(xintercept = preds$first.date[1], color = &amp;quot;brown&amp;quot;) + 
    geom_vline(xintercept = preds$second.date[1], color = &amp;quot;blue4&amp;quot;) +
    labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;P(Infected)&amp;quot;) +
    geom_point(data = subdat, aes(x = Date, y = (Confirmed - Removed)/N), color = &amp;quot;black&amp;quot;) +
    geom_text(x = preds$first.date[1] - 1, y = 0, label = format(preds$first.date[1], &amp;quot;%b %d&amp;quot;), size = 4 ) +
    geom_text(x = preds$second.date[1] - 1, y = 0, label = format(preds$second.date[1], &amp;quot;%b %d&amp;quot;), size = 4 )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./plot2.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Therefore, in Italy, at about June 24, we should see decreasing increments of newly infected persons, and from August 17 onwards, the proportion of infected people should start to decline.&lt;/p&gt;

&lt;h2 id=&#34;estimation-for-india&#34;&gt;Estimation for India&lt;/h2&gt;

&lt;p&gt;However, our main concern is to find out the situation of India based on this eSIR modelling. Here, we shall use the $\pi(t)$ function as follows, as per various decision taken by the Government, like restriction on international flights, Janta Curfew and full scale lockdowns etc.&lt;/p&gt;

&lt;p&gt;$$\pi(t) = \begin{cases}
1 &amp;amp; \text{ if } t &amp;lt; \text{15th March, 2020}\\&lt;br /&gt;
0.95 &amp;amp; \text{ if } \text{15th March, 2020} &amp;lt; t &amp;lt; \text{19th March, 2020}\\&lt;br /&gt;
0.9 &amp;amp; \text{ if } \text{19th March, 2020} &amp;lt; t &amp;lt; \text{22th March, 2020}\\&lt;br /&gt;
0.5 &amp;amp; \text{ if } \text{22th March, 2020} &amp;lt; t &amp;lt; \text{25th March, 2020}\\&lt;br /&gt;
0.1 &amp;amp; \text{ if } t &amp;gt; \text{25th March, 2020}\\&lt;br /&gt;
\end{cases}$$&lt;/p&gt;

&lt;p&gt;Also, note that in India, many places are rural in nature, while most of the urban population and neighbourhood areas are more susceptible to get the disease than others, due to availablity of metro cities and international airports. So, rather than using the whole population size i.e. $131$ crores as $N$, we shall use about $350$ lakhs as the adjusted population size.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;subdat &amp;lt;- dat %&amp;gt;% mutate(Removed = Deaths + Recovered, Infected = Confirmed - Removed) %&amp;gt;%
    filter(Country == &amp;quot;India&amp;quot; &amp;amp; Removed &amp;gt; 0 &amp;amp; Infected &amp;gt; 0)

N &amp;lt;- 350e5   # adjusted population for metro cities and neighbourhood areas

R &amp;lt;- subdat$Removed / N
I &amp;lt;- subdat$Infected/N

pi0 &amp;lt;- ifelse(subdat$Date &amp;lt; as.Date(&#39;2020-03-15&#39;), 1, 
              ifelse(subdat$Date &amp;lt; as.Date(&#39;2020-03-19&#39;), 0.9, 
                     ifelse(subdat$Date &amp;lt; as.Date(&#39;2020-03-22&#39;), 0.8, 
                            ifelse(subdat$Date &amp;lt; as.Date(&#39;2020-03-25&#39;), 0.5, 0.1)) ))

init.params &amp;lt;- list(R0 = 12.48, R0_sd = 1, gamma0 = 0.117, gamma0_sd = 1)
control.params &amp;lt;- list(nchain=4, nadapt=1e4, ndraw=5e2, thin=5, nburnin=2e2)
control.params$mclen &amp;lt;- round(control.params$ndraw / control.params$thin) * control.params$nchain 

mcmc_samples &amp;lt;- do.MCMC(I, R, pi0, init.params, control.params)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally, we perform the prediction till the end of the lockdown, i.e. till $14$-th April, to see the effect of quarantining.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pi1 &amp;lt;- c(pi0, rep(0.1, 19))   # from 27th March, it is 19 days of lockdown left 
preds &amp;lt;- forecast.SIR(19, 25, mcmc_samples, pi = pi1, control.params = control.params, start.date = &amp;quot;2020-03-02&amp;quot;)
ggplot(preds$infected, aes(x = date)) + 
    geom_ribbon(aes(ymin = lower, ymax = upper, fill = phase), alpha = 0.25) +
    geom_line(aes(y = mean, color = phase), size = 1) +
    labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;P(Infected)&amp;quot;) +
    geom_point(data = subdat, aes(x = Date, y = Infected/N), color = &amp;quot;black&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./unnamed-chunk-23-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Although it seems that the mean proportion is overestimated during the ovserved period, however the effect of quanting seems to work to a moderate extent, as the estimated proportion stays more or less at the same level. The exact count of the infected people at the end of lockdown, provided that it is maintained properly, should be about $1571$ many infected persons, and about $2114$ confirmed cases of COVID-19.&lt;/p&gt;

&lt;p&gt;Similar to Italy, if we extend its forecast for some more days, and keep the same level of quarantining, the model predictions are given in the following plot.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pi2 &amp;lt;- c(pi0, rep(0.1, 200)) 
preds &amp;lt;- forecast.SIR(200, 25, mcmc_samples, pi = pi2, control.params = control.params, start.date = &amp;quot;2020-03-02&amp;quot;)
ggplot(preds$infected, aes(x = date)) + 
    geom_ribbon(aes(ymin = lower, ymax = upper, fill = phase), alpha = 0.25) +
    geom_line(aes(y = mean, color = phase), size = 1) +
    geom_vline(xintercept = preds$first.date[1], color = &amp;quot;brown&amp;quot;, size = 1, linetype = &amp;quot;dashed&amp;quot;) + 
    geom_vline(xintercept = preds$second.date[1], color = &amp;quot;blue4&amp;quot;, size = 1, linetype = &amp;quot;dashed&amp;quot;) +
    labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;P(Infected)&amp;quot;) +
    geom_point(data = subdat, aes(x = Date, y = (Confirmed - Removed)/N), color = &amp;quot;black&amp;quot;) +
    geom_text(x = preds$first.date[1] - 7, y = 2e-4, label = format(preds$first.date[1], &amp;quot;%b %d&amp;quot;), size = 4 ) +
    geom_text(x = preds$second.date[1] + 7, y = 3e-4, label = format(preds$second.date[1], &amp;quot;%b %d&amp;quot;), size = 4 )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./unnamed-chunk-25-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Therefore, as it seems, if we keep the lockdown (as it is now), based on the data, it might take till October to have the proportion of infected people becoming insignificant, provided we keep the same level of quanranting effect. However, from March 17, we should see a decrease in the rate of increment in the number of infected, and from March 29 onwards, we should be at a position from where the number of newly infected persons per day starts to decrease gradually.&lt;/p&gt;

&lt;p&gt;Since, it is very economically and physically dissatisfying to be in a lockdown (or state of home quarantine till October, where Durga Puja festival is going to occur), we might want to go outside. In that case, assuming $\pi(t)$ increases to $0.5$, which is indeed less than 1, due to our increasing awareness of the scenario, then, the predicted situation would look like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pi3 &amp;lt;- c(pi0, rep(0.1, 19), rep(0.5, 2 * 365)) 
preds &amp;lt;- forecast.SIR(2 * 365 + 19, 25, mcmc_samples, pi = pi3, control.params = control.params, start.date = &amp;quot;2020-03-02&amp;quot;)
preds$infected$upper_new &amp;lt;- pmin(preds$infected$upper, 0.05)   # just for visual purpose
ggplot(preds$infected, aes(x = date)) + 
    geom_ribbon(aes(ymin = lower, ymax = upper_new, fill = phase), alpha = 0.25) +
    geom_line(aes(y = mean, color = phase), size = 1) +
    geom_vline(xintercept = preds$first.date[1], color = &amp;quot;brown&amp;quot;, size = 1, linetype = &amp;quot;dashed&amp;quot;) + 
    geom_vline(xintercept = preds$second.date[1], color = &amp;quot;blue4&amp;quot;, size = 1, linetype = &amp;quot;dashed&amp;quot;) +
    labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;P(Infected)&amp;quot;) +
    geom_point(data = subdat, aes(x = Date, y = (Confirmed - Removed)/N), color = &amp;quot;black&amp;quot;) +
    geom_text(x = preds$first.date[1] - 7, y = 2e-4, label = format(preds$first.date[1], &amp;quot;%b %d&amp;quot;), size = 4 ) +
    geom_text(x = preds$second.date[1] + 7, y = 3e-4, label = format(preds$second.date[1], &amp;quot;%b %d&amp;quot;), size = 4 )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./plot5.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This scenario would be very severe for us, since it would affect about $6\%$ of the urban population on about June 27, and then it would start to decrease gradually. However, it would take almost 2 years, to make this epidemic insignificant, provided there would be no generally accepted antiviral medicine for COVID-19 till then.&lt;/p&gt;

&lt;p&gt;Hence, we must avoid such situation at all costs, and to counter it, we must stay home, at a shelter in place home quarantining atmosphere.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This &lt;strong&gt;eSIR model is not perfect.&lt;/strong&gt; No statistical model is. But the main idea and examples that we are seeing currently all over the world, should give us a hint about what is to come.&lt;/p&gt;

&lt;p&gt;There are several backdrops in &lt;strong&gt;eSIR&lt;/strong&gt; model.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;It does not talk about underreporting. When you are in a place like India, underreporting is a major issue, as there might be some people who are affected by coronavirus, but lightly takes their symptoms as seasonal cold.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the basis of SIR model, we assume the existence of only 3 states. To incorporate the effect of quarantining, we only considering the people in Infected state. However, there might be people who are infected, and can spread the disease, yet they do not have the symptoms, and hence does not fall under the effect of quarantining.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;WHO Guidelines to STAY SAFE:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The best way to prevent and slow down transmission is be well informed about the COVID-19 virus, the disease it causes and how it spreads.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Wash your hands frequently with an alcohol based rub or soap.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Maintain social distancing.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Avoid touching eyes, nose and mouth.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Practice respiratory hygiene.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If you have fever, cough and difficulty breathing, seek medical care early.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Stay informed and follow advice given by your healthcare provider.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Protect yourselves and protect others.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Signing Off&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$\qquad &amp;mdash;$ Subhrajyoty Roy &amp;amp; Soham Bonnerjee&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Stay home, Stay safe, Keep safe your friends, families and neighbours.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>SIR Modelling of COVID-19 Pandemic situation</title>
      <link>/post/post6/</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/post6/</guid>
      <description>

&lt;h1 id=&#34;the-current-global-scenario&#34;&gt;The Current Global Scenario&lt;/h1&gt;

&lt;p&gt;At the end of December 2019, a cluster of an unknown pneumonia like cases were reported in Wuhan, a city in the Hubei province, China. They quickly identified that the source of the infection was a novel coronavirus belonging to the coronavirus family, which includes the virus related to the outbreaks of  Severe Acute Respiratory Syndrome (SARS) from 2002-2004 and Middle East Respiratory Syndrome (MERS) in 2012. It  spread through and outside of Wuhan, resulting in an rapidly escalating and deadly contagious epidemic throughout China, followed by an increasing number of cases in other countries throughout the world.&lt;br /&gt;
On January 30, the WHO declared coronavirus a global emergency  as the death toll in China jumped to 170, with 7,711 cases reported in the country, where the virus had spread to all 31 provinces.  In mid-February WHO announced that the new coronavirus would be called &amp;ldquo;COVID-19&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;China&amp;rsquo;s bold approach to contain the rapid spread of this new respiratory pathogen  by massive lockdowns and electronic surveillance measures has changed the changed the course of the epidemic . The number of new infections reported in China has been declining gradually. With over 422,915 reported cases and more than 18,543 recorded deaths worldwide , the outbreak of COVID-19 has surpassed the toll of the 2002-2003 SARS outbreak, which also originated in China and is expected to continue to increase.  Although the infection originated in China, now the epicenter of the pandemic is Europe, which now has more cases reported each day than China did at the height of its outbreak.  In Italy alone the COVID-19 has infected more than 69,000 people and killed at least 6,800. There is an increasing number of cases in several EU/EEA countries without epidemiological links to explain the source of transmission. The speed with which COVID-19 can cause nationally incapacitating epidemics once transmission within the community is established indicates that it is likely that in a few weeks or even days, similar situations to those seen in China and Italy may be seen in other EU/EEA countries or the UK, as more countries report evidence of community transmission.
The COVID-19 virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes.  At the time of this writing, there are no specific vaccines or treatments for COVID-19 which is generally accepted.&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This is a post to give a basic understanding of statistical modelling of epidemiology. In past few days of Shelter-in-place lockdown situation in Kolkata (and throughout the whole India from today onwards), me and one of my ingenious friend &lt;a href=&#34;https://soham01.netlify.com/&#34; target=&#34;_blank&#34;&gt;Soham Bonnerjee&lt;/a&gt; (we were attending the same college before lockdown to purse Masters degree in Statistics), was reading about different epidemiological models available in the literature and using it to generate projections for the number of infected people in India.&lt;/p&gt;

&lt;p&gt;In this post, we shall explore the performance of deterministic SIR model which to be fitted using a least squares procedure. Then, we shall use it to generate projections for the epidemic situation in India, till the end of this lockdown, which is currently annouced to be remain till April 14, 2020, about 3 weeks from today.&lt;/p&gt;

&lt;h1 id=&#34;exploratory-analysis&#34;&gt;Exploratory Analysis&lt;/h1&gt;

&lt;p&gt;Before proceeding with introducing the SIR model, let us first read the data into &lt;code&gt;R&lt;/code&gt; (which is what we are going to use through out), and perform some exploratory analysis. We shall be using the &lt;a href=&#34;https://www.tidyverse.org/&#34; target=&#34;_blank&#34;&gt;tidyverse&lt;/a&gt; library, which is a collection of some very useful packages for data proprocessing and exploratory analysis.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(readr)
library(dplyr)
library(tidyr)
library(lubridate)
library(ggplot2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the related coronavirus, there are may different sources available. Many international organizations like WHO (World Health Organization), ECDC (European Centre for Disease Prevention and Control) and many national governments are releasing day to day basis publicly available data. As well as different news agencies are also collecting and compileing data from the hospitals and different other sources on a regular basis. John Hopkins University (JHU) CSSE department has also released a dataset compiled from the collection of these sources in a github repository &lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The datasets are being updated on a daily basis. We shall use this data provided by &lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19&#34; target=&#34;_blank&#34;&gt;JHU CSSE&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;confirmed.dat &amp;lt;- read_csv(&#39;./datasets/time_series_19-covid-Confirmed.csv&#39;)
deaths.dat &amp;lt;- read_csv(&#39;./datasets/time_series_19-covid-Deaths.csv&#39;)
recovered.dat &amp;lt;- read_csv(&#39;./datasets/time_series_19-covid-Recovered.csv&#39;)

confirmed.dat &amp;lt;- confirmed.dat %&amp;gt;%
    rename( State = `Province/State`, Country = `Country/Region` ) %&amp;gt;% 
    gather(key = &amp;quot;Date&amp;quot;, value = &amp;quot;Confirmed&amp;quot;, -c(1:4) ) %&amp;gt;%
    mutate(Date = mdy(Date))
    

deaths.dat &amp;lt;- deaths.dat %&amp;gt;%
    rename( State = `Province/State`, Country = `Country/Region` ) %&amp;gt;% 
    gather(key = &amp;quot;Date&amp;quot;, value = &amp;quot;Deaths&amp;quot;, -c(1:4) ) %&amp;gt;% 
    mutate(Date = mdy(Date))

recovered.dat &amp;lt;- recovered.dat %&amp;gt;%
    rename( State = `Province/State`, Country = `Country/Region` ) %&amp;gt;% 
    gather(key = &amp;quot;Date&amp;quot;, value = &amp;quot;Recovered&amp;quot;, -c(1:4) ) %&amp;gt;%
    mutate(Date = mdy(Date))


# merge all of them together
dat &amp;lt;- Reduce(merge, list(confirmed.dat, deaths.dat, recovered.dat))
dat &amp;lt;- as_tibble(dat)

knitr::kable(head(dat, 5))
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;State&lt;/th&gt;
&lt;th&gt;Country&lt;/th&gt;
&lt;th&gt;Lat&lt;/th&gt;
&lt;th&gt;Long&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;th&gt;Confirmed&lt;/th&gt;
&lt;th&gt;Deaths&lt;/th&gt;
&lt;th&gt;Recovered&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Adams, IN&lt;/td&gt;
&lt;td&gt;US&lt;/td&gt;
&lt;td&gt;39.8522&lt;/td&gt;
&lt;td&gt;-77.2865&lt;/td&gt;
&lt;td&gt;2020-01-22&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adams, IN&lt;/td&gt;
&lt;td&gt;US&lt;/td&gt;
&lt;td&gt;39.8522&lt;/td&gt;
&lt;td&gt;-77.2865&lt;/td&gt;
&lt;td&gt;2020-01-23&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adams, IN&lt;/td&gt;
&lt;td&gt;US&lt;/td&gt;
&lt;td&gt;39.8522&lt;/td&gt;
&lt;td&gt;-77.2865&lt;/td&gt;
&lt;td&gt;2020-01-24&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adams, IN&lt;/td&gt;
&lt;td&gt;US&lt;/td&gt;
&lt;td&gt;39.8522&lt;/td&gt;
&lt;td&gt;-77.2865&lt;/td&gt;
&lt;td&gt;2020-01-25&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adams, IN&lt;/td&gt;
&lt;td&gt;US&lt;/td&gt;
&lt;td&gt;39.8522&lt;/td&gt;
&lt;td&gt;-77.2865&lt;/td&gt;
&lt;td&gt;2020-01-26&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Now we shall try to see how the total number of Confirmed, deaths and recovered people changed across the globe.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;temp &amp;lt;- dat %&amp;gt;% group_by(Date) %&amp;gt;% 
    summarise(Confirmed = sum(Confirmed), Deaths = sum(Deaths), Recovered = sum(Recovered)) %&amp;gt;%
    gather(key = &amp;quot;Variable&amp;quot;, value = &amp;quot;Count&amp;quot;, -Date)

ggplot(temp, aes(x = Date, y = Count, color = Variable)) + geom_line(size = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The situation is very severe, as the current trend in exponentially increasing in terms of confirmed cases, and the growth rate of recovered is sufficiently slow. If we particular focus on the situation of India, (after 1st March, 2020),&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;unnamed-chunk-7-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As of now, there are very less number of recoveries, some deaths, and a lot of (about 400) affected people. We have also compiled how these changes occurs spatially in different countries. They are shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./confirmed-1.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./deaths-1.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;sir-model-description&#34;&gt;SIR Model Description&lt;/h1&gt;

&lt;p&gt;The SIR model is one of the compartmental models in epidemiology which is used to mathematically model the spread of an infectious disease. This model was firstly introduced by William Ogilvy Kermack and A. G. McKendrick, and named as &lt;strong&gt;Kermack-McKendrick Model.&lt;/strong&gt; However, with time, this model has obtained several variants, each being better than the one before.&lt;/p&gt;

&lt;p&gt;SIR modelling starts with defining 3 different compartments, of states in which a person can be. The states are as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR;
    S(Susceptible) --&amp;gt; I(Infected);
    I(Infected) --&amp;gt; R(Recovered / Removed);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Susceptibles&lt;/strong&gt; are the general population, who is susceptible to get the disease from an infectious person. &lt;strong&gt;Infected&lt;/strong&gt; state reperesents the persons who have the symptoms of the infection and is able to spread it. And finally, &lt;strong&gt;Recovered&lt;/strong&gt; or &lt;strong&gt;Removed&lt;/strong&gt; is the state when a person is recovered from the disease and gain immunity to it, or is dead. Let, $Y_t^S, Y_t^I, Y_t^R$ dentoes the number of people in these states respectively at the time $t$. The corresponding proportions are denoted by $\theta_t^S, \theta_t^I$ and $\theta_t^R$, where the proportion is defined as the number of people in a state divided by the total number of people, i.e. the population count. Note that, since these three states are assumed to be exhaustive, hence $Y_t^S + Y_t^I + Y_t^R = N$, where $N$ is the total population of the particular region under study.&lt;/p&gt;

&lt;p&gt;The mathematical relations between these quantities are defined as follows:&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
\dfrac{d\theta_t^S}{dt} &amp;amp; = -\beta \theta_t^S \theta_t^I\\&lt;br /&gt;
&amp;amp; \\&lt;br /&gt;
\dfrac{d\theta_t^I}{dt} &amp;amp; = \beta \theta_t^S \theta_t^I - \gamma \theta_t^I\\&lt;br /&gt;
&amp;amp; \\&lt;br /&gt;
\dfrac{d\theta_t^R}{dt} &amp;amp; = \gamma \theta_t^I\\&lt;br /&gt;
\end{align}
$$&lt;/p&gt;

&lt;p&gt;Note that, since $Y_t^S + Y_t^I + Y_t^R = N$, we have $\theta_t^S + \theta_t^I + \theta_t^R = 1$, which is constant. Hence, we must have,&lt;/p&gt;

&lt;p&gt;$$ \dfrac{d\theta_t^S}{dt} + \dfrac{d\theta_t^I}{dt} + \dfrac{d\theta_t^R}{dt} = 0 $$&lt;/p&gt;

&lt;p&gt;which is satisfied by the mathematical formulation. In this, $\beta, \gamma$ are unknown parameters which is to be estimated from the data.&lt;/p&gt;

&lt;p&gt;These mathematical equations did not drop from the sky. Let us understand how these mathematical formula emerges from an intuitive points of view.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;We consider the third equation first. It models the change in the number of recovered people. Now, the change in the number (or proportion) of recovered people can occur only when an infectious person, gets treatment, which happens with rate $\gamma$, which can be interpreted as the recovery rate of an infectious person. Therefore, it changes by the amount $\gamma \theta_t^I$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Now we consider the first equation. It models the change in the number of susceptible population. The change in susceptible population occurs, when an infectious person comes in contact with a susceptible person, and the infection spreads. Now, there are $Y_t^I Y_t^S$ many interactions possible, and each interaction would spread the virus with rate $\beta$ say. Considering proportions, we have the change being equal to $-\beta\theta_t^I\theta_t^S$, with the negative sign showing that number of susceptibles can only decrease.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Due to the restriction, $\dfrac{d\theta_t^S}{dt} + \dfrac{d\theta_t^I}{dt} + \dfrac{d\theta_t^R}{dt} = 0$, the choice of $\dfrac{d\theta_t^I}{dt}$ can be justfied from previous points.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A very important measure in this model is the quantity;&lt;/p&gt;

&lt;p&gt;$$R_0 = \dfrac{\beta}{\gamma}$$&lt;/p&gt;

&lt;p&gt;This basically interprets as the average number of people an infectious person infects before recovering or dying. So, if $R_0 &amp;lt; 1$, then an infectious person infects less than one person in average before recovering, which means the infection pandemic will eventually die out. Whereas, if $R_0 &amp;gt; 1$, then an infectious person infects more than one person in average before recovering, hence the number of infected would increase exponentially and eventually all of the population will become infected.&lt;/p&gt;

&lt;h1 id=&#34;estimation-of-sir-model&#34;&gt;Estimation of SIR Model&lt;/h1&gt;

&lt;p&gt;There are two possible ways of estimation of the parameters of an SIR model.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A deterministic estimation.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A stochastic estimation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A deterministic estimation does not require any other assumptions on the model parameters, as well as the data. It basically works simply on the basis of solution to the above differential equations. However, a stochastic estimation requires specifications of the distributional assumptions on the data, as well as model parameters. In this post, we are going to use only a deterministic setup of the model as explained in the mathematical formulation, nothing more.&lt;/p&gt;

&lt;p&gt;For this reason, we shall use &lt;a href=&#34;https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods&#34; target=&#34;_blank&#34;&gt;Runge Kutta methods&lt;/a&gt; of numerically solving a system of differential equations, under some particular choice of the model parameters $\beta, \gamma$. However, as we solve the differential equations, we shall be able to obtain estimates $\hat{Y}_t^S, \hat{Y}_t^I$ and $\hat{Y}_t^R$. Then, we can use a Least Squares approach to solve this problem, to estimate $\beta, \gamma$ as;&lt;/p&gt;

&lt;p&gt;$$
(\hat{\beta}, \hat{\gamma}) = \min_{\beta, \gamma} \sum_t \left[ \left(Y_t^S - \hat{Y}_t^S\right)^2 + \left(Y_t^I - \hat{Y}_t^I\right)^2 + \left(Y_t^R - \hat{Y}_t^R\right)^2 \right]
$$&lt;/p&gt;

&lt;p&gt;To this end, we write the function &lt;code&gt;pred.SIR&lt;/code&gt; and &lt;code&gt;LS.SIR&lt;/code&gt; which performs the prediction given the initial value and parameters, and computes the value of the objective function written above by tallying it with the observed data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pred.SIR &amp;lt;- function(n_time, beta, gamma, init.theta) {
    theta &amp;lt;- matrix(0, nrow = n_time, ncol = 3)
    theta[1, ] &amp;lt;- init.theta / sum(init.theta)
    
    for (t in 2:n_time) {
        # for each time, create the Runge Kutta approximation
        Km &amp;lt;- numeric(12)   # 12 coefficients are needed
        
        # computes coefficients of runge kutta
        Km[1] &amp;lt;- - beta * theta[t-1,1] * theta[t-1,2]
        Km[9] &amp;lt;- gamma * theta[t-1,2]
        Km[5] &amp;lt;- -Km[1]-Km[9]
        
        Km[2] &amp;lt;- - beta * (theta[t-1,1]+ 0.5*Km[1]) * (theta[t-1,2]+0.5*Km[5])
        Km[10] &amp;lt;- gamma*(theta[t-1,2]+0.5*Km[5])
        Km[6] &amp;lt;- -Km[2]-Km[10]
        
        Km[3] &amp;lt;- -beta*(theta[t-1,1]+0.5*Km[2])*(theta[t-1,2]+0.5*Km[6])
        Km[11] &amp;lt;- gamma*(theta[t-1,2]+0.5*Km[6])
        Km[7] &amp;lt;- -Km[3]-Km[11]
      
        Km[4] &amp;lt;- -beta*(theta[t-1,1]+Km[3])*(theta[t-1,2]+Km[7])
        Km[12] &amp;lt;- gamma*(theta[t-1,2]+Km[7])
        Km[8] &amp;lt;- -Km[4]-Km[12]
        
        innov.S &amp;lt;- ( Km[1] + 2 *Km[2] + 2*Km[3] + Km[4])/6
        innov.I &amp;lt;- ( Km[5] + 2 *Km[6] + 2*Km[7] + Km[8])/6
        innov.R &amp;lt;- ( Km[9] + 2 *Km[10] + 2*Km[11] + Km[12])/6
        
        theta[t, ] &amp;lt;- theta[(t-1), ] + c(innov.S, innov.I, innov.R)
        theta[t, ] &amp;lt;- theta[t, ]/sum(theta[t, ])
    }
    
    return(theta)
}
LS.SIR &amp;lt;- function(params, Y) {
    beta &amp;lt;- params[1]
    gamma &amp;lt;- params[2]
    n_time &amp;lt;- nrow(Y)
    N &amp;lt;- sum(Y[1, ])
    
    init.theta &amp;lt;- Y[1, ] / N
    preds &amp;lt;- pred.SIR(n_time, beta, gamma, init.theta)
    
    return( sum((Y - preds * N )^2) ) 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;performance-of-sir-model&#34;&gt;Performance of SIR model&lt;/h1&gt;

&lt;h2 id=&#34;estimation-for-hong-kong-china&#34;&gt;Estimation for Hong Kong, China&lt;/h2&gt;

&lt;p&gt;We choose the Hong Kong province in China to see how SIR model performs. According to World Bank data, the province Hong Kong is home to about $73.9$ lakhs people. Therefore, we have $N = 73.9\times 10^5$, in our model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 73.9e5   # population of Hong Kong
temp  &amp;lt;- dat %&amp;gt;% filter(Country == &amp;quot;China&amp;quot; &amp;amp; State == &amp;quot;Hong Kong&amp;quot;) %&amp;gt;% 
    mutate(Removed = Deaths + Recovered, Susceptible = N - Removed - Confirmed) %&amp;gt;% 
    select(Date, Susceptible, Confirmed, Removed)

knitr::kable(head(temp))
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;th&gt;Susceptible&lt;/th&gt;
&lt;th&gt;Confirmed&lt;/th&gt;
&lt;th&gt;Removed&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2020-01-22&lt;/td&gt;
&lt;td&gt;7390000&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2020-01-23&lt;/td&gt;
&lt;td&gt;7389998&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2020-01-24&lt;/td&gt;
&lt;td&gt;7389998&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2020-01-25&lt;/td&gt;
&lt;td&gt;7389995&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2020-01-26&lt;/td&gt;
&lt;td&gt;7389992&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2020-01-27&lt;/td&gt;
&lt;td&gt;7389992&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As you can see, the first appearence of Coronavirus in Hong Kong is on 23rd of Janurary, 2020. So, we should use the data from 2nd row onwards to fit into SIR model. We shall be using the data corresponding to $61$ days, i.e. till 23rd of May, 2020. Among this, we shall be training the model using data of first $47$ days, and then we build the prediction for next $14$ days, to see the performance of the fitted model. We shall be using &lt;code&gt;optim&lt;/code&gt; function of &lt;code&gt;R&lt;/code&gt; to optimize our objective function in order to find Least Sqaures estimate.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Y &amp;lt;- as.matrix(temp[2:48, 2:4])
ops &amp;lt;- optim(par = c(1e-2, 1e-5), fn = LS.SIR, method = &amp;quot;BFGS&amp;quot;, Y = Y, control = list(trace = 1))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;initial  value 552556.538521 
iter  10 value 68951.183015
final  value 65031.692121 
converged
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ops$par[1] / ops$par[2]   # R0
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[1] 3.149289
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have the estimated parameters, we can simply generate predictions for last $14$ days, using this &lt;code&gt;pred.SIR&lt;/code&gt; method. Before that, estimate of $R_0$ turns out to be higher than 1, thereby showing the seriousness of the COVID-19 epidemic situation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# since predictions are proportions, we multiply with the population to get the count estimates

preds &amp;lt;- pred.SIR(14, ops$par[1], ops$par[2], as.matrix(temp[49, 2:4])) * N
pred.dat &amp;lt;- tibble(Date = temp$Date[49:62], Pred.Confirmed = preds[, 2], Pred.Removed = preds[, 3])
pred.dat &amp;lt;- left_join(temp, pred.dat, by = c(&amp;quot;Date&amp;quot; = &amp;quot;Date&amp;quot;))  # create a full dataset containing predicted data as well

ggplot( pred.dat , aes(x = Date) ) +
    geom_line(aes(y = Confirmed), color = &amp;quot;black&amp;quot;, size = 1) +
    geom_line(aes(y = Pred.Confirmed), color = &amp;quot;blue&amp;quot;, size = 1, linetype = &amp;quot;dashed&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./unnamed-chunk-12-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Therefore, we see that SIR model overestimates the true number by about $75$ people at the last day. Nevertheless, this is simple model, which performs fairly good. However, it would have been nice if we could give a confidence interval around that estimate.&lt;/p&gt;

&lt;h2 id=&#34;performance-for-india&#34;&gt;Performance for India&lt;/h2&gt;

&lt;p&gt;We perform the same exercise for India as well. For India, the population is huge (about $131$ crores) and there is lesser amount of data available, although the first confirmed case of COVID-19 was identified in 30th January, 2020. We have data on $54$ days, among which we shall use all the data before last week, and generate predictions for last week, to visualize its performance.&lt;/p&gt;

&lt;p&gt;The $R_0$ coefficient turns out to be 12.4850813, which is severe as it is a lot more than $1$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;preds &amp;lt;- pred.SIR(7, ops$par[1], ops$par[2], as.matrix(temp[48, 2:4])) * N
pred.dat &amp;lt;- tibble(Date = temp$Date[48:54], Pred.Confirmed = preds[, 2], Pred.Removed = preds[, 3])
pred.dat &amp;lt;- left_join(temp, pred.dat, by = c(&amp;quot;Date&amp;quot; = &amp;quot;Date&amp;quot;))  # create a full dataset containing predicted data as well

ggplot( pred.dat , aes(x = Date) ) +
    geom_line(aes(y = Confirmed), color = &amp;quot;black&amp;quot;, size = 1) +
    geom_line(aes(y = Pred.Confirmed), color = &amp;quot;blue&amp;quot;, size = 1, linetype = &amp;quot;dashed&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./unnamed-chunk-14-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Yet in this case, the SIR model does a underestimation in determining the number of confirmed cases by about $100$ cases. Therefore, the seriousness of the pandemic situation is even worse than what is depicted by the number $R_0$, i.e. 12.4850813, it is enitrely possible that the true $R_0$ value is even more at the last week, thereby showing on average an infectious person is infecting more than $12$ persons, which is bad, seriously bad.&lt;/p&gt;

&lt;h2 id=&#34;predictions-till-the-end-of-the-lockdown&#34;&gt;Predictions till the end of the Lockdown&lt;/h2&gt;

&lt;p&gt;To make this prediction, we use all the available datapoints for estimation of the parameters. So, we refit the model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Y &amp;lt;- as.matrix(temp[, 2:4])
ops &amp;lt;- optim(par = c(1e-2, 1e-5), fn = LS.SIR, method = &amp;quot;BFGS&amp;quot;, Y = Y)

preds &amp;lt;- pred.SIR(22, ops$par[1], ops$par[2], as.matrix(temp[54, 2:4])) * N
pred.dat &amp;lt;- tibble(Date = temp$Date[54] + 1:22, 
                   Pred.Confirmed = preds[, 2], Pred.Removed = preds[, 3])
pred.dat &amp;lt;- full_join(temp, pred.dat, by = c(&amp;quot;Date&amp;quot; = &amp;quot;Date&amp;quot;))  # create a full dataset containing predicted data as well

ggplot( pred.dat , aes(x = Date) ) +
    geom_line(aes(y = Confirmed), color = &amp;quot;black&amp;quot;, size = 1) +
    geom_line(aes(y = Pred.Confirmed), color = &amp;quot;blue&amp;quot;, size = 1, linetype = &amp;quot;dashed&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./unnamed-chunk-15-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So, if there was no lockdown happening, the prediction of confirmed cases in India by middle of April would have be about $4000$.&lt;/p&gt;

&lt;p&gt;Note that SIR is the best-case scenario, hence this is an underestimate. Thus without any quarantining intervention, the situation looks grim; But all hope is not lost, for in the next post, we&amp;rsquo;ll show how different measures of quarantining, including lockdown, help decrease the rate of increase in confirmed cases. Till then stay united, and stay safe. We would get through this hard time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;WHO Guidelines to STAY SAFE:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The best way to prevent and slow down transmission is be well informed about the COVID-19 virus, the disease it causes and how it spreads.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Wash your hands frequently with an alcohol based rub or soap.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Maintain social distancing.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Avoid touching eyes, nose and mouth.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Practice respiratory hygiene.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If you have fever, cough and difficulty breathing, seek medical care early.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Stay informed and follow advice given by your healthcare provider.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Protect yourselves and protect others.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;dl&gt;
&lt;dt&gt;Signing off&lt;/dt&gt;
&lt;dd&gt;$\qquad &amp;mdash; $ Subhrajyoty Roy &amp;amp; Soham Bonnerjee
&lt;br /&gt;&lt;/dd&gt;
&lt;/dl&gt;

&lt;blockquote&gt;
&lt;p&gt;Stay informed, Stay home, Stay safe.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Image Segmentation Techniques</title>
      <link>/post/post5/</link>
      <pubDate>Mon, 23 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/post5/</guid>
      <description>

&lt;h2 id=&#34;a-lot-of-long-distance-good-luck&#34;&gt;A lot of long distance Good Luck&lt;/h2&gt;

&lt;p&gt;Currently, as I am writing this, it is a global pandemic situation all over the world, with a newly infectious disease caused by a newly discovered coronavirus, called COVID-19. According to the website to &lt;a href=&#34;https://www.who.int/health-topics/coronavirus&#34; target=&#34;_blank&#34;&gt;WHO (World Health Organization)&lt;/a&gt;, most people infected with the COVID-19 virus will experience mild to moderate respiratory illness and recover without requiring special treatment.  Older people, and those with underlying medical problems like cardiovascular disease, diabetes, chronic respiratory disease, and cancer are more likely to develop serious illness.&lt;/p&gt;

&lt;p&gt;There are about 13 thousands people we have lost to the adverse effect of the disease, and more than three hundred thousands persons currently fighting against this disease, which rapidly grew out of China just within last three to four months. People are currently comparing it to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Spanish_flu&#34; target=&#34;_blank&#34;&gt;Spanish flu&lt;/a&gt;, which took place roughly $100$ years before, and hence, almost none of us have any experience of dealing with a pandemic at such a large scale. As a countermeasure to reduce the spread of infectious disease, my hometown is at a Shelter-in-place lockdown, and hence I have almost nothing to do except scrapping through the internet to learn some new things that interests me.&lt;/p&gt;

&lt;p&gt;However, I sincerely wish you all to stay unite, and stay safe. We would get through the hard times.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;WHO Guidelines to STAY SAFE:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The best way to prevent and slow down transmission is be well informed about the COVID-19 virus, the disease it causes and how it spreads.&lt;/li&gt;
&lt;li&gt;Wash your hands frequently with an alcohol based rub or soap.&lt;/li&gt;
&lt;li&gt;Maintain social distancing.&lt;/li&gt;
&lt;li&gt;Avoid touching eyes, nose and mouth.&lt;/li&gt;
&lt;li&gt;Practice respiratory hygiene.&lt;/li&gt;
&lt;li&gt;If you have fever, cough and difficulty breathing, seek medical care early.&lt;/li&gt;
&lt;li&gt;Stay informed and follow advice given by your healthcare provider.&lt;/li&gt;
&lt;li&gt;Protect yourselves and protect others.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Now, as you have quite guessed from the title of the post, this is about segmenting an image about different regions. Image Segmentation refers to the task that is used to seperate foreground from the background of an image. Before understanding what it means, note that, an image can be described as a collection of geometric shapes (lines, curves, polygons etc.) having various aesthestics mapping to real life (or fictional) quantities or objects. I find it essentially similar to that of &lt;code&gt;ggplot&lt;/code&gt; plotting technique, we have a dataset comprising of some measurements or objects which we want to measure, and we can build a plot (read an image) out of it, by creating an aesthestic mapping of those measurements to geometric objects such as points, lines, curves, polygons etc. The book written by Leland Wilkinson named &lt;a href=&#34;https://books.google.co.in/books/about/The_Grammar_of_Graphics.html?id=ZiwLCAAAQBAJ&amp;amp;source=kp_book_description&amp;amp;redir_esc=y&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;The Grammar of Graphics&lt;/strong&gt;&lt;/a&gt; is one such book which describes the process in great detail.&lt;/p&gt;

&lt;p&gt;Let me start with an example. Let&amp;rsquo;s say you have an image of a red chair, with a white wall at the back. Now, when you look at the image, you are seeing possibly some polygons (which may have some curved edges), which defines different regions of a chair, each having a slightly different shade of colour, due to disparate reflective nature and absorption of photons when there is a lighting source used to illuminate the scene. Now, in simple terms, there is a latent dataset with rich measurements of different characteristics of the objects in the scene (namely the chair and the wall), namely containing the spatial and lighting informations, however we do not get to observe that.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x coordinate&lt;/th&gt;
&lt;th&gt;y coordinate&lt;/th&gt;
&lt;th&gt;z coordinate&lt;/th&gt;
&lt;th&gt;colour(rgb)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2.2123&lt;/td&gt;
&lt;td&gt;23.443&lt;/td&gt;
&lt;td&gt;0. 1768&lt;/td&gt;
&lt;td&gt;(240, 75, 90)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Now, an image can also be described as a mapping of the above rows to a rows like this (which is observable by means of the image at hand),&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x coordinate&lt;/th&gt;
&lt;th&gt;y coordinate&lt;/th&gt;
&lt;th&gt;colour(rgb)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;35&lt;/td&gt;
&lt;td&gt;(240, 75, 90)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;where the new x and y coordinates are coordinates in the 2-dimensional map described by the plane of the image. Now, to each of the rows of the latent dataset, we associated an object (because that is how we interprete it), for instance, the rows which consitute the 3-dimensional spatial coordinates of the location of atoms of the chiar, are associated with the chair, and the rest, maybe associated with the floor and the white wall. Image segmentation specifically refers to generating that interpretation in a systematic and automatic way, just by looking at the visual sensory information (i.e. the 2nd type of dataset), and the problem is difficult, as the mapping by which the latent dataset converted to this, is extremely complicated in nature (maybe unknown). Therefore, a perfect image segmentation achieves the hidden representation of an image into something that is more meaningful and easier to analyze.&lt;/p&gt;

&lt;h2 id=&#34;importing-necessary-python-packages&#34;&gt;Importing Necessary Python Packages&lt;/h2&gt;

&lt;p&gt;For this, we shall be using &lt;code&gt;numpy&lt;/code&gt; for vector and matrix manipulations, &lt;code&gt;cv2&lt;/code&gt;to read image files as a numpy array, and finally &lt;code&gt;matplotlib&lt;/code&gt; for various plotting related tasks. I am using python 3.7.4 for this.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: If you want to know an image can be represented in an array (or a tensor), go check out the section on &amp;ldquo;Making some utlity functions&amp;rdquo; on my other post on &lt;a href=&#34;https://subroy13.github.io/post/post3/&#34; target=&#34;_blank&#34;&gt;Texture Networks&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2
import numpy as np
import matplotlib.pyplot as plt
import sys
sys.version
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&#39;3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I shall be using the beautiful picture of &lt;a href=&#34;https://en.wikipedia.org/wiki/Mona_Lisa&#34; target=&#34;_blank&#34;&gt;Mona Lisa&lt;/a&gt; by Leaonardo Da Vinci. I am not a connoisseur about aesthestics, but I appreciate the image from the point of view that it has a subject, and a background, two very clearly seperated item in the image.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bw_img = cv2.imread(&#39;./Mona_Lisa.jpg&#39;)  # read a black and white version
col_img = cv2.imread(&#39;./Mona_Lisa_color.jpg&#39;)   # and a color version

# since, cv2 reads in BGR format, we need to use RGB format for plotting
col_img = cv2.cvtColor(col_img, cv2.COLOR_BGR2RGB)

fig, plots = plt.subplots(1,2, figsize = (15, 10))     # subplot with 1 row, 2 columns

# subplot 1
plots[0].imshow(bw_img, cmap=&#39;gray&#39;)
plots[0].set_title(&#39;Black &amp;amp; White&#39;)

# subplot 2
plots[1].imshow(col_img)
plots[1].set_title(&#39;Colored&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_5_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;histogram-thresholding&#34;&gt;Histogram Thresholding&lt;/h2&gt;

&lt;p&gt;Histogram thresholding is the most basic way of segmenting an image. It is fast, simple yet very effective. For instance, assume we have a grayscale image, where the foreground and background has different grayscale intensities. Let us say, we have the grayscale intensity $i$ ($0 \leq i \leq 255$), appearing $f_i$ times in the image (i.e. there are $f_i$ pixels with grayscale intensity level $i$), and $N$ is the total number of digital pixels in the image. Now, assume that, the background object has a mean intensity level $\mu_b$, and foreground has mean intensity level $\mu_f$. Then, we assume,&lt;/p&gt;

&lt;p&gt;$$
I_{x, y} \sim
\begin{cases}
g(i_{x, y}; \mu_f) &amp;amp; \text{if (x, y) pixel belongs to foreground} \\&lt;br /&gt;
g(i_{x, y}; \mu_b) &amp;amp; \text{if (x, y) pixel belongs to background} \\&lt;br /&gt;
\end{cases}
$$&lt;/p&gt;

&lt;p&gt;where $I_{x, y}$ is the intensity level at (x, y)-th pixel of the image, which is a random variable (and is taking value $i_{x, y}$) and follows a density $g(\cdot)$ whose location is specified by $\mu_f$ or $\mu_b$, based on whether the pixel corresponds to background object (the white wall) or the foreground object (the red chair).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(bw_img.ravel(), bins = 255)
plt.title(&#39;Histogram of grayscale values of Mona Lisa Image&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_7_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, there are 3 clear regions in the histogram, which might indicates, there are 3 such segments possible, since it looks like the histogram comprises of mixture of 3 random variables.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First component has a mean about at 5.&lt;/li&gt;
&lt;li&gt;Second component shows a mode (or peak) about at 45.&lt;/li&gt;
&lt;li&gt;Third component shows a mode at 130.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, we consider some thresholds, at 25 and 95. Now, we shall simply use these thresholds, and assign different values to all pixel intensities that are smaller than 25 to one value, all in between 25 and 95 to another value, and all pixels higher than 95 to another value. This should give us some possible segmentation, although not automatic, but is useful to gain insights and idea about the method.&lt;/p&gt;

&lt;p&gt;The following function performs the segmentation given the values of the thresholds.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def segment(img, thresholds):
    bins = [0]    # these are used to binning the image
    for i in range(len(thresholds)):
        bins.append( thresholds[i] )
    bins.append(256)
    
    img = np.array(img)
    bins = np.array(bins)
    segment_bin = np.digitize(img, bins)
    return(bins[segment_bin] )    # return the segmented image&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we apply the threshold 25, 95 and both to see which regions are being identified as foreground and backgrund for which thresholds.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;thres1 = [25]
thres2 = [95]
thres3 = [25, 95]

fig, plots = plt.subplots(1,3, figsize = (15, 10))     # subplot with 1 row, 3 columns

# subplot 1
plots[0].imshow(segment(bw_img, thres1), cmap=&#39;gray&#39;)
plots[0].set_title(&#39;Segment with threshold = 25&#39;)

# subplot 2
plots[1].imshow(segment(bw_img, thres2), cmap=&#39;gray&#39;)
plots[1].set_title(&#39;Segment with threshold = 95&#39;)

# subplot 3
plots[2].imshow(segment(bw_img, thres3), cmap=&#39;gray&#39;)
plots[2].set_title(&#39;Segment with threshold = 25 &amp;amp; 95&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_11_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It seems that, the threshold at 25 separates the clothings and darker shades of skin of Mona Lisa from the background, while threshold at 95 tend to separate the lighter shades of skin (the face, the hands). Combining both yields a good understanding of the different segments available in the image.&lt;/p&gt;

&lt;p&gt;Now, let us perform similar analysis on the coloured version of the image. In that case, we process each of the channel seperately through this procedure.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, plots = plt.subplots(1,3, figsize = (15, 3))     # subplot with 1 row, 3 columns

# subplot 1
plots[0].hist(col_img[:,:,0].ravel(), bins = 255)
plots[0].set_title(&#39;Histogram of R channel&#39;)

# subplot 2
plots[1].hist(col_img[:,:,1].ravel(), bins = 255)
plots[1].set_title(&#39;Histogram of G channel&#39;)

# subplot 3
plots[2].hist(col_img[:,:,2].ravel(), bins = 255)
plots[2].set_title(&#39;Histogram of B channel&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_13_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now, it seems a lot more difficult to separate background from foreground on the basis of thresholds just by subjectively judging them.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;thres_R = [50, 100]
thres_G = [100]
thres_B = [55]

segment_R = np.expand_dims( segment(col_img[:, :, 0], thres_R) , 2)    # apply segmentation and convert to 3d array
segment_G = np.expand_dims( segment(col_img[:, :, 1], thres_G) , 2)
segment_B = np.expand_dims( segment(col_img[:, :, 2], thres_B) , 2)

segment_img = np.concatenate( (segment_R, segment_G, segment_B), axis = 2)


fig, plots = plt.subplots(1,4, figsize = (20, 5))     # subplot with 1 row, 4 columns

# subplot 1
plots[0].imshow(segment_R[:, :, 0], cmap = &#39;gray&#39;)
plots[0].set_title(&#39;Segmentation of R channel&#39;)

# subplot 2
plots[1].imshow(segment_G[:, :, 0], cmap = &#39;gray&#39;)
plots[1].set_title(&#39;Segmentation of G channel&#39;)

# subplot 3
plots[2].imshow(segment_B[:, :, 0], cmap = &#39;gray&#39;)
plots[2].set_title(&#39;Segmentation of B channel&#39;)

# final image
plots[3].imshow(segment_img)
plots[3].set_title(&#39;Final segmentation of image&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_15_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Therefore, we see that most the segmentation generally comes from the G channel itself, but the final segmentation does not look very promising. As you can see, there are lots of artifacts and noises all around. Therefore, we need our workaround against this, which we shall be discussing shortly.&lt;/p&gt;

&lt;h2 id=&#34;otsu-s-method&#34;&gt;Otsu&amp;rsquo;s Method&lt;/h2&gt;

&lt;p&gt;Otsu&amp;rsquo;s method is a automatic way of detecting such thresholds. To better understand it, consider a threshold value $s$. Then, the best estimate of $\mu_f$ and $\mu_b$ are as follows:&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
\mu_b^{(s)} &amp;amp; = \dfrac{\sum_{i = 0}^{s} i f_i }{\sum_{i = 0}^{s} f_i}\\&lt;br /&gt;
\mu_f^{(s)} &amp;amp; = \dfrac{\sum_{i = (s+1)}^{255} i f_i }{\sum_{i = (s+1)}^{255} f_i}\\&lt;br /&gt;
\end{align}
$$&lt;/p&gt;

&lt;p&gt;where the superscript is used to denote the threshold $s$.&lt;/p&gt;

&lt;p&gt;Assuming that, foreground has higher lower intensity level than the foreground (or object) due to the fact that object is properly illuminated, one way to choose the threshold that minimizes the weighted intraclass variance, or conversely maximize the weighted interclass variance. Now, the formula of weighted interclass variance is simply;&lt;/p&gt;

&lt;p&gt;$$V = \dfrac{{\sum_{i = 0}^{s} f_i}}{N} \times \dfrac{{\sum_{i = 0}^{s} f_i}}{N} \times \left[ \mu_b^{(s)}  - \mu_f^{(s)}\right]^2$$&lt;/p&gt;

&lt;p&gt;The following function computes this interclass variance for all threshold levels $s = 1, 2, \dots 254$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def interclass_var(img):
    var = np.zeros(256)
    freq = np.zeros(256)
    
    # obtain f_i&#39;s
    for i in range(img.shape[0]):
        for j in range(img.shape[1]):
            freq[img[i, j]] += 1     # increase corresponding intensity frequency
            
    for s in range(1, 255):
        w1 = freq[0:s].sum()
        w2 = freq[s:].sum()
        if w1!=0 and w2!= 0:
            mu1 = sum( np.arange(s) * freq[0:s] ) / w1
            mu2 = sum( np.arange(s, 256) * freq[s:] ) / w2
            var[s] = w1 * w2 * ((mu1 - mu2)**2)
    
    return var
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;var = interclass_var(bw_img)
plt.plot(var)
plt.title(&amp;quot;Otku&#39;s Interclass variation&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_18_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.argmax(var)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;78
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It seems that 78 is the proper threshold where the interclass variance is maximized. So, let us take a look how the segmentation is for only one threshold equal to 78.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;thres = [78]

fig, plots = plt.subplots(1,2, figsize = (10, 5))     # subplot with 1 row, 2 columns

# subplot 1
plots[0].imshow(bw_img, cmap=&#39;gray&#39;)
plots[0].set_title(&#39;Original Image&#39;)

# subplot 2
plots[1].imshow(segment(bw_img, thres), cmap=&#39;gray&#39;)
plots[1].set_title(&#39;Segment with threshold = 78&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_21_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We see that, the segmentation looks pretty good for just two shades of gray. However, it would have been better if the background was given a different shade from &lt;a href=&#34;https://en.wikipedia.org/wiki/Lisa_del_Giocondo&#34; target=&#34;_blank&#34;&gt;Lisa del Giocondo&lt;/a&gt;, the possible model for Mona Lisa. However, since in the original picture, they both have similar shades of gray, we cannot do it just based on the histogram of colour intensities. So, we need to account for spatial information as well.&lt;/p&gt;

&lt;p&gt;Let us see, how much this automatic thresholding give us in the colour image of Mona Lisa.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;var_r = interclass_var(col_img[:, :, 0])
var_g = interclass_var(col_img[:, :, 1])
var_b = interclass_var(col_img[:, :, 2])

print(&#39;Threshold for R channel&#39;, np.argmax(var_r))
print(&#39;Threshold for G channel&#39;, np.argmax(var_g))
print(&#39;Threshold for B channel&#39;, np.argmax(var_b))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Threshold for R channel 94
Threshold for G channel 82
Threshold for B channel 61
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Exactly same code as before
thres_R = [94]
thres_G = [82]
thres_B = [61]

segment_R = np.expand_dims( segment(col_img[:, :, 0], thres_R) , 2)    # apply segmentation and convert to 3d array
segment_G = np.expand_dims( segment(col_img[:, :, 1], thres_G) , 2)
segment_B = np.expand_dims( segment(col_img[:, :, 2], thres_B) , 2)

segment_img = np.concatenate( (segment_R, segment_G, segment_B), axis = 2)

fig, plots = plt.subplots(1,4, figsize = (20, 5))     # subplot with 1 row, 4 columns

# subplot 1
plots[0].imshow(segment_R[:, :, 0], cmap = &#39;gray&#39;)
plots[0].set_title(&#39;Segmentation of R channel&#39;)

# subplot 2
plots[1].imshow(segment_G[:, :, 0], cmap = &#39;gray&#39;)
plots[1].set_title(&#39;Segmentation of G channel&#39;)

# subplot 3
plots[2].imshow(segment_B[:, :, 0], cmap = &#39;gray&#39;)
plots[2].set_title(&#39;Segmentation of B channel&#39;)

# final image
plots[3].imshow(segment_img)
plots[3].set_title(&#39;Final segmentation of image&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_24_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The final segmentation clearly segments all the regions, like background and the model. However, it is extremely noisy. So, otsu&amp;rsquo;s automatic thresholding does not perform any better for colour images.&lt;/p&gt;

&lt;h2 id=&#34;incorporating-spatial-information-co-occurence-matrix&#34;&gt;Incorporating Spatial Information: Co-occurence Matrix&lt;/h2&gt;

&lt;p&gt;To incorporate spatial information, as well as consider the intensity values of the image, there is a popular notion of Co-occurence matrix. As the name suggests, it captures how many times two intensity levels $i$ and $j$ appear spatially together in the image, and put that frequency in $(i, j)$-th cell of the matrix.&lt;/p&gt;

&lt;p&gt;For example, the $(a, b)$-th entry of the co-occurence matrix is the frequency of the number of times, when $a$ appears at a pixel location of the image, and the color $b$ appears at the specified offset from that pixel locations. Now, if the offset is set to be $(1, 0)$, then it means the number of times the pixel intensities $(a, b)$ appear side by side horizontally in this order is given by the $(a, b)$-th element of the co-occurence matrix.&lt;/p&gt;

&lt;p&gt;So, let us create a function which computes the co-occurence matrix for us.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create co-occurence matrix
def CCM(img, offset):
    comat = np.zeros((256, 256))
    for i in range(img.shape[0] - offset[0]):
        for j in range(img.shape[1] - offset[1]):
            comat[img[i, j], img[i + offset[0], j + offset[1]] ] += 1
    return(comat)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have our co-occurence matrix, consider a specified threshold $s$. Then, the $256\times 256$ order co-occurence matrix can be partitioned into $4$ parts as follows;&lt;/p&gt;

&lt;p&gt;$$
\begin{bmatrix}
A &amp;amp; B\\&lt;br /&gt;
C &amp;amp; D
\end{bmatrix}
$$&lt;/p&gt;

&lt;p&gt;where $A$ is of order $s \times s$. Let, $a, b, c, d$ denotes the sum of the entries in partioned regions $A, B, C, D$ respectively. Clearly, if the threshold is selected as such so that background and foreground can be broken up nicely, then it means, the entries of $B$ and $C$ are going to be small. Since, they represent the number of times a background color changes to foreground color and vice-versa. Armed with this knowledge, two very simple measures we can propose.&lt;/p&gt;

&lt;p&gt;$$m_1 = \dfrac{b+c}{a + b + c + d}$$&lt;/p&gt;

&lt;p&gt;and another based on conditional probability;&lt;/p&gt;

&lt;p&gt;$$m_2 = \dfrac{1}{2} \left( \dfrac{b}{a+b} + \dfrac{c}{c+d} \right)$$&lt;/p&gt;

&lt;p&gt;Minimizing these disparity measures with respect to the choice of $s$, should give us a reasonably good segmentation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def discre_measure(comat, s):
    a = comat[0:s, 0:s].sum()
    b = comat[0:s, s:].sum()
    c = comat[s:, 0:s].sum()
    d = comat[s:, s:].sum()
    
    m1 = (b + c)/(a + b + c + d)
    m2 = ((b / (a + b)) + (c / (c + d)) )/2
    
    return((m1, m2))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_h = CCM(bw_img, offset = (0, 1))   # consider horizontal offset
T_v = CCM(bw_img, offset = (1, 0))   # consider vertical offset
T_hv = T_h + T_v   # sum to obtain a good spatially informed co-occurence like matrix
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we compute these measures for all thresholds from $0$ to $255$, and then plot their graphs in order to find the minimum of the objective function (or the measure)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;scores_m1 = []
scores_m2 = []

for s in range(1, 255):
    score = discre_measure(T_hv, s)
    scores_m1.append( score[0] )
    scores_m2.append( score[1] )
    
fig, plots = plt.subplots(1,2, figsize = (20, 5))     # subplot with 1 row, 2 columns

# subplot 1
plots[0].plot(scores_m1)
plots[0].set_title(&#39;Measure 1&#39;)

# subplot 2
plots[1].plot(scores_m2)
plots[1].set_title(&#39;Measure 2&#39;)
plots[1].set_ylim([0, 0.1])

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_31_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now, there are several local minimas. Note that, using a global rule for obtaining the minimum value would possibly lead to a poor segmentation. Hence, we apply a moving window through the array of these scores, to find a value which is minimum in a window span of length $2K$, where $K$ is a choice by the user. Clearly, higher values of $K$ will lead to smaller number of segments, and lower values of $K$ will give higher number of segments. This moving minimum algorithm is implemented in the following piece of code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def find_min(scores, k = 1):
    thresholds = []
    for i in range(k, len(scores) - k):
        if min(scores[(i-k):(i+k)]) == scores[i]:
            thresholds.append(i)
    return(thresholds)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we consider the images segmentation for $k = 1, 3, 5, 10, 20$, firstly for Measure 1 and then for Measure 2.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, plots = plt.subplots(1,5, figsize = (20, 5))     # subplot with 1 row, 5 columns

# subplot 1
plots[0].imshow( segment(bw_img, find_min(scores_m1, k = 1)) , cmap = &#39;gray&#39;)
plots[0].set_title(&#39;k = 1 ( Measure 1 )&#39;)

# subplot 2
plots[1].imshow( segment(bw_img, find_min(scores_m1, k = 3)) , cmap = &#39;gray&#39;)
plots[1].set_title(&#39;k = 3 ( Measure 1 )&#39;)

# subplot 3
plots[2].imshow( segment(bw_img, find_min(scores_m1, k = 5)) , cmap = &#39;gray&#39;)
plots[2].set_title(&#39;k = 5 ( Measure 1 )&#39;)

# subplot 4
plots[3].imshow( segment(bw_img, find_min(scores_m1, k = 10)) , cmap = &#39;gray&#39;)
plots[3].set_title(&#39;k = 10 ( Measure 1 )&#39;)

# subplot 4
plots[4].imshow( segment(bw_img, find_min(scores_m1, k = 20)) , cmap = &#39;gray&#39;)
plots[4].set_title(&#39;k = 20 ( Measure 1 )&#39;)


plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_35_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, plots = plt.subplots(1,5, figsize = (20, 5))     # subplot with 1 row, 5 columns

# subplot 1
plots[0].imshow( segment(bw_img, find_min(scores_m2, k = 1)) , cmap = &#39;gray&#39;)
plots[0].set_title(&#39;k = 1 ( Measure 2 )&#39;)

# subplot 2
plots[1].imshow( segment(bw_img, find_min(scores_m2, k = 3)) , cmap = &#39;gray&#39;)
plots[1].set_title(&#39;k = 3 ( Measure 2 )&#39;)

# subplot 3
plots[2].imshow( segment(bw_img, find_min(scores_m2, k = 5)) , cmap = &#39;gray&#39;)
plots[2].set_title(&#39;k = 5 ( Measure 2 )&#39;)

# subplot 4
plots[3].imshow( segment(bw_img, find_min(scores_m2, k = 10)) , cmap = &#39;gray&#39;)
plots[3].set_title(&#39;k = 10 ( Measure 2 )&#39;)

# subplot 4
plots[4].imshow( segment(bw_img, find_min(scores_m2, k = 20)) , cmap = &#39;gray&#39;)
plots[4].set_title(&#39;k = 20 ( Measure 2 )&#39;)


plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_36_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It looks like for $k = 10$, with measure 1 and for $k = 20$ with meausre 2, we have similar kind of segmentation of the image. However, with measure 2, $k = 20$, the segmentation looks much clearer and ideal, as there are light noises to the background, and almost no noises in definiting the edge of the clothings of the model from the background.&lt;/p&gt;

&lt;p&gt;Now, let us inspect how does the thresholds found by this algorithm maps to the co-occurence matrix.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cutoffs1 = np.array(find_min(scores_m1, k = 10))
cutoffs2 = np.array(find_min(scores_m2, k = 20))

fig, plots = plt.subplots(1, 2, figsize = (20, 5))     # subplot with 1 row, 2 columns

# subplot 1
a = plots[0].imshow(np.log(T_hv))   # take logarithm for ease of visualization
fig.colorbar(a, ax = plots[0])
for cut in cutoffs1:
    plots[0].plot( [cut, cut], [255, 0] , &amp;quot;r-&amp;quot;)
    plots[0].plot( [255, 0], [cut, cut], &amp;quot;r-&amp;quot;)

# subplot 2
b = plots[1].imshow(np.log(T_hv))   # take logarithm for ease of visualization
fig.colorbar(b, ax = plots[1])
for cut in cutoffs2:
    plots[1].plot( [cut, cut], [255, 0] , &amp;quot;r-&amp;quot;)
    plots[1].plot( [255, 0], [cut, cut], &amp;quot;r-&amp;quot;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_38_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, there are lots of unnecessary thresholds selected for measure1. However, then two principal thresholds selected by these methods are at about $25$ and at about $95$, which were the thresholds we picked at the very beginning just by looking at the thresholds. Even with the very basic tool such a histogram, and a little subjective evaluation, we could achieve a reasonable amount of segmentation.&lt;/p&gt;

&lt;p&gt;Now, let us try to apply this method on the colour image as well.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_hv_red = CCM(col_img[:, :, 0], offset = (0, 1)) + CCM(col_img[:, :, 0], offset = (1, 0))
T_hv_green = CCM(col_img[:, :, 1], offset = (0, 1)) + CCM(col_img[:, :, 1], offset = (1, 0))
T_hv_blue = CCM(col_img[:, :, 2], offset = (0, 1)) + CCM(col_img[:, :, 2], offset = (1, 0))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;scores_m1 = np.zeros((3, 256))
scores_m2 = np.zeros((3, 256))

for s in range(1, 255):
    score_red = discre_measure(T_hv_red, s)
    score_green = discre_measure(T_hv_green, s)
    score_blue = discre_measure(T_hv_blue, s)
    
    scores_m1[:, s] =  (score_red[0], score_green[0], score_blue[0])
    scores_m2[:, s] =  (score_red[1], score_green[1], score_blue[1])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, plots = plt.subplots(1, 2, figsize = (20, 5))     # subplot with 1 row, 2 columns

# subplot 1
plots[0].plot(scores_m1[0, :], c = &#39;r&#39;)
plots[0].plot(scores_m1[1, :], c = &#39;g&#39;)
plots[0].plot(scores_m1[2, :], c = &#39;b&#39;)
plots[0].set_title(&#39;Measure 1&#39;)

# subplot 2
plots[1].plot(scores_m2[0, :], c = &#39;r&#39;)
plots[1].plot(scores_m2[1, :], c = &#39;g&#39;)
plots[1].plot(scores_m2[2, :], c = &#39;b&#39;)
plots[1].set_title(&#39;Measure 2&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_42_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note that, for Red and Blue channel, the 2nd measure gives prominent minimums, while for the  Green channel, the prominent minima is obtained by the first measure. Therefore, with some custom tweaking of $K$ and the which measure to choose, you can get a segmentation which is better than methods discussed previously, just because it uses the spatial information, which connects these three channels together.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Exactly same code as before
thres_R = find_min(scores_m2[0, :], k = 30)
thres_G = find_min(scores_m1[1, :], k = 25)
thres_B = find_min(scores_m2[2, :], k = 10)

segment_R = np.expand_dims( segment(col_img[:, :, 0], thres_R) , 2)    # apply segmentation and convert to 3d array
segment_G = np.expand_dims( segment(col_img[:, :, 1], thres_G) , 2)
segment_B = np.expand_dims( segment(col_img[:, :, 2], thres_B) , 2)

segment_img = np.concatenate( (segment_R, segment_G, segment_B), axis = 2)

fig, plots = plt.subplots(1,4, figsize = (20, 5))     # subplot with 1 row, 4 columns

# subplot 1
plots[0].imshow(segment_R[:, :, 0], cmap = &#39;gray&#39;)
plots[0].set_title(&#39;Segmentation of R channel&#39;)

# subplot 2
plots[1].imshow(segment_G[:, :, 0], cmap = &#39;gray&#39;)
plots[1].set_title(&#39;Segmentation of G channel&#39;)

# subplot 3
plots[2].imshow(segment_B[:, :, 0], cmap = &#39;gray&#39;)
plots[2].set_title(&#39;Segmentation of B channel&#39;)

# final image
plots[3].imshow(segment_img)
plots[3].set_title(&#39;Final segmentation of image&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./Image%20Segmentation%20Techniques_44_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As it seems, the final segmented image looks better than the previous ones. Note that,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The sky is coloured green, (since the green channel was able to capture that properly)&lt;/li&gt;
&lt;li&gt;The visible first layer of background, which is the valley, is coloured blue, as blue channel properly captures that.&lt;/li&gt;
&lt;li&gt;The visible second layer of background, which constitutes what is possibly iceberg (or some rivers) is captures in the red channel (and hence is of brown colour in the final segmentation).&lt;/li&gt;
&lt;li&gt;The skin of Mona Lisa is capture in red and green channel, and slightly by blue channel, which mixes up to give a different tone for the skin.&lt;/li&gt;
&lt;li&gt;The dark clothing is not captured through any of the channel segmentation, hence remains dark in the final outcome.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following are some references if you wish to boost up your knowledge and learn subtle details of the algorithms presented above.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;F. Deravi and S.K. Pal, &amp;ldquo;Gray Level Thresholding Using Second-order Statistics&amp;rdquo;, Pattern Recogn. Letters, vol. 1, pp.417-422, 1983. &lt;a href=&#34;https://www.isical.ac.in/~sankar/paper/pdf21.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Thresholding_(image_processing)&#34; target=&#34;_blank&#34;&gt;Wikipedia article Thresholding (image processing).&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Otsu%27s_method&#34; target=&#34;_blank&#34;&gt;Wikipedia article Otsu&amp;rsquo;s Method&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Image_segmentation&#34; target=&#34;_blank&#34;&gt;Wikipedia article Image Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Linda G. Shapiro and George C. Stockman (2001): âComputer Visionâ, pp 279-325, New Jersey, Prentice-Hall, ISBN 0-13-030796-3 &lt;a href=&#34;http://nana.lecturer.pens.ac.id/index_files/referensi/computer_vision/Computer%20Vision.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;em&gt;The model, Lisa del Giocondo, was a member of the Gherardini family of Florence and Tuscany, and the wife of wealthy Florentine silk merchant Francesco del Giocondo. The Italian name for the painting, La Gioconda, means &amp;lsquo;jocund&amp;rsquo; (&amp;lsquo;happy&amp;rsquo; or &amp;lsquo;jovial&amp;rsquo;) or, literally, &amp;lsquo;the jocund one&amp;rsquo;, a pun on the feminine form of Lisa&amp;rsquo;s married name, Giocondo. In French, the title La Joconde has the same meaning, i.e. to become happy. ~ Wikipedia&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&amp;amp; Finally, stay safe and keep safe your families, friends and neighbours!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intuitive Introduction to Differential Geometry</title>
      <link>/post/post4/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/post4/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In high school level, we generally hear about &lt;a href=&#34;https://en.wikipedia.org/wiki/Euclidean_geometry&#34;&gt;Euclidean Geometry&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Analytic_geometry&#34;&gt;Co-ordinate Geometry&lt;/a&gt;. If you are familiar in Co-ordinate Geometry, then you should be able to feel that theorems and consequences in Euclidean Geometry can be naturally extended to Co-ordinate Geometry. However, Co-ordinate Geometry is extremely specific to the coordinate system that we are using, for instance, if we are working on a plane (i.e. similar to Euclidean Geometry), then a planar coordinate system or &lt;a href=&#34;https://en.wikipedia.org/wiki/Cartesian_coordinate_system&#34;&gt;Cartesian co-ordinate system&lt;/a&gt; is naturally used. However, when we are performing geometry on a sphere, naturally, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Polar_coordinate_system&#34;&gt;Polar coordinate system&lt;/a&gt; is used. However, suppose you are a Geographist, who wants to devise a coordinate system for the natural geography of Earth, clearly a Polar or Spherical Coordinate system does not work, since the shape is Earth is similar to that of an ellipsoid. So, it could be very much useful if we can generalize the idea of a coordinate system, to naturally represent any type of curvature, for instance, the image shown at the very beginning.&lt;/p&gt;

&lt;p&gt;Most of the recent advancements of geometry are developed for the specific needs of visualizing different abstract concepts of diverse mathematical fields. Generalizing the idea of the coordinate system would allow one to think some abstract set (or family or collection of objects pertaining to the related mathematical theory) as a curvature, to which we can associate geometrical objects and visualize them in a different and distinguished way.&lt;/p&gt;

&lt;h1 id=&#34;a-general-curvature-manifold&#34;&gt;A General Curvature: Manifold&lt;/h1&gt;

&lt;p&gt;Suppose you want to create a map of the earth, however, the earth is not flat (at least I am not a believer of &lt;a href=&#34;https://en.wikipedia.org/wiki/Flat_Earth&#34;&gt;Flat Earth Theory&lt;/a&gt;), so one would primarily find it confusing that we can create a flat map of the earth. For example, consider the following map from &lt;a href=&#34;https://www.welt-atlas.de/worldatlas&#34;&gt;World Atlas&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;earth-map.png&#34; alt=&#34;&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;But see that, we have a single location (like some part of Alaska &amp;amp; Russia) being mapped to two different places in the map. Also note that, due to the curvature of earth&#39;s surface, the supposedly gridlines are mapped to curved lines on the map. But if you look at it, there is a nice structure in the map, all the curved gridlines look very natural and aesthetically pleasing to understand. Moreover, if you draw tangent on the horizontal curves and tangent on the vertical curves, at their intersection point, you would find them meeting at a right angle. And if you are good at the field of &lt;strong&gt;Complex Analysis&lt;/strong&gt;, then you would probably figure out that it has something to do with analytic functions.&lt;/p&gt;

&lt;p&gt;Let us formalize the intuitions that we have. We have an arbitrary set $S$, which can be called a &lt;strong&gt;Manifold&lt;/strong&gt;, if we have a one-one mapping $\phi : S\rightarrow \mathbb{R}^n$ for some $n\in \mathbb{N}$. Since the mapping is one-one, we can identify each point of $S$ by a $n$-length vector $v \in \mathbb{R}^n$, which can be used to serve as a coordinate system for the points in $S$. Going back to our example of earth, to identify a location in the surface of the earth, we lay out the map and find the x-coordinates (latitude) and the y-coordinates (longitude) of the particular location. So, we have a mapping (created by geographists) that maps $S$, the surface of the earth to $\mathbb{R}^2$. As you might have guessed, the number $n$ is the dimension of the manifold.&lt;/p&gt;

&lt;p&gt;Let me cite another motivating example from &lt;a href=&#34;https://en.wikipedia.org/wiki/Manifold&#34;&gt;Wikipedia page about Manifolds&lt;/a&gt; to show that only one mapping may not be good enough.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Circle_with_overlapping_manifold_charts.svg/1024px-Circle_with_overlapping_manifold_charts.svg.png&#34; width = &#34;200px&#34;&gt;&lt;/p&gt;

&lt;p&gt;Note that, the circle is essentially a curved line, with ends being merged together. Clearly, it is a one dimensional structure, although embedded on a 2-dimensional plane. So, to interpret it as a manifold, we need to create (a or several) one-one mapping(s) from it to $\mathbb{R}$. Let us consider the circle having center at the origin and having radius equal to 1 unit. Here, we shall use $4$ different one-one mappings from part of the circle to the unit interval $[0, 1]$ which is a subset of $\mathbb{R}$.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
\phi_{Top}((x, y)) &amp; = (x + 1)/2 \text{ if } y \geq 0\\
\phi_{Left}((x, y)) &amp; = (y + 1)/2 \text{ if } x &lt; 0\\
\phi_{Right}((x, y)) &amp; = (y + 1)/2 \text{ if } x \geq 0\\
\phi_{Bottom}((x, y)) &amp; = (x + 1)/2 \text{ if } y &gt; 0
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;These simple functions actually give the one-one mapping shown in the above figure. Hence, for some point on the circle, we may end up having more than one representation.&lt;/p&gt;

&lt;p&gt;In general, manifolds can be very abstract. However, one usually puts some restrictions like those maps being smooth and analytic, so that it is easy to work with.&lt;/p&gt;

&lt;h1 id=&#34;defining-horizons&#34;&gt;Defining Horizons&lt;/h1&gt;

&lt;p&gt;When we walk on the earth, it seems that earth is flat, as we see horizons just straight ahead of us, not below us. This is precisely because earth&#39;s geometry is locally flat, i.e. at any point on earth, the local surface of the earth can be well approximated by a plane. Mathematically, this concept is called &lt;strong&gt;Tangent Spaces&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;defining-curves&#34;&gt;Defining Curves&lt;/h2&gt;

&lt;p&gt;To formalize the idea, we need to introduce curves. These are some lines on the surface of the manifold (or on the manifold). For our example, this means a path which we take when walking on the surface of the earth.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;curves.jpeg&#34; alt=&#34;&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Although the curve (the path shown in red) is a little jagged on the surface, usually mathematicians deal with the curves which are differentiable and smooth, and is easy to deal with mathematically. A curve $\gamma$ is a continuous function from $\mathbb{R}$ or some interval of $\mathbb{R}$ to the manifold $S$. For example, consider a path $\gamma : [0, 1] \rightarrow S$, then $\gamma(0)$ and $\gamma(1)$ denotes the two endpoints of the curve.&lt;/p&gt;

&lt;h2 id=&#34;working-towards-tangents&#34;&gt;Working towards tangents&lt;/h2&gt;

&lt;p&gt;Now that you have a curve to walk on, you can stop at any point and ask where the horizon is. Similar to how we define tangents or derivatives of a function, we can think of the following quantity;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\gamma&#39;(x_0) = \lim_{x \rightarrow x_0}\dfrac{\gamma(x) - \gamma(x_0)}{(x - x_0)}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We could have done that precisely, but here&#39;s the catch, is the subtraction $\gamma(x) - \gamma(x_0)$ is even possible? Since, in general, both these are elements of $S$, which being an arbitrary set, may not support simple operations like addition or multiplication. There are two ways we can circumvent the problem:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Restrict the type of the manifold to fields or some special algebraic structures to allow support for addition or multiplication.&lt;/li&gt;
&lt;li&gt;Consider an arbitrary function $f : S \rightarrow\mathbb{R}$, which can be composed with the curve to define a derivative-like object. Then, we might try to explore possible ways to remove the arbitrary function from the expression.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The 2nd approach is the one univocally taken by the mathematical community. Here, we consider any function $f$, and consider the derivative;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{d}{dt}f(\gamma(t)) 
= \lim_{h \rightarrow 0}\dfrac{f(\gamma(t+h)) - f(\gamma(t))}{h}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Let the coordinate system $\phi : S\rightarrow \mathbb{R}^n$ be given as; $\phi(p) = [\xi_1(p), \xi_2(p), \dots \xi_n(p)]$, where $\xi_i$&#39;s are coordinate functions. Let, $\gamma(t)$ be denoted by the coordinates $[\gamma_1(t), \gamma_2(t), \dots \gamma_n(t)]$. Note that,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{d}{dt}f(\gamma(t)) 
= \sum_{i=1}^{n} \frac{\partial f}{\partial\xi_i} \frac{d\gamma_i(t)}{dt}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Note that, since we took $f$ to be an arbitrary function, the tangent is given by $\sum_{i=1}^{n} \frac{d\gamma_i(t)}{dt} \frac{\partial}{\partial\xi_i}$, i.e. some linear combination (based on which curve we are walking on) of the partial differential operators $\frac{\partial}{\partial\xi_i}$. Hence, the tangent space, i.e. the space (or set) containing all such tangents at a point $p$ on the manifold $S$, is given by a vector space, with the basis being the partial differential operators $\left\{ \left( \frac{\partial}{\partial\xi_i} \right)_p : i = 1, 2, \dots n\right\}$.&lt;/p&gt;

&lt;h2 id=&#34;intuitive-understanding-of-tangents&#34;&gt;Intuitive Understanding of Tangents&lt;/h2&gt;

&lt;p&gt;Now probably you are trying to think how it all makes sense. Because after all, the tangent space is a vector space, and we generally are comfortable with vectors whose entries are some scaler quantities, specifically real numbers or complex numbers. So, how tangent spaces of manifolds have these $\left( \frac{\partial}{\partial\xi_i} \right)_p$ operators as a basis vector.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Tangentialvektor.svg/960px-Tangentialvektor.svg.png&#34; width = &#34;250px&#34;&gt;&lt;/p&gt;

&lt;p&gt;Carefully look at the picture above. The manifold $M$ is the curved regions shaded in dark grey. The tangent space $T_xM$ at the point $x$ contains all the vectors $v$ which are tangential to the manifold at the point $x$. One such vector $v$, shown in the image, does not belong to the manifold, but it belongs to the larger space in which the manifold is embedded. So, if we represent the tangent space using only $n$ element vector, this will end up specifying vectors in $\mathbb{R}^n$ (i.e. in the coordinate system), but this would only give some curve in the manifold, not something outside of the manifold $M$. Therefore, we need some type of generalization of vectors, which can be applied to any space (precisely the space where manifold $M$ is embedded in).&lt;/p&gt;

&lt;p&gt;The differential operator precisely does that. It tracks how the inverse map (or inverse coordinate system) $\phi^{-1}$ applies from $\mathbb{R}^n$ to the space (the space where manifold $M$ is embedded in), and hence it allows one to define vectors in that space, by tracking how changes in $\mathbb{R}^n$ transfers to that space.&lt;/p&gt;

&lt;h1 id=&#34;from-tangents-to-riemannian-manifold&#34;&gt;From Tangents to Riemannian Manifold&lt;/h1&gt;

&lt;p&gt;Now that we have a geometry of tangent space and the manifold, the next thing one would try to find is the distance between two points on the manifold. We know that linear algebra allows us to define distance through a norm or with inner product in general, and we have a vector space here to perform these techniques from linear algebra. So, for each point $p \in S$, i.e. point on the manifold, the tangent space $T_pS$ at $p$, is a vector space, hence can be equipped with an inner product. Such a manifold is called &lt;strong&gt;Riemannian&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Let us use the notation $(\partial_i)_p$ to denote the basis vector $(\frac{\partial}{\partial\xi_i})_p$ in the tangent space $T_pS$. Also, we define an inner product $\langle (\partial_i)_p, (\partial_j)_p\rangle = (g_{ij})_p$. Then, we can define the matrix $G_p$ with entries $\left\{ (g_{ij})_p \right\}_{i, j =1}^n$, which constitutes the inner product matrix on the tangent space $T_pS$. Due to this being an inner product matrix, it must be a positive definite matrix.&lt;/p&gt;

&lt;p&gt;Also, note that, the length of a vector in that space is;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\Vert D\Vert = \langle D, D\rangle = \sum_{i, j = 1}^n (g_{ij})_p D_iD_j\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where $D = \sum_i D_i (\partial_i)_p$. Generalizing this notion, we can find small such distances on the tangent spaces, and add (or integrate) them up in order to find the distance of a curve on that manifold. So, if $\gamma : [a, b]\rightarrow S$, is a curve on the manifold $S$, then its length is simply given by;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\Vert\gamma\Vert = \int_{a}^{b} (g_{ij})_p (\gamma&#39;_i)_p(\gamma&#39;_j)_p dt\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where $(\gamma&#39;_i)_p = (\frac{\partial}{\partial t}\gamma_i)_{\gamma^{-1}(p)}$, where $\gamma_i(t)$&#39;s are the coordinates of $\gamma(t)$. So, you consider the derivative of the coordinate functions of the curve at the point $\gamma^{-1}(p)$, i.e. the point of $t \in [a, b]$ at which we have the curve function at required point $p$, so that we can use corresponding tangent space in order to calculate length.&lt;/p&gt;

&lt;h1 id=&#34;generalizing-straight-lines&#34;&gt;Generalizing Straight Lines&lt;/h1&gt;

&lt;p&gt;Using these concepts of lengths, we can define &lt;strong&gt;Geodesics&lt;/strong&gt;, which are kind of a generalized version of a straight line. In a Euclidean plane, a straight line is axiomatically defined as the shortest curve connecting two points.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;geodesic.jpg&#34; alt=&#34;&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;In this figure, for example, the blue points shown on the plane, are connected via a curve which entirely lies on the plane, hence the curve with smallest such distance is the straight line segment joining those points. However, if we take two points on the surface of the sphere, then the shortest possible curve joining those two, which entirely lies on the surface of that sphere is a curved line. For instance, if you think of going from Hong Kong to New York, you cannot travel in a straight line, since that would require you to dig through earth&#39;s crust, make a tunnel to travel through. However, if we consider the shortest possible curve, which an aeroplane takes, is actually a curved line in 3-dimensional space, but is natural analogue to the gridlines (i.e. the meridians or latitudinal lines), hence seems like a straight line with respect to the surface of the manifold.&lt;/p&gt;

&lt;p&gt;Hence, as we know precisely how we can define lengths on a Riemannian manifold, we can move on to compute geodesic. Intuitively, for many cases, finding the geodesic is easier, once we use the coordinate functions. Note that, $\phi: S\rightarrow \mathbb{R}^n$ is our coordinate map of the manifold $S$. Hence, any geodesic in the manifold would in principle, be a geodesic in the coordinate system (which happens under certain tricky mathematical conditions), hence, we can, in principle, find the geodesic connecting the point $a, b\in S$, as;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\left\{ \phi^{-1}(\phi(a) t + (1-t)\phi(b)) : t \in [0, 1] \right\}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;since $\phi(a) t + (1-t)\phi(b)$, as $t\in[0, 1]$ is the equation of the straight line connecting the coordinates $\phi(a)$ and $\phi(b)$ in the coordinate space, and then for each point on that line, we invert it back to the manifold to get a geodesic connecting $a$ and $b$.&lt;/p&gt;

&lt;p&gt;However, such intuitive things break down under very general conditions. Surprisingly enough, on the surface of a manifold, you can create a generalized triangle, whose sides are geodesics, and have the sum of the angles of the triangle be greater or less than 180 degrees. For instance, we see one such example on our beloved planet as well.&lt;/p&gt;

&lt;div class=&#34;row&#34;&gt;
&lt;div class =&#34;col-md-6&#34;&gt;&lt;img src = &#34;more180.jpg&#34; width=&#34;100%&#34;&gt;&lt;/div&gt;
&lt;div class =&#34;col-md-6&#34;&gt;&lt;img src = &#34;less180.png&#34; width=&#34;100%&#34;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;So, as you can guess, many intuitive results of Euclidean geometry breaks down, thus creating a lot of newer scope of growth in this discipline.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Manifold&#34;&gt;https://en.wikipedia.org/wiki/Manifold&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Shun-Ichi Amari, Hiroshi Nagaoka - Methods of information geometry-American Mathematical Society (2000)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it! ð&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Texture Networks</title>
      <link>/post/post3/</link>
      <pubDate>Mon, 13 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/post3/</guid>
      <description>

&lt;p&gt;This is a rather long post, but here we shall about something really interesting, about how to mix the content of a picture with the style from another picture, called Neural Style Transfer. &lt;strong&gt;Texture Networks&lt;/strong&gt; is a Neural Network approach devised by Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi and Victor Lempitsky in 2016, which is extremely useful to synthesize new types of textures (which is extensively used in the production of clothing with exclusive designs), as well as being able to work as a style transfer mechanism.&lt;/p&gt;

&lt;h2 id=&#34;what-is-neural-style-transfer&#34;&gt;What is Neural Style Transfer?&lt;/h2&gt;

&lt;p&gt;I think the answer to this question is better to show visually, rather than talking about it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./out1.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./out2.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./out3.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;So, I think now one should get the idea. Neural Style transfer combines the aesthetics of an image on to another image (here the image of the girl named Karya, which has been provided by &lt;a href=&#34;https://github.com/DmitryUlyanov/texture_nets&#34; target=&#34;_blank&#34;&gt;Dmitry Ulyanov&amp;rsquo;s Github&lt;/a&gt;), retaining the content of the image (i.e. retaining the girl in the stylized output). Note that, the effect is particularly visible in 2nd and 3rd images, whereas, for the first image, the style aspect is greatly emphasized.&lt;/p&gt;

&lt;p&gt;I am going to discuss exactly how I created those stylized images, and hopefully, after reading this, you would be able to reproduce similar results with images of your choice.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;I am not talking about things you need to know beforehand to understand the intricate details of the mechanism, but the software requirements that I will be using to create something similar to texture networks. So, at the very beginning, I import all the required packages in &lt;code&gt;python&lt;/code&gt;. Also, I shall be using an &lt;strong&gt;NVidia GeForce GTX 1060Ti&lt;/strong&gt; Graphics card with CUDA computing capability 6.1, which is not at par with the GPU devices used at a professional level, but this speeds up the computations by a lot rather than using a CPU.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
import numpy as np
import time
import functools
import PIL.Image
import IPython.display as display
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.__version__
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&#39;2.1.0-rc0&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.test.gpu_device_name()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&#39;/device:GPU:0&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;making-some-utility-functions&#34;&gt;Making Some Utility Functions&lt;/h2&gt;

&lt;p&gt;Before proceeding with describing how the Texture Network is created, I would create some utility functions to help us later. For starters, these utility functions will allow us to load images from a path, and visualize an image given its tensor.&lt;/p&gt;

&lt;h3 id=&#34;what-is-a-tensor&#34;&gt;What is a tensor?&lt;/h3&gt;

&lt;p&gt;Let us revisit some high school mathematics for a bit. We know that matrix is a 2-dimensional array of numbers, if simply put. &lt;a href=&#34;https://en.wikipedia.org/wiki/Tensor&#34; target=&#34;_blank&#34;&gt;Tensor&lt;/a&gt; is a generalization of that, it is an n-dimensional array.&lt;/p&gt;

&lt;p&gt;Now, when a mathematician introduces matrix, it is essentially an efficient way of representing linear functions from a vector space to another vector space. A &lt;strong&gt;Vector Space&lt;/strong&gt; is a space comprised of vectors, and a vector is something that satisfies some mathematical properties. But, we don&amp;rsquo;t need that. Think of vectors in the most simple way, it is something that has a magnitude and a direction, like speed.&lt;/p&gt;

&lt;p&gt;Coming back to tensor, it is introduced as an efficient representation of &lt;strong&gt;Multilinear Maps&lt;/strong&gt; between vector spaces. Let $V_1, V_2, \dots V_n, W$ be some vector spaces, then, a function $f : V_1 \times V_2 \times\dots V_n \rightarrow W$ is said to be a multilinear map, if $f(v_1, v_2, \dots v_i, \dots v_n)$ is linear in $v_i$ given all other arguments $v_1, \dots v_{i-1}, v_{i+1}, \dots v_n$ are fixed. And such linearity holds of any of its arguments. So, if you are comfortable with the mathematics of Linear Algebra and Matrices, then you would clearly understand that tensor is just a multidimensional generalization of matrices.&lt;/p&gt;

&lt;h3 id=&#34;what-an-image-is-do-to-with-tensor&#34;&gt;What an Image is do to with Tensor?&lt;/h3&gt;

&lt;p&gt;Now, to understand how tensor comes into play to define images, one needs to understand the mechanism of how an image is stored digitally. For this, consider a black grid like chessboard, but all cells are coloured white. Now, you start colouring some cells to black, and then you would be able to generate some pictures with tons of block like artifacts. The following example from &lt;a href=&#34;http://logicalzero.com/gamby/reference/image_formats.html&#34; target=&#34;_blank&#34;&gt;logicalzero.com&lt;/a&gt; shows such a smiley face just colouring a 8x8 grid.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./smiley.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;A smiley was okay, but it was not very appealing. Now, since we have a 8x8 grid, and each of the cells can be coloured in 2 ways, black or white. Hence, by simple combinatorics, this generates $2^{256}$ possible images, out of this 8x8 grid, which is about $1.15 \times 10^{77}$. That&amp;rsquo;s a lot! However, not all such combinations will result in visually appealing images, something that we can actually call as a potential image with our natural sense. So, among these vast majority of combinations, only a few will make up images, that our brain can visualize and understand as an image.&lt;/p&gt;

&lt;p&gt;However, if we wish to create more complicated images, we need a bigger grid. The reason being that these 8x8 grid cannot be used to approximate complicated curves in the image we encounter in daily life. For instance, increasing the number of cells in the grid, we can create an image of a panda.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;nonogram.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: This image transpires as a solution of a puzzle called &lt;a href=&#34;https://en.wikipedia.org/wiki/Nonogram&#34; target=&#34;_blank&#34;&gt;Nonogram&lt;/a&gt;, which is also called as Picross or Visual Crosswords or Japanese Crosswords.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Using finer grids actually results in a better picture, as you can see. Digital black and white images are represented using the technique described above, and each of the grid cell is called a &lt;a href=&#34;https://en.wikipedia.org/wiki/Pixel&#34; target=&#34;_blank&#34;&gt;Pixel&lt;/a&gt;. Now, note that, we can represent this grid using a 2-dimensional matrix of 0&amp;rsquo;s and 1&amp;rsquo;s, where a white pixel would be represented as 0 and a black pixel would be repesented as 1. For instance, the smiley image can be matrixified like this:&lt;/p&gt;

&lt;div style=&#34;width: 500px; margin:auto; display: flex;&#34;&gt;
&lt;img src=&#34;./smiley.png&#34; style=&#34;display: inline-block;&#34;/&gt;

&lt;div style = &#34;display: inline-block;&#34;&gt;
    $$\begin{bmatrix}
    0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\
    0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
    1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1\\
    1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1\\
    1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1\\
    1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1\\
    0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
    0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\
    \end{bmatrix}$$
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Now, we shall use colour images in this context of Neural Style Transfer. To represent a colour image, we require 3 such matrices. One for Red channel, one for Blue channel and another for Green Channel. Also, the elements of the matrices will be allowed to take values between 0 and 255 (to be represented by 8 digit binary numbers) or to take any real value between 0 and 1, representing the denisty of the colour. For instance, in the above black and white images, we can put the value 0.5 in some elements to represent that those pixels should be coloured using gray, which is a colour midway between black and white. Hence, allowing floating point values would ensure a richer distribution of images.&lt;/p&gt;

&lt;p&gt;Coming back to the link between images and tensor, an image is represented by 3 such matrices, in combination, a 3-dimensional tensor, which the dimension or shape being (3, height of the image, width of the image), where 3 being number of channels.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def tensor_to_image(tensor):
    tensor = tf.clip_by_value(tensor, clip_value_min=0.0, clip_value_max=255.0)
    tensor = np.array(tensor, dtype=np.uint8)   # convert tf array to np array of integers
    if np.ndim(tensor)&amp;gt;3:
        assert tensor.shape[0] == 1  # asserts that the BATCH_SIZE = 1
        tensor = tensor[0]   # take the first image
    return PIL.Image.fromarray(tensor)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def load_img(path_to_img, rescale = False):
    # we rescale the image to max dimension 256 for fasters processing
    max_dim = 256    
    img = tf.io.read_file(path_to_img)   # read the image
    img = tf.image.decode_image(img, channels=3)    # decode into image content
    img = tf.image.convert_image_dtype(img, tf.float32)    # convert to float
    
    if rescale:
        img = tf.image.resize(img, tf.constant([max_dim, max_dim]))
    else:
        shape = tf.cast(tf.shape(img)[:-1], tf.float32)   
        # get the shape of image, cast it to float type for division, expect the last channel dimension
        long_dim = max(shape)
        scale = max_dim / long_dim    # scale accordingly
        new_shape = tf.cast(shape * scale, tf.int32)   # cast the new shape to integer
        img = tf.image.resize(img, new_shape)   # resize image
        
    img = img[tf.newaxis, :]   # newaxis builts a new batch axis in the image at first dimension
    return img
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;style_path = &#39;../input/artistic-style-transfer/pattern-rooster.jpg&#39;
content_path = &#39;../input/artistic-style-transfer/celebGAN_male.png&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;content_image = load_img(content_path, rescale = True)
tensor_to_image(content_image * 255.0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_13_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is the content image, on which we shall apply some style. Note that, the loaded images have elements between 0 and 1, while the &lt;code&gt;tensor_to_img&lt;/code&gt; function takes in a tensor with values between 0 and 255, hence we need to multiply all the elements by 255 to convert it to the crucial range where it can be visualized.&lt;/p&gt;

&lt;p&gt;Surprisingly, the image I am using is actually an image generated via Neural Network called &lt;a href=&#34;https://arxiv.org/abs/1710.10196&#34; target=&#34;_blank&#34;&gt;Progressive GAN&lt;/a&gt;, which is really interesting work by Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen, who trained a network on &lt;strong&gt;Celeb A&lt;/strong&gt; dataset to generate images of new celebrities, who do not exist in real life.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;style_image = load_img(style_path, rescale = False)
tensor_to_image(style_image * 255.0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_15_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is the style image that we can going to use. So, we think, the final image would look like the image of the artificial celebrity, tessalatted like the style image.&lt;/p&gt;

&lt;h1 id=&#34;architecture-of-texture-network&#34;&gt;Architecture of Texture Network&lt;/h1&gt;

&lt;p&gt;Texture Network comprised of two main components.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A Generator Network.&lt;/li&gt;
&lt;li&gt;A Descriptor Network.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A &lt;strong&gt;Generator Network&lt;/strong&gt; is a neural network which takes input of the content image and some random noise, and output our desired stylized image.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;Descriptor Network&lt;/strong&gt; is a neural network which takes input of the desired stylized image, and then try to figure out the underlying style and content of the stylized image, and try to match it with the original style image and content image.&lt;/p&gt;

&lt;p&gt;If you are familiar with &lt;a href=&#34;https://en.wikipedia.org/wiki/Generative_adversarial_network&#34; target=&#34;_blank&#34;&gt;Generative Adversarial Networks&lt;/a&gt; (GAN), then you would find a lot similarity of Texture Network with GANs. However, unlike to the case of GAN, here, the descriptor network will not be trained, but will be used to measure the style and content of the stylized images generated by Generator Network.&lt;/p&gt;

&lt;p&gt;To understand why such a descriptor network is needed at all, consider an image of a person. If one shifts the image just by one pixel to the left, then using a simple squared error loss between the original image and shifted image would become large, however, from our perception, both images would look identical. Hence, to actually compare the two images, we specifically need to compare high level representations of the images, which will be provided by the descriptor network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;texnet.png&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;choice-of-descriptor-network&#34;&gt;Choice of Descriptor Network&lt;/h2&gt;

&lt;p&gt;We use &lt;a href=&#34;https://arxiv.org/abs/1409.1556&#34; target=&#34;_blank&#34;&gt;VGG19&lt;/a&gt; as our descriptor network as it is a Very Deep Convolutional Networks for Large-Scale Image Recognition. It was developed by Karen Simonyan, Andrew Zisserman in 2014, and is trained with the ImageNet dataset, comprising of millions of images. Hence, high level features of this network will be an acurate representation of the style and content of images.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vgg = tf.keras.applications.VGG19(include_top=False, weights=&#39;imagenet&#39;)   # Load VGG19 pretrained Network from Keras

print()
for layer in vgg.layers:
    print(layer.name)   # print layer names so that we can reference them later
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5
80142336/80134624 [==============================] - 1s 0us/step

input_1
block1_conv1
block1_conv2
block1_pool
block2_conv1
block2_conv2
block2_pool
block3_conv1
block3_conv2
block3_conv3
block3_conv4
block3_pool
block4_conv1
block4_conv2
block4_conv3
block4_conv4
block4_pool
block5_conv1
block5_conv2
block5_conv3
block5_conv4
block5_pool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that, VGG19 has a sturcture called a Convolutional Block. Each such block consists of 5 layers of neuron, the first 4 layers being convolutional layers, and the last layers being a pooling layer. The top layer which was excluded from the loaded model is specifically a hidden dense layer that connects the final pooled layer to the output layer, which outputs the classification of the Imagenet image. However, we do not need this final hidden dense layer.&lt;/p&gt;

&lt;h2 id=&#34;creating-generator-network&#34;&gt;Creating Generator Network&lt;/h2&gt;

&lt;p&gt;To build the generator network, we need something called a Circular Convolution. To understand various convolutional arithmetic properly, I would recommend checking out the following resources.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vdumoulin/conv_arithmetic&#34; target=&#34;_blank&#34;&gt;https://github.com/vdumoulin/conv_arithmetic&lt;/a&gt; contains a simple animation showing convolutions with different parameters.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ezyang.github.io/convolution-visualizer/index.html&#34; target=&#34;_blank&#34;&gt;https://ezyang.github.io/convolution-visualizer/index.html&lt;/a&gt; has a really good interactive environment where you can set the parameters and take look at the corresponding convolutional operation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Coming back to circular convolution, it is a simple 2-dimensional convolution with with a particular type of padding called &lt;strong&gt;Circular Padding&lt;/strong&gt;. It essentially wraps the image from top to bottom and from left to right.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def periodic_padding(x, padding=1):
    &#39;&#39;&#39;
    x: shape (batch_size, d1, d2)
    return x padded with periodic boundaries. i.e. torus or donut
    &#39;&#39;&#39;
    d1 = x.shape[1] # dimension 1: height
    d2 = x.shape[2] # dimension 2: width
    p = padding
    # assemble padded x from slices
    #            tl,tc,tr
    # padded_x = ml,mc,mr
    #            bl,bc,br
    top_left = x[:, -p:, -p:] # top left
    top_center = x[:, -p:, :] # top center
    top_right = x[:, -p:, :p] # top right
    middle_left = x[:, :, -p:] # middle left
    middle_center = x # middle center
    middle_right = x[:, :, :p] # middle right
    bottom_left = x[:, :p, -p:] # bottom left
    bottom_center = x[:, :p, :] # bottom center
    bottom_right = x[:, :p, :p] # bottom right
    top = tf.concat([top_left, top_center, top_right], axis=2)
    middle = tf.concat([middle_left, middle_center, middle_right], axis=2)
    bottom = tf.concat([bottom_left, bottom_center, bottom_right], axis=2)
    padded_x = tf.concat([top, middle, bottom], axis=1)
    return padded_x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the function &lt;code&gt;periodic_padding&lt;/code&gt;, we give several input images, however, with only one channel for each image. Here, the input is a tensor of the shape (number of images, height of the image, width of the image). Hence, we can think of the input as if we are passing many 2-dimensional matrices of shape (height of the image, width of the image). We also pass the amount of padding that we want. Let us see what it outputs, when we pass a single 2D matrix as follows;&lt;/p&gt;

&lt;p&gt;$$\begin{bmatrix}
 1 &amp;amp; 2 &amp;amp; 3 \\ 4 &amp;amp; 5 &amp;amp; 6 \\ 7 &amp;amp; 8 &amp;amp; 9
\end{bmatrix}$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = tf.constant([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])
periodic_padding(a)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(1, 5, 5), dtype=int32, numpy=
array([[[9, 7, 8, 9, 7],
        [3, 1, 2, 3, 1],
        [6, 4, 5, 6, 4],
        [9, 7, 8, 9, 7],
        [3, 1, 2, 3, 1]]], dtype=int32)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that, the output tensor has size 5x5, which it obtained by one unit of padding in top, bottom, left and right. Also note that, to the left of 1, we have 3, hence it is as if the rightmost column of the original matrix is wrapped to the left side of the matrix. Similar wrapping is also seen in vertical direction.&lt;/p&gt;

&lt;p&gt;However, for an image, we need to perform this circular padding for each of the channel. Hence, we first split up the 3 channels of the image, then append circular padding for each one, and finally combine them together. This is done through &lt;code&gt;CircularPadding&lt;/code&gt; function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def CircularPadding(inputs, kernel_size = 3):
    &amp;quot;&amp;quot;&amp;quot;Prepares padding for Circular convolution&amp;quot;&amp;quot;&amp;quot;
    # split all the filters
    n_filters_in = inputs.shape[-1]
    input_split = tf.split(inputs, n_filters_in, axis = -1)
    output_split = []
    for part in input_split:
        part = tf.squeeze(part, axis = -1)
        outs = periodic_padding(part, padding = int(kernel_size / 2))
        outs = tf.expand_dims(outs, axis = -1)
        output_split.append(outs)
    return tf.concat(output_split, axis = -1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;convolution.png&#34; width = &#34;500px&#34;&gt;&lt;/p&gt;

&lt;p&gt;The diagram above shows the main idea of a convolutional layer. Let us say, we have an image, represented by a 5x5x3 tensor (pretty bad for visualizing, but pretty good for understanding), and we wish to perform convolution of this image with a 3x3 kernel. Then the convolution is basically a weighted combination of all the neighbouring pixels from all the layers. To understand mathematically, let us introduce some notations.&lt;/p&gt;

&lt;p&gt;Let $X$ denote the tensor image, and $X_{ijc}$ denote the value of the $(i, j)$-th pixel at the $c$-th channel. Now, let us say we wish to find out the value of the convolution at $(i_0, j_0)$ cell. Then, the convolution is defined as;&lt;/p&gt;

&lt;p&gt;$$H(i_0, j_0) = b + \sum_{(i, j) \in N(i_0, j_0)}\sum_{c} X_{ijc} K((i - i_0), (j - j_0), c)$$&lt;/p&gt;

&lt;p&gt;where $K(\cdot, \cdot, \cdot)$ is kernel weights i.e. some parameters of the network which the network is going to learn. Also, the parameter $b$ is the bias term and $N(i_0, j_0)$ is a neighbourhood of the pixel $(i_0, j_0)$. Therefore, if we have an image with $n$ channels, and we convolute a $k\times k$ kernel on it, then we specifically require $(k^2n + 1)$ parameters including a bias term.&lt;/p&gt;

&lt;h3 id=&#34;why-do-we-need-convolution&#34;&gt;Why do we need Convolution?&lt;/h3&gt;

&lt;p&gt;To understand convolution better, consider two vectors $\textbf{x} = [x_1, x_2, \dots x_n]$ and $\textbf{y} = [y_1, y_2, \dots y_n]$, then the dot product between them is defined as;&lt;/p&gt;

&lt;p&gt;$$\textbf{x}\cdot\textbf{y} = \sum_{k=1}^{n} x_ky_k$$&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s high school algebra. However, we also know that, dot product measures the similarity between the vectors $\textbf{x}$ and $\textbf{y}$, i.e. it is maximum when $\textbf{x},\textbf{y}$ are collinear, and is minimum when these are orthogonal to each other. Note that, the above formula of convolution exactly looks like the formula of a dot product, hence it measures the similarity between the patch of the image and the kernel that we have.&lt;/p&gt;

&lt;p&gt;Now suppose, we have a kernel that looks like as follows:&lt;/p&gt;

&lt;p&gt;$$K = \begin{bmatrix}
-1 &amp;amp; 1 &amp;amp; -1\\ -1 &amp;amp; 1 &amp;amp; -1\\ -1 &amp;amp; 1 &amp;amp; -1
\end{bmatrix}$$&lt;/p&gt;

&lt;p&gt;Then, if we convolute the image with this kernel, then it attains maximum value when we have a horizontal line, and it will attain minimum value when we have a vertical line. Hence, the convolution will tell us the presence of horizontal and vertical edges in the images, hence will provide combined information or featurs about the images to next level. Similar lower level features can again be convoluted to give rise to higher level features.&lt;/p&gt;

&lt;p&gt;It should also be ntoeworthy that each such result of convolution will tells about existence of one particular feature in the image. Hence, to effectively use it, we shall need to learn many such features, which in the literature of &lt;strong&gt;Image Processing&lt;/strong&gt; is described as filters.&lt;/p&gt;

&lt;p&gt;Coming back to the Design of Generator of Texture Network, we need several blocks. There are mainly two types of blocks.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Convolutional Block:&lt;/strong&gt; It takes the input tensor (may be image or may be features processed in lower level of the network), and performs some Circular Convolution to process the tensor further and obtain some higher level features.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Join Block:&lt;/strong&gt; It takes the lower resolution tensor and a high resolution processed noise tensor as input, then it upsamples the lower resolution tensor to match the shape of the high resolution noise and finally merges them together. This noise actually allows the image to have very delicate and intricate variation in the image, as well as create the effect of increment of resolution.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;designing-convolutional-block&#34;&gt;Designing Convolutional Block&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def conv_block(input_size, in_filters, out_filters):
    &amp;quot;&amp;quot;&amp;quot;Implements the convolutional block with 3x3, 3x3, 1x1 filters, with proper batch normalization and activation&amp;quot;&amp;quot;&amp;quot;
    inputs = tf.keras.layers.Input((input_size, input_size, in_filters, ))   # in_filters many channels of input image
    
    # first 3x3 conv
    conv1_pad = tf.keras.layers.Lambda(lambda x: CircularPadding(x))(inputs)
    conv1_out = tf.keras.layers.Conv2D(out_filters, kernel_size = (3, 3), strides = 1, 
                                       padding = &#39;valid&#39;, name = &#39;conv1&#39;)(conv1_pad)
    hidden_1 = tf.keras.layers.BatchNormalization()(conv1_out)
    conv1_out_final = tf.keras.layers.LeakyReLU(name = &#39;rel1&#39;)(hidden_1)
    
    # second 3x3 conv
    conv2_pad = tf.keras.layers.Lambda(lambda x: CircularPadding(x))(conv1_out_final)
    conv2_out = tf.keras.layers.Conv2D(out_filters, kernel_size = (3, 3), strides = 1, 
                                       padding = &#39;valid&#39;, name = &#39;conv2&#39;)(conv2_pad)
    hidden_2 = tf.keras.layers.BatchNormalization()(conv2_out)
    conv2_out_final = tf.keras.layers.LeakyReLU(name = &#39;rel2&#39;)(hidden_2)
    
    # final 1x1 conv
    conv3_out = tf.keras.layers.Conv2D(out_filters, kernel_size = (1, 1), strides = 1, 
                                       padding = &#39;same&#39;, name = &#39;conv3&#39;)(conv2_out_final)
    hidden_3 = tf.keras.layers.BatchNormalization()(conv3_out)
    conv3_out_final = tf.keras.layers.LeakyReLU(name = &#39;rel3&#39;)(hidden_3)
    
    # final model
    conv_block = tf.keras.models.Model(inputs, conv3_out_final)
    return conv_block
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = conv_block(16, 3, 8)
model.summary()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 16, 16, 3)]       0         
_________________________________________________________________
lambda (Lambda)              (None, 18, 18, 3)         0         
_________________________________________________________________
conv1 (Conv2D)               (None, 16, 16, 8)         224       
_________________________________________________________________
batch_normalization (BatchNo (None, 16, 16, 8)         32        
_________________________________________________________________
rel1 (LeakyReLU)             (None, 16, 16, 8)         0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 18, 18, 8)         0         
_________________________________________________________________
conv2 (Conv2D)               (None, 16, 16, 8)         584       
_________________________________________________________________
batch_normalization_1 (Batch (None, 16, 16, 8)         32        
_________________________________________________________________
rel2 (LeakyReLU)             (None, 16, 16, 8)         0         
_________________________________________________________________
conv3 (Conv2D)               (None, 16, 16, 8)         72        
_________________________________________________________________
batch_normalization_2 (Batch (None, 16, 16, 8)         32        
_________________________________________________________________
rel3 (LeakyReLU)             (None, 16, 16, 8)         0         
=================================================================
Total params: 976
Trainable params: 928
Non-trainable params: 48
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.keras.utils.plot_model(model, show_shapes=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_29_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the function &lt;code&gt;conv_block&lt;/code&gt;, we take the height and width of the input tensor and the number of channels of the input tensor, and the number of filters to finally output after processing. Note that, after each of the Circular Convolution, we perform a Batch Normalization and a Leaky ReLU layer.&lt;/p&gt;

&lt;p&gt;Batch Normalization layer basically normalizes the outputs with respect to the batch axis (i.e. with respect to the ? or None axis indicated in the diagram, which means you are pass arbitrary number of images through the network) so that the mean values remain close to 0, and standard deviation remains close to 1. The following image from the &lt;a href=&#34;https://arxiv.org/abs/1803.08494&#34; target=&#34;_blank&#34;&gt;2018 paper on Group Normalization&lt;/a&gt; by Yuxin Wu, Kaiming He described the idea of normalization through the following interesting image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;norm.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;Finally, Leaky ReLU is a layer than performs an nonlinear activation to the input vector. Leaky ReLU is basically the following function;&lt;/p&gt;

&lt;p&gt;$$f(x) = \begin{cases}
x &amp;amp; \text{ if } x \geq 0 \\ \alpha x &amp;amp; \text{ if } x &amp;lt; 0
\end{cases}$$&lt;/p&gt;

&lt;p&gt;where $\alpha &amp;lt; 1$ is a non-trainable constant. It is usually fixed at a very low value like $0.05$ or $0.01$. Compared to that, ReLU function is similar to Leaky ReLU, but it outputs 0 if $x &amp;lt; 0$. Hence, Leaky ReLU is essentially a leaky version of ReLU, as it leaks out some small value for negative arguments. Since, such nonlinearity does not require additional parameters, we see 0 in the above summary output.&lt;/p&gt;

&lt;h3 id=&#34;designing-join-block&#34;&gt;Designing Join Block&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def join_block(input_size, n_filter_low, n_filter_high):
    input1 = tf.keras.layers.Input((input_size, input_size, n_filter_low, ))  # input to low resolution image
    input2 = tf.keras.layers.Input((2*input_size, 2*input_size, n_filter_high, ))  # input to high resolution image
    upsampled_input = tf.keras.layers.UpSampling2D(size = (2, 2))(input1)
    hidden_1 = tf.keras.layers.BatchNormalization()(upsampled_input)
    hidden_2 = tf.keras.layers.BatchNormalization()(input2)
    
    outputs = tf.keras.layers.Concatenate(axis=-1)([hidden_1, hidden_2])
    
    # final model
    join_block = tf.keras.models.Model([input1, input2], outputs)
    return join_block
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = join_block(128, 32, 8)
model.summary()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model_1&amp;quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 128, 128, 32 0                                            
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 256, 256, 32) 0           input_3[0][0]                    
__________________________________________________________________________________________________
input_4 (InputLayer)            [(None, 256, 256, 8) 0                                            
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 256, 256, 32) 128         up_sampling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 256, 256, 8)  32          input_4[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 256, 256, 40) 0           batch_normalization_3[0][0]      
                                                                 batch_normalization_4[0][0]      
==================================================================================================
Total params: 160
Trainable params: 80
Non-trainable params: 80
__________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.keras.utils.plot_model(model, show_shapes=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_33_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;join block&lt;/code&gt; is extremely simple, it upsamples the low resolution processed features of the image. Then it normalizes both the higher resolution processed noise, and upsampled version of low resolution features, so that the effect of both branches remain comparable in the network. Finally, it combines the normalized versions.&lt;/p&gt;

&lt;h3 id=&#34;completing-the-generator&#34;&gt;Completing the Generator&lt;/h3&gt;

&lt;p&gt;According to the &lt;a href=&#34;https://arxiv.org/abs/1603.03417&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; describing Texture Networks, the generator should have a structure similar to the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;gen.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;However, it was also mentioned that for style transfer, increasing the number of noise from 5 to 6, actually provides much better quality. So, we start the network from a noise of size 8x8x3, and keep increasing it till 256x256x3, which is of the same size of our original content image.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generator_network():
    # create input nodes for noise tensors
    noise1 = tf.keras.layers.Input((256, 256, 3, ), name = &#39;noise_1&#39;)
    noise2 = tf.keras.layers.Input((128, 128, 3, ), name = &#39;noise_2&#39;)
    noise3 = tf.keras.layers.Input((64, 64, 3, ), name = &#39;noise_3&#39;)
    noise4 = tf.keras.layers.Input((32, 32, 3, ), name = &#39;noise_4&#39;)
    noise5 = tf.keras.layers.Input((16, 16, 3, ), name = &#39;noise_5&#39;)
    noise6 = tf.keras.layers.Input((8, 8, 3, ), name = &#39;noise_6&#39;)
    content = tf.keras.layers.Input((256, 256, 3, ), name = &#39;content_input&#39;)

    # downsample the content image
    content_image_8 = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, tf.constant([8, 8])))(content)
    content_image_16 = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, tf.constant([16, 16])))(content)
    content_image_32 = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, tf.constant([32, 32])))(content)
    content_image_64 = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, tf.constant([64, 64])))(content)
    content_image_128 = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, tf.constant([128, 128])))(content)
    
    # create concatenation of downsampled content image and input nodes
    noise6_con = tf.keras.layers.Concatenate(axis=-1)([noise6, content_image_8])
    noise5_con = tf.keras.layers.Concatenate(axis=-1)([noise5, content_image_16])
    noise4_con = tf.keras.layers.Concatenate(axis=-1)([noise4, content_image_32])
    noise3_con = tf.keras.layers.Concatenate(axis=-1)([noise3, content_image_64])
    noise2_con = tf.keras.layers.Concatenate(axis=-1)([noise2, content_image_128])
    noise1_con = tf.keras.layers.Concatenate(axis=-1)([noise1, content])
    
    noise6_conv = conv_block(8, 6, 8)(noise6_con)   # that produces 8x8x8 tensor
    noise5_conv = conv_block(16, 6, 8)(noise5_con)   # that produces 16x16x8 tensor
    join5 = join_block(8, 8, 8)([noise6_conv, noise5_conv])   # that produces 16x16x16 tensor
    
    join5_conv = conv_block(16, 16, 16)(join5)   # produces 16x16x16 tensor
    noise4_conv = conv_block(32, 6, 8)(noise4_con)   # that produces 32x32x8 tensor
    join4 = join_block(16, 16, 8)([join5_conv, noise4_conv])   # produces 32x32x24 tensor
    
    join4_conv = conv_block(32, 24, 24)(join4)   # produces 32x32x24 tensor
    noise3_conv = conv_block(64, 6, 8)(noise3_con)  # that produces 64x64x8 tensor
    join3 = join_block(32, 24, 8)([join4_conv, noise3_conv])   # produces 64x64x32 tensor
    
    join3_conv = conv_block(64, 32, 32)(join3)   # produces 64x64x32 tensor
    noise2_conv = conv_block(128, 6, 8)(noise2_con)  # that produces 128x128x8 tensor
    join2 = join_block(64, 32, 8)([join3_conv, noise2_conv])   # produces 128x128x40 tensor
    
    join2_conv = conv_block(128, 40, 40)(join2)   # produces 128x128x40 tensor
    noise1_conv = conv_block(256, 6, 8)(noise1_con)  # that produces 256x256x8 tensor
    join1 = join_block(128, 40, 8)([join2_conv, noise1_conv])   # produces 256x256x48 tensor
    
    output = conv_block(256, 48, 3)(join1)   # produces 256x256x3 tensor
    
    model = tf.keras.models.Model([content, noise1, noise2, noise3, noise4, noise5, noise6], output, name = &#39;generator&#39;)
    
    return model
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;generator = generator_network()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;generator.summary()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Model: &amp;quot;generator&amp;quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
content_input (InputLayer)      [(None, 256, 256, 3) 0                                            
__________________________________________________________________________________________________
noise_6 (InputLayer)            [(None, 8, 8, 3)]    0                                            
__________________________________________________________________________________________________
lambda_2 (Lambda)               (None, 8, 8, 3)      0           content_input[0][0]              
__________________________________________________________________________________________________
noise_5 (InputLayer)            [(None, 16, 16, 3)]  0                                            
__________________________________________________________________________________________________
lambda_3 (Lambda)               (None, 16, 16, 3)    0           content_input[0][0]              
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 6)      0           noise_6[0][0]                    
                                                                 lambda_2[0][0]                   
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 16, 16, 6)    0           noise_5[0][0]                    
                                                                 lambda_3[0][0]                   
__________________________________________________________________________________________________
model_2 (Model)                 (None, 8, 8, 8)      1192        concatenate_1[0][0]              
__________________________________________________________________________________________________
model_3 (Model)                 (None, 16, 16, 8)    1192        concatenate_2[0][0]              
__________________________________________________________________________________________________
noise_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
lambda_4 (Lambda)               (None, 32, 32, 3)    0           content_input[0][0]              
__________________________________________________________________________________________________
model_4 (Model)                 (None, 16, 16, 16)   64          model_2[1][0]                    
                                                                 model_3[1][0]                    
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 32, 32, 6)    0           noise_4[0][0]                    
                                                                 lambda_4[0][0]                   
__________________________________________________________________________________________________
model_5 (Model)                 (None, 16, 16, 16)   5104        model_4[1][0]                    
__________________________________________________________________________________________________
model_6 (Model)                 (None, 32, 32, 8)    1192        concatenate_3[0][0]              
__________________________________________________________________________________________________
noise_3 (InputLayer)            [(None, 64, 64, 3)]  0                                            
__________________________________________________________________________________________________
lambda_5 (Lambda)               (None, 64, 64, 3)    0           content_input[0][0]              
__________________________________________________________________________________________________
model_7 (Model)                 (None, 32, 32, 24)   96          model_5[1][0]                    
                                                                 model_6[1][0]                    
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 64, 64, 6)    0           noise_3[0][0]                    
                                                                 lambda_5[0][0]                   
__________________________________________________________________________________________________
model_8 (Model)                 (None, 32, 32, 24)   11304       model_7[1][0]                    
__________________________________________________________________________________________________
model_9 (Model)                 (None, 64, 64, 8)    1192        concatenate_4[0][0]              
__________________________________________________________________________________________________
noise_2 (InputLayer)            [(None, 128, 128, 3) 0                                            
__________________________________________________________________________________________________
lambda_6 (Lambda)               (None, 128, 128, 3)  0           content_input[0][0]              
__________________________________________________________________________________________________
model_10 (Model)                (None, 64, 64, 32)   128         model_8[1][0]                    
                                                                 model_9[1][0]                    
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 128, 128, 6)  0           noise_2[0][0]                    
                                                                 lambda_6[0][0]                   
__________________________________________________________________________________________________
model_11 (Model)                (None, 64, 64, 32)   19936       model_10[1][0]                   
__________________________________________________________________________________________________
model_12 (Model)                (None, 128, 128, 8)  1192        concatenate_5[0][0]              
__________________________________________________________________________________________________
noise_1 (InputLayer)            [(None, 256, 256, 3) 0                                            
__________________________________________________________________________________________________
model_13 (Model)                (None, 128, 128, 40) 160         model_11[1][0]                   
                                                                 model_12[1][0]                   
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 256, 256, 6)  0           noise_1[0][0]                    
                                                                 content_input[0][0]              
__________________________________________________________________________________________________
model_14 (Model)                (None, 128, 128, 40) 31000       model_13[1][0]                   
__________________________________________________________________________________________________
model_15 (Model)                (None, 256, 256, 8)  1192        concatenate_6[0][0]              
__________________________________________________________________________________________________
model_16 (Model)                (None, 256, 256, 48) 192         model_14[1][0]                   
                                                                 model_15[1][0]                   
__________________________________________________________________________________________________
model_17 (Model)                (None, 256, 256, 3)  1431        model_16[1][0]                   
==================================================================================================
Total params: 76,567
Trainable params: 75,269
Non-trainable params: 1,298
__________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.keras.utils.plot_model(generator, show_shapes = True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_38_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, we have a generator model with about 75,000 parameters.&lt;/p&gt;

&lt;p&gt;It should be clear that the low level features, i.e. the outputs of the lower level (closer to input layer that final layer in VGG19) layers are basically features such a strokes, edges which defines the artistic features of the image, whereas, high level features, i.e. the outputs of the higher level (closer to final layer than input layer in VGG19) are features that contains the summarization of the image, namely the abstract object that image contains, specifically the content of the image.&lt;/p&gt;

&lt;p&gt;However, Gatys et al. in their &lt;a href=&#34;https://arxiv.org/abs/1508.06576&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; &lt;strong&gt;A Neural Algorithm for Artistic Style&lt;/strong&gt; depicts that the style is basically represented by the correlation of the low level features of an image, rather than the particular output of those low level features. An intuitive explanation to this particular observation can be given as follows: Consider our style image to be an image of a floor covered with square tiles, like a chessboard. Now, think of applying this tiling style to a complicated image of a landscape. To do this, one may need to consider rotating the tiles to be in a shape like diamonds, which may be able to capture better details of some corners presented in the content image. In this case, if we have a vertical and horizontal line detecting kernels as well as criss-cross line detecting kernels in VGG19, then such a pattern would not give similar output for both style image and final stylized image, but would give similar amount of correlation between those features.&lt;/p&gt;

&lt;p&gt;Based on this idea, Gatys et al. introduced &lt;strong&gt;Gram Matrix&lt;/strong&gt; to measure the style of an image. Let us particularly concentration on one such low level layer first, for which we have a output tensor of shape (batch size, height of the image, width of the image, number of filters output). Then the corresponding gram matrix of this layer is defined as a matrix of shape (batch size, height of the image, width of the image), whose elements are given by;&lt;/p&gt;

&lt;p&gt;$$G_{bij} = \dfrac{\sum_c \sum_d X_{bijc} X_{bijd}}{HW}$$&lt;/p&gt;

&lt;p&gt;where $X$ is the output tensor of that layer, $H, W$ denoting the height and width of the images. This formula can be implemented by &lt;code&gt;tf.linalg.einsum&lt;/code&gt; function available in &lt;code&gt;tensorflow&lt;/code&gt;, which performs the sum operation (of the numerator here) based on a given equation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gram_matrix(input_tensor):
    result = tf.linalg.einsum(&#39;bijc,bijd-&amp;gt;bcd&#39;, input_tensor, input_tensor)   # compute the sum in numerator
    input_shape = tf.shape(input_tensor)  # get the shape
    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)  
    return result/(num_locations)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, I create a function called &lt;code&gt;vgg_layers&lt;/code&gt; which returns our descriptor network, given the name of layers of VGG19 network which will be used to compute content and style of the image outputted by Generator Network.&lt;/p&gt;

&lt;p&gt;Furthermore, a class called &lt;code&gt;TextureNetwork&lt;/code&gt; is created, which is inherited from &lt;code&gt;tf.keras.models.Model&lt;/code&gt;. &lt;code&gt;tf.keras.models.Model&lt;/code&gt; is a basic class to build new type of neural network model in &lt;code&gt;keras&lt;/code&gt;. Inheritence allows us to automatically defines several properties of the keras model, in order to perform optimization and necessary computation in the training stage. Any object of &lt;code&gt;TextureNetwork&lt;/code&gt; class is a Texture Network, initialized by specified names of content layers and style layers. Also, the &lt;code&gt;call&lt;/code&gt; method of the class has been overridden from call method of &lt;code&gt;tf.keras.models.Model&lt;/code&gt; class, which allows to define the exact workflow of feed forward system of our Texture Network. It finally outputs the generated image, along with the content output and style outputs (i.e. the gram matrices for style layers), which we can use to compute the loss function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vgg_layers(layer_names):
    &amp;quot;&amp;quot;&amp;quot; Creates a vgg model that returns a list of intermediate output values.&amp;quot;&amp;quot;&amp;quot;
    # Load our model. Load pretrained VGG, trained on imagenet data
    vgg = tf.keras.applications.VGG19(include_top=False, weights=&#39;imagenet&#39;)  # load the vgg model
    vgg.trainable = False    # do not train over vgg model parameters
  
    outputs = [vgg.get_layer(name).output for name in layer_names]    # the output of the layers that we want

    model = tf.keras.Model([vgg.input], outputs)   # create a keras model
    return model


class TextureNetwork(tf.keras.models.Model):
    def __init__(self, style_layers, content_layers):
        super(TextureNetwork, self).__init__()   # initialize the superClass
        self.vgg =  vgg_layers(style_layers + content_layers)    # obtain a VGG19 model with outputs being the style and content layers
        self.style_layers = style_layers
        self.content_layers = content_layers
        self.num_style_layers = len(style_layers)
        self.vgg.trainable = False  # we are not going to train vgg network

        self.gen = generator_network()   # create a generator network as part of it
        self.gen.trainable = True   # we are going to train this generator
        

    def call(self, content, batch_size = 16):
        # generates noise required for the network
        noise1 = tf.random.uniform((batch_size, 256, 256, 3))
        noise2 = tf.random.uniform((batch_size, 128, 128, 3))
        noise3 = tf.random.uniform((batch_size, 64, 64, 3))
        noise4 = tf.random.uniform((batch_size, 32, 32, 3))
        noise5 = tf.random.uniform((batch_size, 16, 16, 3))
        noise6 = tf.random.uniform((batch_size, 8, 8, 3))
    
        gen_image = self.gen([content, noise1, noise2, noise3, noise4, noise5, noise6])   # pass through the generator to obtain generated image
    
        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(gen_image)  # preprocess the image
        outputs = self.vgg(preprocessed_input)  # get the output from only the required layers
        
        style_outputs, content_outputs = (outputs[:self.num_style_layers], 
                                      outputs[self.num_style_layers:])
        
        style_outputs = [gram_matrix(style_output)
                         for style_output in style_outputs]  # create style type output to compare

        style_dict = {style_name:value
                      for style_name, value
                      in zip(self.style_layers, style_outputs)}

        content_dict = {content_name:value 
                    for content_name, value 
                    in zip(self.content_layers, content_outputs)}


        return {&#39;gen&#39;:gen_image, &#39;content&#39;:content_dict, &#39;style&#39;:style_dict}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have defined our Texture Network class, we need to specify exactly which layers we are going to use for style features. According to the &lt;a href=&#34;https://arxiv.org/abs/1603.03417&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; on Texture Network, the style layers should be &lt;em&gt;rel1_1, rel2_1, rel3_1, rel4_1&lt;/em&gt;, while the content layer should be &lt;em&gt;rel4_2&lt;/em&gt;. To compare it with the naming convention of VGG19, replace &lt;em&gt;rel&lt;/em&gt; with &lt;em&gt;block&lt;/em&gt; and add the convolution layer specified. However, I decided to add layer &lt;em&gt;block5_conv2&lt;/em&gt;, since according to Gatys et al. this layer has good features related to the content, and for the style image, we can get the tessellation effect accurately by using this particular layer. However, you are welcome to try different layers, which may result in interesting stylized images.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;style_layers = [&#39;block1_conv1&#39;,
                &#39;block2_conv1&#39;,
                &#39;block3_conv1&#39;,
                &#39;block4_conv1&#39;,
                &#39;block5_conv2&#39;]
content_layers = [&#39;block4_conv2&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)  # use an Adam optimizer
tex_net = TextureNetwork(style_layers, content_layers)   # create the texture network
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that, since the initial weights are set to 0, or close to 0, the illiterate network produces a black image. So, let us teach the network to learn its trainable parameters and weights, so that it can produces meaningful outputs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;output = tex_net(content_image, 1)
tensor_to_image(output[&#39;gen&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_47_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Before training, we require the target variables, based on which our network compares its performance and computes the loss function. In the &lt;a href=&#34;https://arxiv.org/abs/1603.03417&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; on Texture Network, the author trains this network on MS COCO and ImageNet dataset. However, we shall use only the content and style image that we have repeatedly, since training on those large datasets would take particularly large amount of time and much computational power, which I lack because of scarcity of funds. So, I shall feed in the same content image and style image to the network repeatedly, and hope that the random noise inputs to the network is going to prevent overfitting of the network.&lt;/p&gt;

&lt;p&gt;So, I define a function called &lt;code&gt;extract targets&lt;/code&gt; which will extract the style and content from the style and content images, and then those values can be used to compute the loss function. To explicity write the loss function, we shall use the formula;&lt;/p&gt;

&lt;p&gt;$$\text{Loss} = w_c \times L_c + w_s \times \sum_{k} L_{s_k}$$&lt;/p&gt;

&lt;p&gt;where $L_c$ is the content loss, computed as the mean squared error between the content tensor of content image and the content tensor of generated image. $L_{s_k}$ is similar mean squared error between the gram matix of style image and gram matrix of generated image at $k$-th style layer, and $w_c$ and $w_s$ denotes the weights corresponding to content and style loss.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def extract_targets(inputs):
    inputs = inputs*255.0
    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)  # preprocess the input image
    outputs = vgg_layers(style_layers + content_layers)(preprocessed_input)  # get the output from only the required layers
        
    style_outputs, content_outputs = (outputs[:len(style_layers)], 
                                       outputs[len(style_layers):])
        
    style_outputs = [gram_matrix(style_output)
                         for style_output in style_outputs]  # create style type output to compare

    style_dict = {style_name:value
                      for style_name, value
                      in zip(style_layers, style_outputs)}

    content_dict = {content_name:value 
                    for content_name, value 
                    in zip(content_layers, content_outputs)}

    return {&#39;content&#39;:content_dict, &#39;style&#39;:style_dict}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;style_targets = extract_targets(style_image)[&#39;style&#39;]
content_targets = extract_targets(content_image)[&#39;content&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;style_weight = 1e-5
content_weight = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This particular choice to style and content weights seems to work for me pretty well. However, you are encouraged to try out different style and content weight combination to dig up more interesting findings.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def custom_loss(outputs, batch_size):
    gen_outputs = outputs[&#39;gen&#39;]
    style_outputs = outputs[&#39;style&#39;]   # for generated image, get the style
    content_outputs = outputs[&#39;content&#39;]  # get content
    batch_loss = 0
    for i in range(batch_size):
        style_loss = tf.add_n([tf.reduce_mean((style_outputs[name][i]-style_targets[name])**2) 
                           for name in style_outputs.keys()])
        style_loss *= style_weight / len(style_layers)

        content_loss = tf.add_n([tf.reduce_mean((content_outputs[name][i]-content_targets[name])**2) 
                                 for name in content_outputs.keys()])
        content_loss *= content_weight / len(content_layers)
        
        loss = style_loss + content_loss
        batch_loss += loss
        
    batch_loss /= batch_size
    return batch_loss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we define our &lt;code&gt;train_step&lt;/code&gt; method, which performs one step of training of the Texture Network. The &lt;code&gt;tf.function&lt;/code&gt; decorator used for the function actually converts this function and underlying object to tensorflow graph, based on which we can perform the feed forward pass as well as compute the gradients based on back propagation. Defining this decorator is essential for the training. More details on this is available at &lt;a href=&#34;https://www.tensorflow.org/tutorials/customization/performance&#34; target=&#34;_blank&#34;&gt;Tensorflow website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the &lt;code&gt;train_step&lt;/code&gt; function, we use &lt;code&gt;tf.GradientTape&lt;/code&gt; to record the feed forward passes through the network. It works like computing a recording of the feed forward pass through the model, and finally playing it backwards, in order to perform back propagation. This gradient tape records all the gradients happening through the graph, and finally enables us to apply the training rule to update the current value of the parameters using these gradients by &lt;code&gt;apply_gradients&lt;/code&gt; function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@tf.function()
def train_step(content_image, batch_size):
    
    with tf.GradientTape() as tape:
        outputs = tex_net(content_image, batch_size)
        loss = custom_loss(outputs, batch_size)
        
    gradients = tape.gradient(loss, tex_net.trainable_variables)  # obtain the gradients recorded by the tape
    optimizer.apply_gradients(zip(gradients, tex_net.trainable_variables))   # apply the training rule using the gradients to modify the current value of prameters
    return output, loss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, I used 2500 iterations of update to train the network. I also show the images generated by the network after each 250 iterations, to see how the network improves over training.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;batch_size = 32
my_content = tf.concat([content_image for _ in range(batch_size)], axis = 0)

n_epoch = 10
n_iter = 250
iter_to_show_output = 25

loss_array = []
for epoch in range(n_epoch):
    msg = &#39;Epoch: &#39; + str(epoch)
    print(msg)
    os.system(&#39;echo &#39; + msg)
    for step in range(n_iter):
        outputs, loss = train_step(my_content, batch_size)
        if step % iter_to_show_output == 0:
            os.system(&#39;echo loss: &#39; + str(float(loss)))
            print(&#39;Loss: &#39;, loss)
            loss_array.append(loss)
    display.display(tensor_to_image(tex_net(content_image, 1)[&#39;gen&#39;]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch: 0
Loss:  tf.Tensor(27138224.0, shape=(), dtype=float32)
Loss:  tf.Tensor(26169034.0, shape=(), dtype=float32)
Loss:  tf.Tensor(26064738.0, shape=(), dtype=float32)
Loss:  tf.Tensor(26034120.0, shape=(), dtype=float32)
Loss:  tf.Tensor(26004590.0, shape=(), dtype=float32)
Loss:  tf.Tensor(24625778.0, shape=(), dtype=float32)
Loss:  tf.Tensor(22932934.0, shape=(), dtype=float32)
Loss:  tf.Tensor(22210894.0, shape=(), dtype=float32)
Loss:  tf.Tensor(21038836.0, shape=(), dtype=float32)
Loss:  tf.Tensor(19649052.0, shape=(), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_57_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch: 1
Loss:  tf.Tensor(16565710.0, shape=(), dtype=float32)
Loss:  tf.Tensor(13611194.0, shape=(), dtype=float32)
Loss:  tf.Tensor(12152441.0, shape=(), dtype=float32)
Loss:  tf.Tensor(11187487.0, shape=(), dtype=float32)
Loss:  tf.Tensor(10576777.0, shape=(), dtype=float32)
Loss:  tf.Tensor(9360093.0, shape=(), dtype=float32)
Loss:  tf.Tensor(8675145.0, shape=(), dtype=float32)
Loss:  tf.Tensor(8301968.0, shape=(), dtype=float32)
Loss:  tf.Tensor(7836261.0, shape=(), dtype=float32)
Loss:  tf.Tensor(7389797.5, shape=(), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_57_3.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch: 2
Loss:  tf.Tensor(7004705.0, shape=(), dtype=float32)
Loss:  tf.Tensor(6608028.0, shape=(), dtype=float32)
Loss:  tf.Tensor(6266657.5, shape=(), dtype=float32)
Loss:  tf.Tensor(6142354.0, shape=(), dtype=float32)
Loss:  tf.Tensor(5814227.0, shape=(), dtype=float32)
Loss:  tf.Tensor(5569461.0, shape=(), dtype=float32)
Loss:  tf.Tensor(5358932.5, shape=(), dtype=float32)
Loss:  tf.Tensor(5086491.0, shape=(), dtype=float32)
Loss:  tf.Tensor(4859027.0, shape=(), dtype=float32)
Loss:  tf.Tensor(4643354.0, shape=(), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_57_5.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch: 3
Loss:  tf.Tensor(4957266.0, shape=(), dtype=float32)
Loss:  tf.Tensor(4299568.5, shape=(), dtype=float32)
Loss:  tf.Tensor(4091998.8, shape=(), dtype=float32)
Loss:  tf.Tensor(4018616.8, shape=(), dtype=float32)
Loss:  tf.Tensor(3923811.2, shape=(), dtype=float32)
Loss:  tf.Tensor(3894964.5, shape=(), dtype=float32)
Loss:  tf.Tensor(3704813.2, shape=(), dtype=float32)
Loss:  tf.Tensor(3689166.5, shape=(), dtype=float32)
Loss:  tf.Tensor(3605529.8, shape=(), dtype=float32)
Loss:  tf.Tensor(3521357.8, shape=(), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_57_7.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch: 4
Loss:  tf.Tensor(3489987.0, shape=(), dtype=float32)
Loss:  tf.Tensor(3460784.0, shape=(), dtype=float32)
Loss:  tf.Tensor(3378974.0, shape=(), dtype=float32)
Loss:  tf.Tensor(3401102.8, shape=(), dtype=float32)
Loss:  tf.Tensor(3323982.8, shape=(), dtype=float32)
Loss:  tf.Tensor(3285243.0, shape=(), dtype=float32)
Loss:  tf.Tensor(3220874.8, shape=(), dtype=float32)
Loss:  tf.Tensor(3295160.2, shape=(), dtype=float32)
Loss:  tf.Tensor(3169093.5, shape=(), dtype=float32)
Loss:  tf.Tensor(3131125.2, shape=(), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_57_9.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch: 5
Loss:  tf.Tensor(3111150.5, shape=(), dtype=float32)
Loss:  tf.Tensor(3085978.8, shape=(), dtype=float32)
Loss:  tf.Tensor(3064475.2, shape=(), dtype=float32)
Loss:  tf.Tensor(3012352.0, shape=(), dtype=float32)
Loss:  tf.Tensor(2990475.0, shape=(), dtype=float32)
Loss:  tf.Tensor(2958282.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2929994.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2923794.8, shape=(), dtype=float32)
Loss:  tf.Tensor(2887351.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2852276.8, shape=(), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_57_11.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch: 6
Loss:  tf.Tensor(2829871.8, shape=(), dtype=float32)
Loss:  tf.Tensor(2833075.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2788235.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2777346.8, shape=(), dtype=float32)
Loss:  tf.Tensor(2735421.0, shape=(), dtype=float32)
Loss:  tf.Tensor(2742302.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2696765.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2700100.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2741282.8, shape=(), dtype=float32)
Loss:  tf.Tensor(3431759.8, shape=(), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_57_13.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch: 7
Loss:  tf.Tensor(2922398.0, shape=(), dtype=float32)
Loss:  tf.Tensor(2783216.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2718686.0, shape=(), dtype=float32)
Loss:  tf.Tensor(2677556.8, shape=(), dtype=float32)
Loss:  tf.Tensor(2645984.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2625516.8, shape=(), dtype=float32)
Loss:  tf.Tensor(2597460.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2577406.0, shape=(), dtype=float32)
Loss:  tf.Tensor(2555875.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2537895.5, shape=(), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_57_15.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch: 8
Loss:  tf.Tensor(2528999.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2511399.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2496434.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2484290.0, shape=(), dtype=float32)
Loss:  tf.Tensor(2472602.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2465217.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2449646.8, shape=(), dtype=float32)
Loss:  tf.Tensor(2445076.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2431088.0, shape=(), dtype=float32)
Loss:  tf.Tensor(2422472.8, shape=(), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_57_17.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch: 9
Loss:  tf.Tensor(2416303.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2413389.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2430066.5, shape=(), dtype=float32)
Loss:  tf.Tensor(3215110.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2591146.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2484897.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2440708.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2416797.5, shape=(), dtype=float32)
Loss:  tf.Tensor(2400430.2, shape=(), dtype=float32)
Loss:  tf.Tensor(2389489.8, shape=(), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_57_19.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note how the generated image becomes better and better with more iteration, by mimicking the style of tessellation on our image of AI generated celebrity. But the image does not change much after epoch 5, except the colour gets more violet-ish rather than blue-ish. However, you can manipulate the style weights and content weights properly, in order to have a good balance between the content and style. Also, if we take a look at the loss function, we see that the loss was decreasing rapidly at the beginning, and finally it has more or less stabilized at a point where it cannot be lowered further by much. Hence, by looking at the loss function, it seems the generator is trained enough to meet its capacities.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(loss_array)
plt.title(&#39;Loss Function over time&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_59_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From the generation the final stylized image, it seems as if the image of the celebrity is properly tessellated, as we desired. However, it is mostly blue, and there are some funny artifacts and the colour of the skin appearing at the forehead area. This can possibly be resolved by carefully trying out different style and content layer repesentations, as well as adding a variational loss to the custom loss function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor_to_image(tex_net(content_image, 1)[&#39;gen&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./index_61_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;useful-references&#34;&gt;Useful References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Texture Networks: Feed-forward Synthesis of Textures and Stylized Images - Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, Victor Lempitsky. &lt;a href=&#34;https://arxiv.org/abs/1603.03417&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1603.03417&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/generative/style_transfer&#34; target=&#34;_blank&#34;&gt;https://www.tensorflow.org/tutorials/generative/style_transfer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Neural Algorithm of Artistic Style - Leon A. Gatys, Alexander S. Ecker, Matthias Bethge. &lt;a href=&#34;https://arxiv.org/abs/1508.06576&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1508.06576&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/DmitryUlyanov/texture_nets&#34; target=&#34;_blank&#34;&gt;https://github.com/DmitryUlyanov/texture_nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perceptual Losses for Real-Time Style Transfer and Super-Resolution - Justin Johnson, Alexandre Alahi, Li Fei-Fei. &lt;a href=&#34;https://arxiv.org/abs/1603.08155&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1603.08155&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Circular_convolution&#34; target=&#34;_blank&#34;&gt;https://en.wikipedia.org/wiki/Circular_convolution&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;thank-you-very-much-for-reading&#34;&gt;Thank you very much for reading!&lt;/h2&gt;

&lt;h3 id=&#34;if-you-find-it-interesting-please-consider-sharing&#34;&gt;If you find it interesting, please consider sharing!&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Integral Calculus for 10&#43;2 Level</title>
      <link>/post/post2/</link>
      <pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/post/post2/</guid>
      <description>&lt;p&gt;This is an introductory notes for learning Integral Calculus and some useful tricks to perform integration at Pre-College level.&lt;/p&gt;

&lt;p&gt;You can find the notes by clicking &lt;a href=&#34;integration.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it! ð&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>D Basu Memorial Presentation on t-SNE: A way to Visualize Multidimensional Dataset</title>
      <link>/post/post1/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/post/post1/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
